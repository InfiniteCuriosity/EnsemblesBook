[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"Welcome Ensembles! book guide entire process building ensemble models beginning end. also give full access Ensembles package automates entire process.’ve done best make book interesting, fun, practical. lots examples using real world data steps included.able wonderful things complete skills book. book show, ensembles much accurate method help us understand model nature. done level accuracy achieved previously. can .phrase “wonderful things” intentional. Howard Carter archaeology, one point November, 1922, quite sure found something important. Carter made small hole see . Lord Carnarvon (paying !) asked Howard Carter, “Can see anything?. Howard Carter’s famous reply,”Yes, wonderful things!“. opened everything , found intact tomb Tutankhamun. contained 5,000 items, enriched knowledge ancient Africa beyond find.tiny taste one 5,000 “wonderful things” found Howard Carter, Lord Carnarvon, team archaeologists.best share many “wonderful things” entire book explore world ensembles.Ensembles package ’ve made entire analysis process automatically. put power ensembles hands, give strongest foundation work, highest degree accuracy.examples book come real data. example (many examples book):• HR Analytics• Predicting winning time London Marathon• World’s accurate score difficult classification problem• Beat best score student Kaggle competitionsWe many practical examples wide range fields enjoy.book show ensembles improve understanding nature, can use ensembles work. results using ensembles much accurate ever possible , demonstrated book. able use ensembles understand world, build models data, level accuracy achieved .","code":""},{"path":"index.html","id":"ensembles-the-new-ai-from-beginner-to-expert","chapter":"1 Welcome!","heading":"1.1 Ensembles: The New AI, from beginner to expert","text":"see, Ensembles new AI. Science gone calculus Newton Leibnetz, differential equations, modern world creating models, many points -. Ensembles powerful way put models together achieve best possible results. book guide process, show can build ensembles pass testing.new AI. Welcome path, ’s extremely fun, look forward sharing !","code":""},{"path":"index.html","id":"what-you-will-be-able-to-do-by-the-end-of-the-book","chapter":"1 Welcome!","heading":"1.2 What you will be able to do by the end of the book","text":"• Make customized ensembles models numerical, classification, logistic time series data.• Use Ensembles package entire process automatically (little customization possible).• Make ensemble solutions packages can shared users.• Make ensemble solutions totally self-contained solutions can shared anyone.• Learn ensembles models can help make wisest possible decision based data.• Learn present results different levels, regular user CEO board directors.• present results social media friendly.• Find data create ensemble solution beginning end (called One chapter exercises)• Solve real world examples book ensembles achieve results :• Beat top score student data science competition 90% (numerical ensembles).• Correctly predict winning time 2024 Men’s London Marathon (time series ensembles).• Produce 100% accurate solution dry beans classification problem (first world data set, done using classification ensembles).• Make recommendations Lebron James can improve performance basketball court (logistic ensembles).• Complete comprehensive Final Project put new skills ensembles together. result can shared employers, advisors, social media, job interviews, anywhere else like share work.","code":""},{"path":"index.html","id":"how-this-book-is-organized-so-you-learn-the-material-as-easily-as-possible","chapter":"1 Welcome!","heading":"1.3 How this book is organized so you learn the material as easily as possible","text":"book begins foundations making ensembles models. look :• Individual numerical models• Ensembles numerical models• Individual classification models• Ensembles classification models• Individual logistic models• Ensembles logistic models• Individual forecasting models• Ensembles forecasting models• Advanced data visualizations• Multiple ways communicate results. range people field, customers, C-Suite (CEO, CTO, board directors, etc.)• look treat data science business. particular pay close attention showing return investment (ROI) data science, using ensembles models.• book conclude showing four examples final comprehensive project. one example numerical data, classification data, logistic data forecasting data. example professionally formatted. source files eight files available github repository.","code":""},{"path":"index.html","id":"how-you-can-learn-the-skills-as-fast-as-possible-how-the-exercises-are-organized","chapter":"1 Welcome!","heading":"1.4 How you can learn the skills as fast as possible: How the exercises are organized","text":"young child, learned much better retention system always called delayed repetition. means learn best fastest see worked example, several practice examples, repeat delay time. delay can range hour days.example, exercises Individual Classification Models chapter ask build models using techniques classification models prior chapters. exercises logistic ensembles ask build models content logistic models chapter, previous chapters. experience repeating fastest way learn new content, retain longest period time.time get Final Comprehensive Project, skills sharp modeling techniques.","code":""},{"path":"index.html","id":"going-from-student-to-teacher-you-are-required-to-post-on-social-media-and-help-others-understand-the-results","chapter":"1 Welcome!","heading":"1.5 Going from student to teacher: You are required to post on social media and help others understand the results","text":"One important parts role data science communicating findings. present many examples summaries reports adapt use projects. also required post results social media. may use appropriate choice social media, needs publicly available. number important benefits :• build body work shows skill level• results demonstrate ability communicate way works wide variety people• work demonstrate good skills video /audio production• Use hashtag #AIEnsembles post social media","code":""},{"path":"index.html","id":"helping-you-use-the-power-of-pre-trained-ensembles-and-individual-models","chapter":"1 Welcome!","heading":"1.6 Helping you use the power of pre-trained ensembles and individual models","text":"Another important part skills learn includes building pre-trained ensembles models. book walk process building pre-trained models ensembles four types data (numerical, classification, logicial, time series).","code":""},{"path":"index.html","id":"helping-you-master-the-material-one-of-your-own-exercises","chapter":"1 Welcome!","heading":"1.7 Helping you master the material: One of your own exercises","text":"One differences exercises Ensembles inclusion One exercises. set exercises include one asks find data (many hints given help find data), define problem, make ensemble, report results.","code":""},{"path":"index.html","id":"keeping-it-real-actual-business-data-and-problems-as-the-source-of-all-the-data-sets","chapter":"1 Welcome!","heading":"1.8 Keeping it real: Actual business data and problems as the source of all the data sets","text":"data sets book use real data. exceptions, synthetic data. sources data cited, real world implications can found simple search. data absolutely real.","code":""},{"path":"index.html","id":"check-your-biases-test-your-model-on-a-neutral-data-set","chapter":"1 Welcome!","heading":"1.9 Check your biases: Test your model on a neutral data set","text":"set exercises ask check one trained models neutral data set. model biases, reveal . ’ll knowledge go back address biases models.","code":""},{"path":"index.html","id":"helping-you-check-your-workand-verifying-that-your-results-beat-previously-published-results","chapter":"1 Welcome!","heading":"1.10 Helping you check your work—and verifying that your results beat previously published results","text":"Many data sets solved previous investigators (competitions), results can easily compared published results.example, look Boston Housing data set look numerical data sets. data set used many times Kaggle competitions, published papers, Github repositories, among many sources.Ensembles package automatically solve data set, return RMSE less 0.20 (slight variation depending parameters set, explained chapters). comparison, Boston Housing data set used Kaggle student competition: https://www.kaggle.com/competitions/uou-g03784-2022-spring/leaderboard?tab=public, best score 2.09684. Ensembles package beat best result Kaggle student competition 90%. Ensembles package requires one line code.","code":""},{"path":"index.html","id":"helping-you-work-as-a-team-with-fully-reproducible-ensembles-and-individual-models","chapter":"1 Welcome!","heading":"1.11 Helping you work as a team with fully reproducible ensembles and individual models","text":"large part skills learn include make results reproducible. include:• Multiple random resamplings data• Learning test totally unseen data individual ensemble models• repeat results (example, 25 times), report accuracy resamplingFor example, make ensembles models, use trained models make predictions totally unseen data.","code":""},{"path":"index.html","id":"the-final-comprehensive-project-will-put-everything-together-for-you","chapter":"1 Welcome!","heading":"1.12 The Final Comprehensive Project will put everything together for you","text":"studying data science, one professors said papers turned “good enough show CEO Board Directors” Fortune 1000 company worked . chapter Final Comprehensive Project share highest level skills following:• Truly understanding business problem• able convey high value data science brings table• able back 100% claims rock solid evidence, facts, clear reasoning• make truly professional quality presentation worthy C-SuiteI’ve incredible pleasure learning many different skills. include able play 20 musical instruments, communicate three languages professional level, manage multi-million dollar division Fortune 1000 company, run two non-profit volunteer groups, snowboard three mile run Colorado, work professional counselor, much . book reading recent project. None skills acquired overnight. huge part success able make slow (usually) steady progress. next chapter reveal big secret getting results, now best plan regular time work contents book.Always remember test everything, save ton problems road.","code":""},{"path":"index.html","id":"exercises-to-help-improve-your-skills","chapter":"1 Welcome!","heading":"1.13 Exercises to help improve your skills","text":"Exercise 1: Schedule regular time work bookYou gain much progress work steady pace. Take everything small pieces. ’s OK go slow, long keep going. Schedule regular time work book, get largest possible reward efforts.Exercise 2: Read chapter least twice begin working material.Reading chapter twice begin working actually speed progress results. actually take less time complete chapter. might believe right now, ’s totally true.Exercise 2a: Read chapter ahead able .Exercise 3: Read chapter ","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"introduction-and-your-first-ensembles","chapter":"2 Introduction and your first ensembles","heading":"2 Introduction and your first ensembles","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"how-a-chicago-blizzard-led-to-the-very-unlikely-story-of-the-best-solutions-to-supervised-data","chapter":"2 Introduction and your first ensembles","heading":"2.1 How a Chicago blizzard led to the very unlikely story of the best solutions to supervised data","text":"journey advanced AI world started actual\nblizzard Chicago. might seem like Chicago never get \nblizzard, 2011, incredibly intense, \nvideo shows:https://www.youtube.com/watch?v=cPiFn52ztd8What Chicago 2011\nSnomageddon\ncreation advanced AI? Everything. ’s\nstory.time 2011 Blizzard worked Recruiter Kelly\nServices, \nworked since 1996. agreed work Kelly Services office \nFrankfort, Illinois time, though worked nearly every\nKelly Services office one time another. trip Frankfort\ninvolved daily commute office, able make best\nuse time road.manager time let know several days advance \nlarge amount snow forecast, might want \nprepared. recent forecasts large amounts snow \nChicago area amounted nothing. perfectly normal days \nChicago area, predicted storm also nothing, based\nrecent results. great example prior\nprediction transferring well current situation.morning went work normal, even look \nweather forecast. Around 2:45 pm manager came office \nsaid “Russ, need come look weather radar!”. \nwalked office, saw map winter storm \nincredibly huge. image zoomed , possible see\nseveral states. tell, massive snow storm \nbarreling Chicago, 15 minutes away \nlocation.told candidate interviewing leaving immediately,\nallowed stay. get home fast \npossible safety.storm started dropping snow trip north back home. commute\ntook around 50% longer normal due rapidly falling snow.later learned, storm forecast start Chicago area\naround 3:00 pm, finish 11:00 - 1:00 pm two days later, \nleave 17 - 19 inches snow.bad ? Even City Chicago snow plows stopped \nsnow:see forecasts looked like, check news report \nday:https://www.nbcchicago.com/news/local/blizzard-unleashes-winter-fury/2096753/turns three predictions blizzard accurate \nlevel almost seemed uncanny : Start time, accumulation, \nend time spot . first time recall ever seeing \nprediction level accuracy. idea type \npredictive accuracy even possible. level accuracy \npredicting results totally blew away. never seen anything \nlevel accuracy, now wanted know done.searched searched accuracy high \nforecast.power method—whatever —obvious . realized\nwork weather, solution method work \nincredibly broad range situations. many areas\ninclude business forecasts, production work, modeling prices, much,\nmuch . point idea accurate prediction\ndone.months later person wrote Tom\nSkilling, chief\nmeteorologist WGN TV Chicago. Tom posted answer opened \nsolution . relevant part Tom Skilling’s\nanswer \n2011 storm forecast accurate:Weather Service developed interesting “SNOWFALL ENSEMBLE\nFORECAST PROBABILITY SYSTEM” draws upon wide range snow\naccumulation forecasts whole set different computer models.\n“blending” model projections, probability snowfalls\nfalling within certain ranges becomes possible. Also, “blending”\nmultiple forecasts “smooths” sometimes huge model disparities\namounts predicted. resulting probabilities therefore\nrepresent “best case” forecast.first step. Ensembles way achieved \nextraordinary prediction accuracy.next goal figure ensembles made. looked \ninformation, became obvious ensembles used ,\nwinning entry Netflix Prize Competition:Netflix Prize\nCompetition sponsored\nNetflix create method accurately predict user ratings \nfilms. minimum winning score needed beat Netflix method\n(named Cinematch) least 10%. Several years work went \nsolving problem, results even included several published\npapers. winning solution ensemble methods beat \nCinematch results 10.09%.now clear ensembles path forward. However,\nidea make ensembles.went graduate school study data science predictive\nanalytics. degree completed 2017, Northwestern\nUniversity. However, still sure ensembles models \nbuilt, find clear methods build (except \npre-made methods, random forests). true \npackages work, nothing found \nlooking : build ensembles models general. Despite\nplaying idea looking online, able build \nensembles wanted build.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"saturday-october-15-2022-at-458-pm.-the-exact-birth-of-the-ensembles-system","chapter":"2 Introduction and your first ensembles","heading":"2.2 Saturday, October 15, 2022 at 4:58 pm. The exact birth of the Ensembles system","text":"Everything changed Saturday, October 15, 2022 4:58 pm. \nplaying various methods make ensemble, got ensemble\nworked first time. results extremely\nmodest standards, clear foundation \nbuild general solution can work extremely wide\nrange areas. journal entry:might asking know day time. \nreasonable question. ’ve keeping journal since 19 years\nold, thousands entries. soon realized \ncorrectly build ensembles, made entry, contains key\nelements make ensemble, steps just \nmoment. Notice subject line journal matches text\n.One ways improve skills keep journal, ’ll\nlooking depth chapter future chapters.\njournal use MacJournal, though \nlarge number options available market.Birth ensembles, Saturday, October 15, 2022 4:58 pm","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"here-is-what-an-ensemble-of-models-looks-like-at-the-most-basic-level-using-the-boston-housing-data-set-as-an-example","chapter":"2 Introduction and your first ensembles","heading":"2.3 Here is what an ensemble of models looks like at the most basic level, using the Boston Housing data set as an example:","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"head-of-boston-housing-data-set","chapter":"2 Introduction and your first ensembles","heading":"2.3.1 Head of Boston Housing data set","text":"start first ensemble data set numerical\nvalues. first example use Boston Housing data set, \nMASS package. Boston Housing data set controversial (\ndiscuss controversies example making\nprofessional quality reports C-Suite), now works \nwell known data set begin journey ensembles.Overview basic steps make ensemble:using Boston Housing data set, let’s look \nBoston images:","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"the-steps-to-build-your-first-ensemble-from-scratch","chapter":"2 Introduction and your first ensembles","heading":"2.4 The steps to build your first ensemble from scratch","text":"Load packages need (MASS, tree)Load packages need (MASS, tree)Load Boston Housing data set, split train (60%) \ntest (40%) sections.Load Boston Housing data set, split train (60%) \ntest (40%) sections.Create linear model fitting linear model training\ndata, make predictions Boston Housing test data. Measure\naccuracy predictions actual values.Create linear model fitting linear model training\ndata, make predictions Boston Housing test data. Measure\naccuracy predictions actual values.Create model using trees fitting tree model training\ndata, making predictions Boston Housing test data.\nMeasure accuracy predictions actual values.Create model using trees fitting tree model training\ndata, making predictions Boston Housing test data.\nMeasure accuracy predictions actual values.Make new data frame. ensemble model\npredictions. One column linear predictions, one \ntree predictions.Make new data frame. ensemble model\npredictions. One column linear predictions, one \ntree predictions.Make new column true values—true values \nBoston Housing test data setMake new column true values—true values \nBoston Housing test data setOnce new ensemble data set, ’s simply another data\nset. different many ways data set (except \nmade).new ensemble data set, ’s simply another data\nset. different many ways data set (except \nmade).Break ensemble data set train (60%) test (40%)\nsections.Break ensemble data set train (60%) test (40%)\nsections.Fit linear model ensemble training data. Make predictions\nusing testing data, measure accuracy predictions\ntest data.Fit linear model ensemble training data. Make predictions\nusing testing data, measure accuracy predictions\ntest data.Summarize results.Summarize results.suggest reading basic steps make ensemble\ncouple times, make sure familiar steps.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"building-the-first-actual-ensemble","chapter":"2 Introduction and your first ensembles","heading":"2.5 Building the first actual ensemble","text":"Load packages need (MASS, tree):Load Boston Housing data set, split train (60%) test\n(40%) sections.Create linear model fitting linear model training data,\nmake predictions Boston Housing test data. Measure \naccuracy predictions actual values.Calculate error modelThe error rate linear model 6.108005. Let’s using\ntree method.Create model using trees fitting tree model training\ndata, making predictions Boston Housing test data. Measure\naccuracy predictions actual values.Calculate error rate tree model:error rate tree model lower (better). error\nrate tree model 5.478017.","code":"\nlibrary(MASS) # for the Boston Housing data set\nlibrary(tree) # To make models using trees\nlibrary(Metrics) # To calculate error rate (root mean squared error)\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ✖ dplyr::select() masks MASS::select()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\ndf <- MASS::Boston\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\n# Let's have a quick look at the train and test sets\nhead(train)\n#>      crim zn indus chas   nox    rm  age    dis rad tax\n#> 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296\n#> 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242\n#> 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242\n#> 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222\n#> 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222\n#> 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222\n#>   ptratio  black lstat medv\n#> 1    15.3 396.90  4.98 24.0\n#> 2    17.8 396.90  9.14 21.6\n#> 3    17.8 392.83  4.03 34.7\n#> 4    18.7 394.63  2.94 33.4\n#> 5    18.7 396.90  5.33 36.2\n#> 6    18.7 394.12  5.21 28.7\nhead(test)\n#>         crim zn indus chas   nox    rm   age    dis rad tax\n#> 401 25.04610  0  18.1    0 0.693 5.987 100.0 1.5888  24 666\n#> 402 14.23620  0  18.1    0 0.693 6.343 100.0 1.5741  24 666\n#> 403  9.59571  0  18.1    0 0.693 6.404 100.0 1.6390  24 666\n#> 404 24.80170  0  18.1    0 0.693 5.349  96.0 1.7028  24 666\n#> 405 41.52920  0  18.1    0 0.693 5.531  85.4 1.6074  24 666\n#> 406 67.92080  0  18.1    0 0.693 5.683 100.0 1.4254  24 666\n#>     ptratio  black lstat medv\n#> 401    20.2 396.90 26.77  5.6\n#> 402    20.2 396.90 20.32  7.2\n#> 403    20.2 376.11 20.31 12.1\n#> 404    20.2 396.90 19.77  8.3\n#> 405    20.2 329.46 27.38  8.5\n#> 406    20.2 384.97 22.98  5.0\nBoston_lm <- lm(medv ~ ., data = train) # Fit the model to the training data\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the model predictions\nhead(Boston_lm_predictions)\n#>       401       402       403       404       405       406 \n#> 12.618507 19.785728 20.919370 13.014507  6.946392  5.123039\nBoston_linear_RMSE <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions)\nBoston_linear_RMSE\n#> [1] 6.108005\nBoston_tree <- tree(medv ~ ., data = train) # Fit the model to the training data\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\n\n# Let's have a quick look at the predictions:\nhead(Boston_tree_predictions)\n#>      401      402      403      404      405      406 \n#> 13.30769 13.30769 13.30769 13.30769 13.30769 13.30769\nBoston_tree_RMSE <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions)\nBoston_tree_RMSE\n#> [1] 5.478017"},{"path":"introduction-and-your-first-ensembles.html","id":"were-ready-to-make-our-first-ensemble","chapter":"2 Introduction and your first ensembles","heading":"2.6 We’re ready to make our first ensemble!!","text":"Make new data frame. ensemble model predictions,\none column true values. One column linear\npredictions, one tree predictions. ’ll make third\ncolumn, true values.Make new column true values—true values \nBoston Housing test data setOnce new ensemble data set, ’s simply another data set. \ndifferent many ways data set (except made).Break ensemble data set train (60%) test (40%) sections.\nnothing special 60/40 split , may use \nnumbers wish.Fit linear model ensemble training data. Make predictions using\ntesting data, measure accuracy predictions \ntest data. Notice similar linear tree models.Summarize results.Clearly ensemble lowest error rate three models. \nensemble easily best three models \nlowest error rate models.","code":"\nensemble <- data.frame(\n  'linear' = Boston_lm_predictions,\n  'tree' = Boston_tree_predictions,\n  'y' = test$medv\n)\n\n# Let's have a look at the ensemble:\nhead(ensemble)\n#>        linear     tree    y\n#> 401 12.618507 13.30769  5.6\n#> 402 19.785728 13.30769  7.2\n#> 403 20.919370 13.30769 12.1\n#> 404 13.014507 13.30769  8.3\n#> 405  6.946392 13.30769  8.5\n#> 406  5.123039 13.30769  5.0\ndim(ensemble)\n#> [1] 105   3\nensemble_train <- ensemble[1:60, ]\nensemble_test <- ensemble[61:105, ]\n\nhead(ensemble_train)\n#>        linear     tree    y\n#> 401 12.618507 13.30769  5.6\n#> 402 19.785728 13.30769  7.2\n#> 403 20.919370 13.30769 12.1\n#> 404 13.014507 13.30769  8.3\n#> 405  6.946392 13.30769  8.5\n#> 406  5.123039 13.30769  5.0\nhead(ensemble_test)\n#>       linear     tree    y\n#> 461 23.88984 13.30769 16.4\n#> 462 23.29129 13.30769 17.7\n#> 463 22.54055 21.84327 19.5\n#> 464 25.50940 21.84327 20.2\n#> 465 22.71231 21.84327 21.4\n#> 466 20.83810 21.84327 19.9\n# Fit the model to the training data\nensemble_lm <- lm(y ~ ., data = ensemble_train)\n\n# Make predictions using the model on the test data\nensemble_lm_predictions <- predict(object = ensemble_lm, newdata = ensemble_test)\n\n# Calculate error rate for the ensemble predictions\nensemble_lm_rmse <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_lm_predictions)\n\n# Report the error rate for the ensemble\nensemble_lm_rmse\n#> [1] 4.826962\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble'),\n  'Error' = c(Boston_linear_RMSE, Boston_tree_RMSE, ensemble_lm_rmse)\n)\n\nresults\n#>      Model    Error\n#> 1   Linear 6.108005\n#> 2     Tree 5.478017\n#> 3 Ensemble 4.826962"},{"path":"introduction-and-your-first-ensembles.html","id":"try-it-yourself-make-an-ensemble-where-the-ensemble-is-made-using-trees-instead-of-linear-models.","chapter":"2 Introduction and your first ensembles","heading":"2.6.1 Try it yourself: Make an ensemble where the ensemble is made using trees instead of linear models.","text":"compare three results? Let’s update \nresults table","code":"\n# Fit the model to the training data\nensemble_tree <- tree(y ~ ., data = ensemble_train)\n\n# Make predictions using the model on the test data\nensemble_tree_predict <- predict(object = ensemble_tree, newdata = ensemble_test)\n\n# Let's look at the predictions\nhead(ensemble_tree_predict)\n#>      461      462      463      464      465      466 \n#> 14.80000 14.80000 18.94286 18.94286 18.94286 18.94286\n\n# Calculate the error rate\nensemble_tree_rmse <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_tree_predict)\n\nensemble_tree_rmse\n#> [1] 5.322011\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble_Linear', 'Ensemble_Tree'),\n  'Error' = c(Boston_linear_RMSE, Boston_tree_RMSE, ensemble_lm_rmse, ensemble_tree_rmse)\n)\n\nresults <- results %>% arrange(Error)\n\nresults\n#>             Model    Error\n#> 1 Ensemble_Linear 4.826962\n#> 2   Ensemble_Tree 5.322011\n#> 3            Tree 5.478017\n#> 4          Linear 6.108005"},{"path":"introduction-and-your-first-ensembles.html","id":"both-of-the-ensemble-models-beat-both-of-the-individual-models-in-this-example","chapter":"2 Introduction and your first ensembles","heading":"2.6.2 Both of the ensemble models beat both of the individual models in this example","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"principle-what-is-one-improvement-that-can-be-made-use-a-diverse-set-of-models-and-ensembles-to-get-the-best-possible-result","chapter":"2 Introduction and your first ensembles","heading":"2.7 Principle: What is one improvement that can be made? Use a diverse set of models and ensembles to get the best possible result","text":"shall see go learn build ensembles, \nnumerical method use build 27 individual models 13\nensembles total 40 results. goal get best\npossible results, diverse set models ensembles, 40\nresults numerical data, produce much better results \nlimited number models ensembles.principal looking classification\ndata, logistic, data, time series forecasting data. use \nlarge number individual models ensembles goal \nachieving best possible result.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"principle-randomizing-the-data-before-the-analysis-will-make-the-results-more-general-and-is-very-easy-to-do","chapter":"2 Introduction and your first ensembles","heading":"2.8 Principle: Randomizing the data before the analysis will make the results more general (and is very easy to do!)","text":"","code":"\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis"},{"path":"introduction-and-your-first-ensembles.html","id":"try-it-yourself-repeat-the-previous-analysis-but-randomize-the-rows-before-the-analysis.-otherwise-keep-the-process-the-same.-share-your-results-on-social-media.","chapter":"2 Introduction and your first ensembles","heading":"2.9 Try it yourself: Repeat the previous analysis, but randomize the rows before the analysis. Otherwise keep the process the same. Share your results on social media.","text":"’ll follow exact steps, except randomizing rows\nfirst.• Randomize rows• Break data train test sets• Fit model training set• Make predictions calculate error model test setThe fact results bit different first ensemble \nuseful. gives us another solid principle use analysis\nmethods:","code":"\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis\n\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\n# Fit the model to the training data\nBoston_lm <- lm(medv ~ ., data = train)\n\n# Make predictions using the model on the test data\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the linear model predictions:\n\nhead(Boston_lm_predictions)\n#>       389        24        13       490       298       497 \n#>  7.209131 13.299431 20.369236  8.253998 18.854953 14.074187\nBoston_linear_rmse <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions)\n\nBoston_tree <- tree(medv ~ ., data = train)\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\nBoston_tree_rmse <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions)\n\n# Let's have a quick look at the tree model predictions:\n\nhead(Boston_tree_predictions)\n#>      389       24       13      490      298      497 \n#> 12.00606 16.90755 16.90755 16.90755 16.90755 16.90755\nensemble <- data.frame( 'linear' = Boston_lm_predictions, 'tree' = Boston_tree_predictions, 'y_ensemble' = test$medv )\n\nensemble <- ensemble[sample(nrow(ensemble)), ] # Randomizes the rows of the ensemble\n\nensemble_train <- ensemble[1:60, ]\nensemble_test <- ensemble[61:105, ]\nensemble_lm <- lm(y_ensemble ~ ., data = ensemble_train)\n\n# Predictions for the ensemble linear model\n\nensemble_prediction <- predict(ensemble_lm, newdata = ensemble_test)\n\n# Root mean squared error for the ensemble linear model\n\nensemble_lm_rmse <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_prediction)\n\n# Same for tree models\n\nensemble_tree <- tree(y_ensemble ~ ., data = ensemble_train)\nensemble_tree_predictions <- predict(object = ensemble_tree, newdata = ensemble_test)\nensemble_tree_rmse <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_tree_predictions)\n\nresults <- list( 'Linear' = Boston_linear_rmse, 'Trees' = Boston_tree_rmse, 'Ensembles_Linear' = ensemble_lm_rmse, 'Ensemble_Tree' = ensemble_tree_rmse )\n\nresults\n#> $Linear\n#> [1] 4.536668\n#> \n#> $Trees\n#> [1] 4.012755\n#> \n#> $Ensembles_Linear\n#> [1] 3.714215\n#> \n#> $Ensemble_Tree\n#> [1] 5.286599"},{"path":"introduction-and-your-first-ensembles.html","id":"the-more-we-can-randomize-the-data-the-more-our-results-will-match-nature","chapter":"2 Introduction and your first ensembles","heading":"2.10 The more we can randomize the data, the more our results will match nature","text":"Just watch: Repeat results 100 times, return mean results\n(hint: ’s two small changes)","code":"\nfor (i in 1:100) {\n\n# First the linear model with randomized data\n\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis\n\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\nBoston_lm <- lm(medv ~ ., data = train)\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the linear model predictions:\n\nhead(Boston_lm_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_linear_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions)\n\nBoston_linear_rmse_mean <- mean(Boston_linear_rmse)\n\n# Let's use tree models\n\nBoston_tree <- tree(medv ~ ., data = train)\n\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\n\n# Let's have a quick look at the tree model predictions:\n\nhead(Boston_tree_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_tree_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions) \nBoston_tree_rmse_mean <- mean(Boston_tree_rmse)\n\nensemble <- data.frame('linear' = Boston_lm_predictions, 'tree' = Boston_tree_predictions, 'y_ensemble' = test$medv )\n\nensemble <- ensemble[sample(nrow(ensemble)), ] # Randomizes the rows of the ensemble\n\nensemble_train <- ensemble[1:60, ]\nensemble_test <- ensemble[61:105, ]\n\n# Ensemble linear modeling\n\nensemble_lm <- lm(y_ensemble ~ ., data = ensemble_train)\n\n# Predictions for the ensemble linear model\n\nensemble_prediction <- predict(ensemble_lm, newdata = ensemble_test)\n\n# Root mean squared error for the ensemble linear model\n\nensemble_lm_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_prediction)\n\nensemble_lm_rmse_mean <- mean(ensemble_lm_rmse)\n\nensemble_tree <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree, newdata = ensemble_test) \n\nensemble_tree_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = \nensemble_tree_predictions)\n\nensemble_tree_rmse_mean <- mean(ensemble_tree_rmse)\n\nresults <- data.frame(\n  'Linear' = Boston_linear_rmse_mean,\n  'Trees' = Boston_tree_rmse_mean,\n  'Ensembles_Linear' = ensemble_lm_rmse_mean,\n  'Ensemble_Tree' = ensemble_tree_rmse_mean )\n\n}\n\nresults\n#>     Linear    Trees Ensembles_Linear Ensemble_Tree\n#> 1 4.798849 4.627021         4.078829      5.040039\nwarnings() # No warnings!"},{"path":"introduction-and-your-first-ensembles.html","id":"principle-is-this-my-very-best-work","chapter":"2 Introduction and your first ensembles","heading":"2.11 Principle: “Is this my very best work?”","text":"best work build ensembles stage skills.\ngoing make number improvements solutions see\n, final result much stronger \nfar. Always strive best work, without excuses.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"where-do-i-get-help-with-errors-or-warnings","chapter":"2 Introduction and your first ensembles","heading":"2.12 “Where do I get help with errors or warnings?”","text":"extremely useful check code returns errors \nwarnings, fix fast possible. numerous sites \nhelp address errors code:https://stackoverflow.comhttps://forum.posit.cohttps://www.r-project.org/help.html","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"is-there-an-easy-way-to-save-all-trained-models","chapter":"2 Introduction and your first ensembles","heading":"2.13 Is there an easy way to save all trained models?","text":"Absolutely! simply add code end section \nsaves four trained models (linear, tree, ensemble_linear \nensemble_tree), follows:","code":"\nlibrary(MASS)\nlibrary(Metrics)\nlibrary(tree)\n\nensemble_lm_rmse <- 0\nensemble_tree_rmse <- 0\n\nfor (i in 1:100) {\n\n# Fit the linear model with randomized data\n\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis\n\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\nBoston_lm <- lm(medv ~ ., data = train)\n\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the linear model predictions:\n\nhead(Boston_lm_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_linear_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions) \nBoston_linear_rmse_mean <- mean(Boston_linear_rmse)\n\n# Let's use tree models\n\nBoston_tree <- tree(medv ~ ., data = train)\n\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\n\n# Let's have a quick look at the tree model predictions:\n\nhead(Boston_tree_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_tree_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions) \nBoston_tree_rmse_mean <- mean(Boston_tree_rmse)\n\nensemble <- data.frame( 'linear' = Boston_lm_predictions, 'tree' = Boston_tree_predictions, 'y_ensemble' = test$medv )\n\nensemble <- ensemble[sample(nrow(ensemble)), ] # Randomizes the rows of the ensemble\n\nensemble_train <- ensemble[1:60, ]\n\nensemble_test <- ensemble[61:105, ]\n\n# Ensemble linear modeling\n\nensemble_lm <- lm(y_ensemble ~ ., data = ensemble_train)\n\n# Predictions for the ensemble linear model\n\nensemble_prediction <- predict(ensemble_lm, newdata = ensemble_test)\n\n# Root mean squared error for the ensemble linear model\n\nensemble_lm_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_prediction)\n\nensemble_lm_rmse_mean <- mean(ensemble_lm_rmse)\n\nensemble_tree <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree, newdata = ensemble_test) \n\nensemble_tree_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_tree_predictions)\n\nensemble_tree_rmse_mean <- mean(ensemble_tree_rmse)\n\nresults <- list( 'Linear' = Boston_linear_rmse_mean, 'Trees' = Boston_tree_rmse_mean, 'Ensembles_Linear' = ensemble_lm_rmse_mean, 'Ensemble_Tree' = ensemble_tree_rmse_mean )\n\n}\n\nresults\n#> $Linear\n#> [1] 4.830464\n#> \n#> $Trees\n#> [1] 4.712343\n#> \n#> $Ensembles_Linear\n#> [1] 4.151828\n#> \n#> $Ensemble_Tree\n#> [1] 5.236029\nwarnings()\n\nBoston_lm <- Boston_lm\nBoston_tree <- Boston_tree\nensemble_lm <- ensemble_lm\nensemble_tree <- ensemble_tree"},{"path":"introduction-and-your-first-ensembles.html","id":"what-about-classification-logistic-and-time-series-data","chapter":"2 Introduction and your first ensembles","heading":"2.13.1 What about classification, logistic and time series data?","text":"subsequent chapters similar processes classification,\nlogistic time series data. ’s possible build ensembles \ntypes data. results extremely similar results\n’ve seen numerical data: ensembles won’t always\nbest results, best diverse set models \nensembles get best possible results.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"principle-ensembles-can-work-with-many-types-of-data-and-we-will-do-that-in-this-book","chapter":"2 Introduction and your first ensembles","heading":"2.13.2 Principle: Ensembles can work with many types of data, and we will do that in this book","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"can-it-make-predictions-on-totally-new-data-from-the-trained-modelsincluding-the-ensembles","chapter":"2 Introduction and your first ensembles","heading":"2.13.3 Can it make predictions on totally new data from the trained models—including the ensembles?","text":"solutions book independent use data. \nlook everything housing prices business analysis HR\nanalytics research medicine. One later examples \nexactly question asking—build individual ensemble\nmodels data, use pre-trained models make predictions\ntotally unseen data. develop set skills later \nbook, ’s minor extension ’re already seen \ncompleted.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"the-way-i-was-taught-how-to-write-code-was-totally-wrong-for-me-the-best-way-for-me-is-to-start-at-the-end-and-work-backward-from-there.-do-not-start-coding-looking-for-a-solution-instead-start-with-the-ending-and-work-backwards-from-there.","chapter":"2 Introduction and your first ensembles","heading":"2.13.4 The way I was taught how to write code was totally wrong for me: The best way for me is to start at the end and work backward from there. Do not start coding looking for a solution, instead, start with the ending and work backwards from there.","text":"Start end work backwards \nthereThe biggest lesson work make ensembles.\n’ve already seen steps, results \ncome. second biggest lesson everything taught \ndata science AI backwards actually works \nreal life. ’ve learned learn, applied skill\n(learning learn) wide range skills, including:• Running multi-million dollar division Fortune 1000 company,\nincluding full profit loss responsibility• Performing professional level many musical instruments• Able communicate English, Spanish sign language \nprofessional setting• Earning #1 place annual undergradate university mathematics\ncompetition—twice• Completing Master’s degree Guidance Counseling, allowing \nhelp many people path toward healthier life• Leader Oak Park, Illinois chapter Amnesty International \nten years, helping release several Prisoners Conscience• President Chicago Apple User Group ten years, helping many\npeople extremely good work hardware software• Leg press 1,000 pounds ten times row• Climbed mountain Colorado• Completed multiple skydives (looking forward )point learned learn, ’ve applied \nskill many areas. started learning data science/AI/coding, \ndifferent way creative whole life.\nway works start end, work backward \n, never give . Maybe best evidence success \nmethod fact:started write code led Ensembles package, \nfollowed steps: Start end, work backward , \nnever give . wound writing average 1,000 lines clean,\nerror free code per month 15 months. Ensembles package around\n15,000 lines clean, error free code.found attitude much important skill set, long\nshot.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"how-i-stuck-with-it-all-the-way-to-the-end-the-best-career-advice-i-ever-received-was-from-a-homeless-man-i-never-met-and-answers-the-question-of-what-most-strongly-predicts-success.","chapter":"2 Introduction and your first ensembles","heading":"2.13.5 How I stuck with it all the way to the end: The best career advice I ever received was from a homeless man I never met, and answers the question of what most strongly predicts success.","text":"Ashford SimpsonLearning building ensembles help make accurate\npredictions. ’s extrdmely good skill setting. \nfound important thing predict success. \nstudied, quite good works subject, \nacademic general population.favorite career advice—listened nearly every day \nworked Ensembles project—man homeless \ntime came words.Nick Ashford Willow Run, Michigan. moved New York, hoping\nget entertainment world dancer. Unfortunately ended\nhomeless streets New York. slept park benches, \ngot food soup kitchens.heard people White Rock Baptist Church feed (\nhomeless man) normal meal, Nick went one Sunday morning. \nmet people, especially choir members, started working \npiano player choir. name Valerie Simpson.Soon Nick Valerie writing songs church choir. Nick\nmentioned homeless, realized New York wasn’t\ngoing “”. determined. words put say:Ain’t mountain high enoughAin’t valley low enoughAin’t river wide enoughValerie took words, set music. sent song \nMotown, released Marvin Gaye Tammy Terrell covering \nvocals. later re-done Ashford Simpson Paul Riser, \nDiana Ross singing lead.short video summarizes experience, concludes\nfinale 1970 version song. attitude \nAshford Simpson expressed song extremely highly predictive \nsuccess, matter field endeavor. found extremely\nmotivating, used overcome obstacles challenges \njourney.skill knowing learn (continue \nshare book), attitude working matter \nhigh mountain long valley wide river, gives \nkeep moving toward success, success fully\nachieved.Later look make presentations, consider \nexample level quality can done:https://www.icloud.com/iclouddrive/002bNfVreagRYCYHAZ9GyQ02w#Ain’t%5FNo%5FMountain%5FHigh%5FEnough","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"exercises","chapter":"2 Introduction and your first ensembles","heading":"2.13.6 Exercises:","text":"Find data science Genesis. data science idea totally\nexcites gets bed every day. idea leads\ncreation many ideas. biggest boldest dreams\ncan possibly . idea strong \n. , benefit use \nreceive good create.Keep journal progress. ’s much easier see results\ntime record. Set journal today (\nweek). use Github journal. journal crazy\nideas, contradictory evidence, writing frustrations \nsuccesses, inspiration, one next thing worked , \nrock solid record path success. Seeing path \ntraversed huge motivation finishing project.best add journal entries regular schedule.Make ensemble using Boston Housing data set. Model \n13 columns data, median value home (14th\ncolumn) working chapter.Start planning comprehensive project. types data\ninterested ? patterns like \ndiscover? Begin looking online now possible data sets, \nlittle basic research. examples provided get\ncloser section book.","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3 Numerical data: How to make 23 individual models, and basic skills with functions","text":"begin building skills make ensembles \nmodels numerical data. However, going much easier \nmight appear first. Let’s see can make easy \npossible.work backwards make function need: Start endWe going start ending, beginning, work\nbackwards . method much, much easier working\nforward, see throughout book. might \nlittle uncomfortable first, skill allow complete\nwork faster rate work forward.’ll use Boston Housing data set, ’ll start Bagged\nRandom Forest function. now ’re going work one\nfunction, keep everything simple. essence, going run\nlike assembly line.want ending error rate model. Virtually customer\nwork going want know, “accurate ?” ’s \nstarting point.determine model accuracy? already previous\nchapter, finding root mean squared error individual models\nensemble models. ’re going steps , \nprocess familiar .get error rate model holdout data sets (test \nvalidation), ’re going need model (Bagged Random Forest \nfirst example), fit training data, use model make\npredictions test data. can measure error \npredictions, just . steps familiar \n. , please re-read previous chapter.need complete steps? ’re going go\nbackward (little) make function allow us work \ndata set.function need? Let’s make list:data (Boston housing)data (Boston housing)Column number (14, median value property)Column number (14, median value property)Train amountTrain amountTest amountTest amountValidation amountValidation amountNumber times resampleNumber times resampleOne key steps change name target variable\ny. initial name nearly anything, method changes\nname target variable y. allows us make one small\nchange allow easiest possible solution:","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"all-our-models-will-be-structured-the-same-way-y-.-data-train","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.1 All our models will be structured the same way: y ~ ., data = train","text":"means y (target value) function \nfeatures, data set training data set. \nvariations 27 models, basic structure \n.","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"having-the-same-structure-for-all-the-models-makes-it-much-easier-to-build-debug-and-deploy-the-completed-models.","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.2 Having the same structure for all the models makes it much easier to build, debug, and deploy the completed models.","text":"need start initial values, run.One extremely nice part creating models way enormous\nefficiency gives us. Bagged Random Forest model\nworking, able use similar (identical many\ncases!) processes models (Support Vector Machines).rock solid foundation lay beginning allow us \nsmooth easy experience foundation solid use \nbuild models. models mainly almost exact\nduplicates fist example.’steps follow:Load libraryLoad librarySet initial values 0Set initial values 0Create functionCreate functionSet random resamplingSet random resamplingBreak data train testBreak data train testFit model training data, make predictions measure\nerror test dataFit model training data, make predictions measure\nerror test dataReturn resultsReturn resultsCheck errors warningsCheck errors warningsTest different data setTest different data set","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"exercise-re-read-the-steps-above-how-we-will-work-backwards-to-come-up-with-the-function-we-need.","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.3 Exercise: Re-read the steps above how we will work backwards to come up with the function we need.","text":"","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bagged-random-forest","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.4 1. Bagged Random Forest","text":"Exercise: Try : Change values train, test \nvalidation, number resamples. See change \nresult.One : Find numerical data set, make bagged random\nforest function data set. (example, may use Auto\ndata set ISLR package. need remove last column,\nvehicle name. Model mpg function features using \nBagged Random Forest function, numerical data set work).Post: Share social first results making numerical function\n(screen shot/video optional stage, learning \nlater)example, “first data science function building making\nensembles later . Got everything run, errors. #AIEnsembles”Now build remaining 22 models numerical data. \nbuilt using structure, foundation.Now know build basic function, let’s build 22 \nsets tools need make ensemble, starting bagging:","code":"\nlibrary(e1071) # will allow us to use a tuned random forest model\nlibrary(Metrics) # Will allow us to calculate the root mean squared error\nlibrary(randomForest) # To use the random forest function\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\nlibrary(tidyverse) # Amazing set of tools for data science\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::combine()  masks randomForest::combine()\n#> ✖ dplyr::filter()   masks stats::filter()\n#> ✖ dplyr::lag()      masks stats::lag()\n#> ✖ ggplot2::margin() masks randomForest::margin()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n# Set initial values to 0. The function will return an error if any of these are left out.\n\nbag_rf_holdout_RMSE <- 0\nbag_rf_holdout_RMSE_mean <- 0\nbag_rf_train_RMSE <- 0\nbag_rf_test_RMSE <- 0\nbag_rf_validation_RMSE <- 0\n\n# Define the function\n\nnumerical_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n#Set up random resampling\n\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col())\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n#Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model to the training data, make predictions on the testing data, then calculate the error rates on the testing data sets.\nbag_rf_train_fit <- e1071::tune.randomForest(x = train, y = train$y, mtry = ncol(train) - 1)\nbag_rf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict( object = bag_rf_train_fit$best.model, newdata = train))\nbag_rf_train_RMSE_mean <- mean(bag_rf_train_RMSE)\nbag_rf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict( object = bag_rf_train_fit$best.model, newdata = test))\nbag_rf_test_RMSE_mean <- mean(bag_rf_test_RMSE)\n\n# Itemize the error on the holdout data sets, and calculate the mean of the results\nbag_rf_holdout_RMSE[i] <- mean(bag_rf_test_RMSE_mean)\nbag_rf_holdout_RMSE_mean <- mean(c(bag_rf_holdout_RMSE))\n\n# These are the predictions we will need when we make the ensembles\nbag_rf_test_predict_value <- as.numeric(predict(object = bag_rf_train_fit$best.model, newdata = test))\n\n\n# Return the mean of the results to the user\n\n} # closing brace for numresamples\n  return(bag_rf_holdout_RMSE_mean)\n\n} # closing brace for numerical_1 function\n\n# Here is our first numerical function in actual use. We will use 25 resamples\n\nnumerical_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 0.3032176\nwarnings() # no warnings, the best possible result"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bagging-bootstrap-aggregating","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.5 2. Bagging (bootstrap aggregating)","text":"","code":"\nlibrary(ipred) #for the bagging function\n\n# Set initial values to 0\nbagging_train_RMSE <- 0\nbagging_test_RMSE <- 0\nbagging_validation_RMSE <- 0\nbagging_holdout_RMSE <- 0\nbagging_test_predict_value <- 0\nbagging_validation_predict_value <- 0\n\n#Create the function:\n\nbagging_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\n\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model to the training data, calculate error, make predictions on the holdout data\n\nbagging_train_fit <- ipred::bagging(formula = y ~ ., data = train)\nbagging_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bagging_train_fit, newdata = train))\nbagging_train_RMSE_mean <- mean(bagging_train_RMSE)\nbagging_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bagging_train_fit, newdata = test))\nbagging_test_RMSE_mean <- mean(bagging_test_RMSE)\nbagging_holdout_RMSE[i] <- mean(bagging_test_RMSE_mean)\nbagging_holdout_RMSE_mean <- mean(bagging_holdout_RMSE)\ny_hat_bagging <- c(bagging_test_predict_value)\n\n} # closing braces for the resampling function\n  return(bagging_holdout_RMSE_mean)\n  \n} # closing braces for the bagging function\n\n# Test the function:\nbagging_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.20, numresamples = 25)\n#> [1] 3.951048\nwarnings() # no warnings"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bayesglm","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.6 3. BayesGLM","text":"","code":"\nlibrary(arm) # to use bayesglm function\n#> Loading required package: MASS\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russellconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\n\n# Set initial values to 0\nbayesglm_train_RMSE <- 0\nbayesglm_test_RMSE <- 0\nbayesglm_validation_RMSE <- 0\nbayesglm_holdout_RMSE <- 0\nbayesglm_test_predict_value <- 0\nbayesglm_validation_predict_value <- 0\n\n# Create the function:\nbayesglm_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n#Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n#Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = gaussian(link = \"identity\"))\nbayesglm_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesglm_train_fit, newdata = train))\nbayesglm_train_RMSE_mean <- mean(bayesglm_train_RMSE)\nbayesglm_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesglm_train_fit, newdata = test))\nbayesglm_test_RMSE_mean <- mean(bayesglm_test_RMSE) \ny_hat_bayesglm <- c(bayesglm_test_predict_value)\n\n} # closing braces for resampling\n  return(bayesglm_test_RMSE_mean)\n  \n} # closing braces for the function\n\nbayesglm_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.20, numresamples = 25)\n#> [1] 4.826467\nwarnings() # no warnings"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bayesrnn","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.7 4. BayesRNN","text":"","code":"\nlibrary(brnn) # so we can use the BayesRNN function\n#> Loading required package: Formula\n#> Loading required package: truncnorm\n\n#Set initial values to 0\n\nbayesrnn_train_RMSE <- 0\nbayesrnn_test_RMSE <- 0\nbayesrnn_validation_RMSE <- 0\nbayesrnn_holdout_RMSE <- 0\nbayesrnn_test_predict_value <- 0\nbayesrnn_validation_predict_value <- 0\n\n# Create the function:\n\nbayesrnn_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model on the training data, make predictions on the testing data\nbayesrnn_train_fit <- brnn::brnn(x = as.matrix(train), y = train$y)\nbayesrnn_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesrnn_train_fit, newdata = train))\nbayesrnn_train_RMSE_mean <- mean(bayesrnn_train_RMSE)\nbayesrnn_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesrnn_train_fit, newdata = test))\nbayesrnn_test_RMSE_mean <- mean(bayesrnn_test_RMSE)\n\ny_hat_bayesrnn <- c(bayesrnn_test_predict_value)\n\n} # Closing brace for number of resamples \n  return(bayesrnn_test_RMSE_mean)\n\n} # Closing brace for the function\n\nbayesrnn_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015979 \n#> gamma= 30.9135    alpha= 4.8006   beta= 20224.62 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015323 \n#> gamma= 31.1033    alpha= 5.5126   beta= 15023.08 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7014854 \n#> gamma= 31.076     alpha= 3.9929   beta= 23622.04 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015874 \n#> gamma= 31.6271    alpha= 5.7396   beta= 13894.38 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016138 \n#> gamma= 31.398     alpha= 4.8438   beta= 17147.03 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015669 \n#> gamma= 30.6412    alpha= 2.7592   beta= 18760.23 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016868 \n#> gamma= 31.0864    alpha= 4.0625   beta= 27297.18 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016809 \n#> gamma= 31.1076    alpha= 4.011    beta= 16655.02 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015619 \n#> gamma= 30.6972    alpha= 3.5254   beta= 60325.72 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015227 \n#> gamma= 30.1146    alpha= 2.9039   beta= 17425.96 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7014854 \n#> gamma= 31.4493    alpha= 5.3886   beta= 15882.44 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015179 \n#> gamma= 30.9071    alpha= 2.7797   beta= 15990.24 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7017045 \n#> gamma= 31.2656    alpha= 4.6721   beta= 14287.39 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015469 \n#> gamma= 31.4396    alpha= 3.8918   beta= 32878.72 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016138 \n#> gamma= 31.6352    alpha= 4.1275   beta= 13990.78 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.701542 \n#> gamma= 31.5509    alpha= 5.3686   beta= 17132.37 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015823 \n#> gamma= 30.621     alpha= 4.5094   beta= 38520.84 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015519 \n#> gamma= 31.125     alpha= 4.393    beta= 14133.96 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016636 \n#> gamma= 30.6163    alpha= 5.3253   beta= 13300.03 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015979 \n#> gamma= 30.5402    alpha= 5.0743   beta= 20441.23 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015569 \n#> gamma= 30.5387    alpha= 5.1334   beta= 16952.94 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016192 \n#> gamma= 31.4644    alpha= 5.4873   beta= 15997.12 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016085 \n#> gamma= 31.4464    alpha= 5.1745   beta= 15916.15 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015619 \n#> gamma= 31.1062    alpha= 5.1507   beta= 16418.65 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015085 \n#> gamma= 30.9124    alpha= 3.9584   beta= 48498.02\n#> [1] 0.1362577\n\nwarnings() # no warnings for BayesRNN function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"boosted-random-forest","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.8 5. Boosted Random Forest","text":"","code":"\nlibrary(e1071)\nlibrary(randomForest)\nlibrary(tidyverse)\n\n#Set initial values to 0\nboost_rf_train_RMSE <- 0\nboost_rf_test_RMSE <- 0\nboost_rf_validation_RMSE <- 0\nboost_rf_holdout_RMSE <- 0\nboost_rf_test_predict_value <- 0\nboost_rf_validation_predict_value <- 0\n\n#Create the function:\nboost_rf_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n#Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit boosted random forest model on the training data, make predictions on holdout data\n\nboost_rf_train_fit <- e1071::tune.randomForest(x = train, y = train$y, mtry = ncol(train) - 1)\nboost_rf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict( object = boost_rf_train_fit$best.model, newdata = train\n  ))\nboost_rf_train_RMSE_mean <- mean(boost_rf_train_RMSE)\nboost_rf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict( object = boost_rf_train_fit$best.model, newdata = test\n  ))\nboost_rf_test_RMSE_mean <- mean(boost_rf_test_RMSE)\n\n} # closing brace for numresamples\n  return(boost_rf_test_RMSE_mean)\n  \n} # closing brace for the function\n\nboost_rf_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 0.3119975\nwarnings() # no warnings for Boosted Random Forest function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"cubist","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.9 6. Cubist","text":"","code":"\nlibrary(Cubist)\n#> Loading required package: lattice\nlibrary(tidyverse)\n\n# Set initial values to 0\n\ncubist_train_RMSE <- 0\ncubist_test_RMSE <- 0\ncubist_validation_RMSE <- 0\ncubist_holdout_RMSE <- 0\ncubist_test_predict_value <- 0\n\n# Create the function:\n\ncubist_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model on the training data, make predictions on the holdout data\ncubist_train_fit <- Cubist::cubist(x = train[, 1:ncol(train) - 1], y = train$y)\ncubist_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = cubist_train_fit, newdata = train))\ncubist_train_RMSE_mean <- mean(cubist_train_RMSE)\ncubist_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = cubist_train_fit, newdata = test))\ncubist_test_RMSE_mean <- mean(cubist_test_RMSE)\n\n} # closing braces for numresamples\n  return(cubist_test_RMSE_mean)\n  \n} # closing braces for the function\n\ncubist_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.40312\nwarnings() # no warnings for individual cubist function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"elastic","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.10 7. Elastic","text":"","code":"\n\nlibrary(glmnet) # So we can run the elastic model\n#> Loaded glmnet 4.1-8\nlibrary(tidyverse)\n\n# Set initial values to 0\n\nelastic_train_RMSE <- 0\nelastic_test_RMSE <- 0\nelastic_validation_RMSE <- 0\nelastic_holdout_RMSE <- 0\nelastic_test_predict_value <- 0\nelastic_validation_predict_value <- 0\nelastic_test_RMSE <- 0\nelastic_test_RMSE_df <- data.frame(elastic_test_RMSE)\nelastic_validation_RMSE <- 0\nelastic_validation_RMSE_df <- data.frame(elastic_validation_RMSE)\nelastic_holdout_RMSE <- 0\nelastic_holdout_RMSE_df <- data.frame(elastic_holdout_RMSE)\n\n# Create the function:\nelastic_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\n# Set up the elastic model\n\ny <- train$y\nx <- data.matrix(train %>% dplyr::select(-y))\nelastic_model <- glmnet::glmnet(x, y, alpha = 0.5)\nelastic_cv <- cv.glmnet(x, y, alpha = 0.5)\nbest_elastic_lambda <- elastic_cv$lambda.min\nbest_elastic_model <- glmnet::glmnet(x, y, alpha = 0, lambda = best_elastic_lambda)\nelastic_test_pred <- predict(best_elastic_model, s = best_elastic_lambda, newx = data.matrix(test %>% dplyr::select(-y)))\n\nelastic_test_RMSE <- Metrics::rmse(actual = test$y, predicted = elastic_test_pred)\nelastic_test_RMSE_df <- rbind(elastic_test_RMSE_df, elastic_test_RMSE)\nelastic_test_RMSE_mean <- mean(elastic_test_RMSE_df$elastic_test_RMSE[2:nrow(elastic_test_RMSE_df)])\n\nelastic_holdout_RMSE <- mean(elastic_test_RMSE_mean)\nelastic_holdout_RMSE_df <- rbind(elastic_holdout_RMSE_df, elastic_holdout_RMSE)\nelastic_holdout_RMSE_mean <- mean(elastic_holdout_RMSE_df$elastic_holdout_RMSE[2:nrow(elastic_holdout_RMSE_df)])\n\n} # closing brace for numresample\n  return(elastic_holdout_RMSE_mean)\n  \n} # closing brace for the elastic function\n\nelastic_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.97869\nwarnings() # no warnings for individual elastic function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"generalized-additive-models-with-smoothing-splines","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.11 8. Generalized Additive Models with smoothing splines","text":"","code":"\nlibrary(gam) # for fitting generalized additive models\n#> Loading required package: splines\n#> Loading required package: foreach\n#> \n#> Attaching package: 'foreach'\n#> The following objects are masked from 'package:purrr':\n#> \n#>     accumulate, when\n#> Loaded gam 1.22-3\n\n# Set initial values to 0\n\ngam_train_RMSE <- 0\ngam_test_RMSE <- 0\ngam_holdout_RMSE <- 0\ngam_test_predict_value <- 0\n\n# Create the function:\ngam1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Set up random resampling\n\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\n# Set up to fit the model on the training data\n\nn_unique_vals <- purrr::map_dbl(df, dplyr::n_distinct)\n\n# Names of columns with >= 4 unique vals\nkeep <- names(n_unique_vals)[n_unique_vals >= 4]\n\ngam_data <- df %>% dplyr::select(dplyr::all_of(keep))\n\n# Model data\n\ntrain1 <- train %>% dplyr::select(dplyr::all_of(keep))\n\ntest1 <- test %>% dplyr::select(dplyr::all_of(keep))\n\nnames_df <- names(gam_data[, 1:ncol(gam_data) - 1])\nf2 <- stats::as.formula(paste0(\"y ~\", paste0(\"gam::s(\", names_df, \")\", collapse = \"+\")))\n\ngam_train_fit <- gam::gam(f2, data = train1)\ngam_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = gam_train_fit, newdata = train))\ngam_train_RMSE_mean <- mean(gam_train_RMSE)\ngam_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = gam_train_fit, newdata = test))\ngam_test_RMSE_mean <- mean(gam_test_RMSE)\ngam_holdout_RMSE[i] <- mean(gam_test_RMSE_mean)\ngam_holdout_RMSE_mean <- mean(gam_holdout_RMSE)\n\n} # closing braces for numresamples\n  return(gam_holdout_RMSE_mean)\n  \n} # closing braces for gam function\n\ngam1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.755033\nwarnings() # no warnings for individual gam function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"gradient-boosted","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.12 9. Gradient Boosted","text":"","code":"\nlibrary(gbm) # to allow use of gradient boosted models\n#> Loaded gbm 2.1.9\n#> This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\n# Set initial values to 0\ngb_train_RMSE <- 0\ngb_test_RMSE <- 0\ngb_validation_RMSE <- 0\ngb_holdout_RMSE <- 0\ngb_test_predict_value <- 0\ngb_validation_predict_value <- 0\n\ngb1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\ngb_train_fit <- gbm::gbm(train$y ~ ., data = train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\ngb_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = gb_train_fit, newdata = train))\ngb_train_RMSE_mean <- mean(gb_train_RMSE)\ngb_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = gb_train_fit, newdata = test))\ngb_test_RMSE_mean <- mean(gb_test_RMSE)\n\n} # closing brace for numresamples\n  return(gb_test_RMSE_mean)\n  \n} # closing brace for gb1 function\n\ngb1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> Using 100 trees...\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> [1] 3.370866\nwarnings() # no warnings for individual gradient boosted function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"k-nearest-neighbors-tuned","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.13 10. K-Nearest Neighbors (tuned)","text":"","code":"\n\nlibrary(e1071)\n\n# Set initial values to 0\nknn_train_RMSE <- 0\nknn_test_RMSE <- 0\nknn_validation_RMSE <- 0\nknn_holdout_RMSE <- 0\nknn_test_predict_value <- 0\nknn_validation_predict_value <- 0\n\nknn1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nknn_train_fit <- e1071::tune.gknn(x = train[, 1:ncol(train) - 1], y = train$y, scale = TRUE, k = c(1:25))\nknn_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict( object = knn_train_fit$best.model,\n    newdata = train[, 1:ncol(train) - 1], k = knn_train_fit$best_model$k))\nknn_train_RMSE_mean <- mean(knn_train_RMSE)\nknn_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict( object = knn_train_fit$best.model,\n    k = knn_train_fit$best_model$k, newdata = test[, 1:ncol(test) - 1]))\nknn_test_RMSE_mean <- mean(knn_test_RMSE)\nknn_holdout_RMSE[i] <- mean(c(knn_test_RMSE_mean))\nknn_holdout_RMSE_mean <- mean(knn_holdout_RMSE)\n\n} # closing brace for numresamples\n  return(knn_holdout_RMSE_mean)\n  \n} # closing brace for knn1 function\n\nknn1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 6.898357\nwarnings() # no warnings for individual knn function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"lasso","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.14 11. Lasso","text":"","code":"\nlibrary(glmnet) # So we can run the lasso model\n\n# Set initial values to 0\n\nlasso_train_RMSE <- 0\nlasso_test_RMSE <- 0\nlasso_validation_RMSE <- 0\nlasso_holdout_RMSE <- 0\nlasso_test_predict_value <- 0\nlasso_validation_predict_value <- 0\nlasso_test_RMSE <- 0\nlasso_test_RMSE_df <- data.frame(lasso_test_RMSE)\nlasso_validation_RMSE <- 0\nlasso_validation_RMSE_df <- data.frame(lasso_validation_RMSE)\nlasso_holdout_RMSE <- 0\nlasso_holdout_RMSE_df <- data.frame(lasso_holdout_RMSE)\n\n# Create the function:\nlasso_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Set up the lasso model\ny <- train$y\nx <- data.matrix(train %>% dplyr::select(-y))\nlasso_model <- glmnet::glmnet(x, y, alpha = 1.0)\nlasso_cv <- cv.glmnet(x, y, alpha = 1.0)\nbest_lasso_lambda <- lasso_cv$lambda.min\nbest_lasso_model <- glmnet::glmnet(x, y, alpha = 0, lambda = best_lasso_lambda)\nlasso_test_pred <- predict(best_lasso_model, s = best_lasso_lambda, newx = data.matrix(test %>% dplyr::select(-y)))\n\nlasso_test_RMSE <- Metrics::rmse(actual = test$y, predicted = lasso_test_pred)\nlasso_test_RMSE_df <- rbind(lasso_test_RMSE_df, lasso_test_RMSE)\nlasso_test_RMSE_mean <- mean(lasso_test_RMSE_df$lasso_test_RMSE[2:nrow(lasso_test_RMSE_df)])\n\nlasso_holdout_RMSE <- mean(lasso_test_RMSE_mean)\nlasso_holdout_RMSE_df <- rbind(lasso_holdout_RMSE_df, lasso_holdout_RMSE)\nlasso_holdout_RMSE_mean <- mean(lasso_holdout_RMSE_df$lasso_holdout_RMSE[2:nrow(lasso_holdout_RMSE_df)])\n\n} # closing brace for numresample\n  return(lasso_holdout_RMSE_mean)\n  \n} # closing brace for the lasso_1 function\n\nlasso_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 5.001568\nwarnings() # no warnings for individual lasso function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"linear-tuned","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.15 12. Linear (tuned)","text":"","code":"\n\nlibrary(e1071) # for tuned linear models\n\n# Set initial values to 0\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_holdout_RMSE <- 0\n\n# Set up the function\nlinear1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nlinear_train_fit <- e1071::tune.rpart(formula = y ~ ., data = train)\nlinear_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = linear_train_fit$best.model, newdata = train))\nlinear_train_RMSE_mean <- mean(linear_train_RMSE)\nlinear_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = linear_train_fit$best.model, newdata = test))\nlinear_holdout_RMSE_mean <- mean(linear_test_RMSE)\n\n} # closing brace for numresamples\n  return(linear_holdout_RMSE_mean)\n  \n} # closing brace for linear1 function\n\nlinear1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.607703\nwarnings() # no warnings for individual lasso function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"lqs","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.16 13. LQS","text":"","code":"\n\nlibrary(MASS) # to allow us to run LQS models\n\n# Set initial values to 0\n\nlqs_train_RMSE <- 0\nlqs_test_RMSE <- 0\nlqs_validation_RMSE <- 0\nlqs_holdout_RMSE <- 0\nlqs_test_predict_value <- 0\nlqs_validation_predict_value <- 0\n\nlqs1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nlqs_train_fit <- MASS::lqs(train$y ~ ., data = train)\nlqs_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = lqs_train_fit, newdata = train))\nlqs_train_RMSE_mean <- mean(lqs_train_RMSE)\nlqs_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = lqs_train_fit, newdata = test))\nlqs_test_RMSE_mean <- mean(lqs_test_RMSE)\n\ny_hat_lqs <- c(lqs_test_predict_value, lqs_validation_predict_value)\n\n} # Closing brace for numresamples\n    return(lqs_test_RMSE_mean)\n\n} # Closing brace for lqs1 function\n\nlqs1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 6.838138\nwarnings() # no warnings for individual lqs function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"neuralnet","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.17 14. Neuralnet","text":"","code":"\nlibrary(neuralnet)\n#> \n#> Attaching package: 'neuralnet'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     compute\n\n#Set initial values to 0\n\nneuralnet_train_RMSE <- 0\nneuralnet_test_RMSE <- 0\nneuralnet_validation_RMSE <- 0\nneuralnet_holdout_RMSE <- 0\nneuralnet_test_predict_value <- 0\nneuralnet_validation_predict_value <- 0\n\n# Fit the model to the training data\nneuralnet1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test data sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nmaxs <- apply(df, 2, max)\nmins <- apply(df, 2, min)\nscaled <- as.data.frame(scale(df, center = mins, scale = maxs - mins))\ntrain_ <- scaled[idx == 1, ]\ntest_ <- scaled[idx == 2, ]\nn <- names(train_)\nf <- as.formula(paste(\"y ~\", paste(n[!n %in% \"y\"], collapse = \" + \")))\nnn <- neuralnet(f, data = train_, hidden = c(5, 3), linear.output = TRUE)\npredict_test_nn <- neuralnet::compute(nn, test_[, 1:ncol(df) - 1])\npredict_test_nn_ <- predict_test_nn$net.result * (max(df$y) - min(df$y)) + min(df$y)\npredict_train_nn <- neuralnet::compute(nn, train_[, 1:ncol(df) - 1])\npredict_train_nn_ <- predict_train_nn$net.result * (max(df$y) - min(df$y)) + min(df$y)\nneuralnet_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict_train_nn_)\nneuralnet_train_RMSE_mean <- mean(neuralnet_train_RMSE)\nneuralnet_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict_test_nn_)\nneuralnet_test_RMSE_mean <- mean(neuralnet_test_RMSE)\n\nneuralnet_holdout_RMSE[i] <- mean(c(neuralnet_test_RMSE))\nneuralnet_holdout_RMSE_mean <- mean(neuralnet_holdout_RMSE)\n\n} # Closing brace for numresamples\n  return(neuralnet_holdout_RMSE_mean)\n  \n} # closing brace for neuralnet1 function\n\nneuralnet1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 3.943471\nwarnings() # no warnings for individual neuralnet function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"partial-least-squares","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.18 15. Partial Least Squares","text":"","code":"\n\nlibrary(pls)\n#> \n#> Attaching package: 'pls'\n#> The following objects are masked from 'package:arm':\n#> \n#>     coefplot, corrplot\n#> The following object is masked from 'package:stats':\n#> \n#>     loadings\n\n# Set initial values to 0\npls_train_RMSE <- 0\npls_test_RMSE <- 0\npls_validation_RMSE <- 0\npls_holdout_RMSE <- 0\npls_test_predict_value <- 0\npls_validation_predict_value <- 0\n\npls1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\npls_train_fit <- pls::plsr(train$y ~ ., data = train)\npls_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = pls_train_fit, newdata = train))\npls_train_RMSE_mean <- mean(pls_train_RMSE)\npls_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = pls_train_fit, newdata = test))\npls_test_RMSE_mean <- mean(pls_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return( pls_test_RMSE_mean)\n  \n} # Closing brace for pls1 function\n\npls1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 6.068721\nwarnings() # no warnings for individual pls function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"principal-components-regression","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.19 16. Principal Components Regression","text":"","code":"\n\nlibrary(pls) # To run pcr models\n\n#Set initial values to 0\npcr_train_RMSE <- 0\npcr_test_RMSE <- 0\npcr_validation_RMSE <- 0\npcr_holdout_RMSE <- 0\npcr_test_predict_value <- 0\npcr_validation_predict_value <- 0\n\npcr1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\npcr_train_fit <- pls::pcr(train$y ~ ., data = train)\npcr_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = pcr_train_fit, newdata = train))\npcr_train_RMSE_mean <- mean(pcr_train_RMSE)\npcr_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = pcr_train_fit, newdata = test))\npcr_test_RMSE_mean <- mean(pcr_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return(pcr_test_RMSE_mean)\n  \n} # Closing brace for PCR function\n\npcr1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 6.527915\nwarnings() # no warnings for individual pls function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"random-forest","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.20 17. Random Forest","text":"","code":"\nlibrary(randomForest)\n\n# Set initial values to 0\nrf_train_RMSE <- 0\nrf_test_RMSE <- 0\nrf_validation_RMSE <- 0\nrf_holdout_RMSE <- 0\nrf_test_predict_value <- 0\nrf_validation_predict_value <- 0\n\n# Set up the function\nrf1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nrf_train_fit <- tune.randomForest(x = train, y = train$y, data = train)\nrf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = rf_train_fit$best.model, newdata = train))\nrf_train_RMSE_mean <- mean(rf_train_RMSE)\nrf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = rf_train_fit$best.model, newdata = test))\nrf_test_RMSE_mean <- mean(rf_test_RMSE)\n\n} # Closing brace for numresamples loop\nreturn(rf_test_RMSE_mean)\n  \n} # Closing brace for rf1 function\n\nrf1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 1.818037\nwarnings() # no warnings for individual random forest function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"ridge-regression","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.21 18. Ridge Regression","text":"","code":"\n\nlibrary(glmnet) # So we can run the ridge model\n\n# Set initial values to 0\nridge_train_RMSE <- 0\nridge_test_RMSE <- 0\nridge_validation_RMSE <- 0\nridge_holdout_RMSE <- 0\nridge_test_predict_value <- 0\nridge_validation_predict_value <- 0\nridge_test_RMSE <- 0\nridge_test_RMSE_df <- data.frame(ridge_test_RMSE)\nridge_validation_RMSE <- 0\nridge_validation_RMSE_df <- data.frame(ridge_validation_RMSE)\nridge_holdout_RMSE <- 0\nridge_holdout_RMSE_df <- data.frame(ridge_holdout_RMSE)\n\n# Create the function:\nridge1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Set up the ridge model\ny <- train$y\nx <- data.matrix(train %>% dplyr::select(-y))\nridge_model <- glmnet::glmnet(x, y, alpha = 0)\nridge_cv <- cv.glmnet(x, y, alpha = 0)\nbest_ridge_lambda <- ridge_cv$lambda.min\nbest_ridge_model <- glmnet::glmnet(x, y, alpha = 0, lambda = best_ridge_lambda)\nridge_test_pred <- predict(best_ridge_model, s = best_ridge_lambda, newx = data.matrix(test %>% dplyr::select(-y)))\n\nridge_test_RMSE <- Metrics::rmse(actual = test$y, predicted = ridge_test_pred)\nridge_test_RMSE_df <- rbind(ridge_test_RMSE_df, ridge_test_RMSE)\nridge_test_RMSE_mean <- mean(ridge_test_RMSE_df$ridge_test_RMSE[2:nrow(ridge_test_RMSE_df)])\n\nridge_holdout_RMSE <- mean(ridge_test_RMSE_mean)\nridge_holdout_RMSE_df <- rbind(ridge_holdout_RMSE_df, ridge_holdout_RMSE)\nridge_holdout_RMSE_mean <- mean(ridge_holdout_RMSE_df$ridge_holdout_RMSE[2:nrow(ridge_holdout_RMSE_df)])\n\n} # closing brace for numresample\n  return(ridge_holdout_RMSE_mean)\n  \n} # closing brace for the ridge function\n\nridge1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 5.09164\nwarnings() # no warnings for individual ridge function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"robust-regression","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.22 19. Robust Regression","text":"","code":"\n\nlibrary(MASS) # To run rlm function for robust regression\n\n# Set initial values to 0\nrobust_train_RMSE <- 0\nrobust_test_RMSE <- 0\nrobust_validation_RMSE <- 0\nrobust_holdout_RMSE <- 0\nrobust_test_predict_value <- 0\nrobust_validation_predict_value <- 0\n\n# Make the function\nrobust1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nrobust_train_fit <- MASS::rlm(x = train[, 1:ncol(df) - 1], y = train$y)\nrobust_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = robust_train_fit$fitted.values)\nrobust_train_RMSE_mean <- mean(robust_train_RMSE)\nrobust_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = MASS::rlm(y ~ ., data = train), newdata = test))\nrobust_test_RMSE_mean <- mean(robust_test_RMSE) \n\n} # Closing brace for numresamples loop\nreturn(robust_test_RMSE_mean)\n  \n} # Closing brace for robust1 function\n\nrobust1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.950721\nwarnings() # no warnings for individual robust function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"rpart","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.23 20. Rpart","text":"","code":"\n\nlibrary(rpart)\n\n# Set initial values to 0\nrpart_train_RMSE <- 0\nrpart_test_RMSE <- 0\nrpart_validation_RMSE <- 0\nrpart_holdout_RMSE <- 0\nrpart_test_predict_value <- 0\nrpart_validation_predict_value <- 0\n\n# Make the function\nrpart1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nrpart_train_fit <- rpart::rpart(train$y ~ ., data = train)\nrpart_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = rpart_train_fit, newdata = train))\nrpart_train_RMSE_mean <- mean(rpart_train_RMSE)\nrpart_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = rpart_train_fit, newdata = test))\nrpart_test_RMSE_mean <- mean(rpart_test_RMSE)\n\n} # Closing loop for numresamples\nreturn(rpart_test_RMSE_mean)\n  \n} # Closing brace for rpart1 function\n\nrpart1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.899757\nwarnings() # no warnings for individual rpart function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"support-vector-machines","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.24 21. Support Vector Machines","text":"","code":"\n\nlibrary(e1071)\n\n# Set initial values to 0\nsvm_train_RMSE <- 0\nsvm_test_RMSE <- 0\nsvm_validation_RMSE <- 0\nsvm_holdout_RMSE <- 0\nsvm_test_predict_value <- 0\nsvm_validation_predict_value <- 0\n\n# Make the function\nsvm1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nsvm_train_fit <- e1071::tune.svm(x = train, y = train$y, data = train)\nsvm_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = svm_train_fit$best.model, newdata = train))\nsvm_train_RMSE_mean <- mean(svm_train_RMSE)\nsvm_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = svm_train_fit$best.model, newdata = test))\nsvm_test_RMSE_mean <- mean(svm_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return(svm_test_RMSE_mean)\n\n} # Closing brace for svm1 function\n\nsvm1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 2.3304\nwarnings() # no warnings for individual Support Vector Machines function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"trees","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.25 22. Trees","text":"","code":"\n\nlibrary(tree)\n\n# Set initial values to 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_validation_RMSE <- 0\ntree_holdout_RMSE <- 0\ntree_test_predict_value <- 0\ntree_validation_predict_value <- 0\n\n# Make the function\ntree1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\ntree_train_fit <- tree::tree(train$y ~ ., data = train)\ntree_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = tree_train_fit, newdata = train))\ntree_train_RMSE_mean <- mean(tree_train_RMSE)\ntree_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = tree_train_fit, newdata = test))\ntree_test_RMSE_mean <- mean(tree_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return(tree_test_RMSE_mean)\n  \n} # Closing brace for tree1 function\n\ntree1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.803725\nwarnings() # no warnings for individual tree function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"xgboost","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.26 23. XGBoost","text":"","code":"\nlibrary(xgboost)\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\n\n# Set initial values to 0\nxgb_train_RMSE <- 0\nxgb_test_RMSE <- 0\nxgb_validation_RMSE <- 0\nxgb_holdout_RMSE <- 0\nxgb_test_predict_value <- 0\nxgb_validation_predict_value <- 0\n\n# Create the function\nxgb1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\ntrain_x <- data.matrix(train[, -ncol(train)])\ntrain_y <- train[, ncol(train)]\n\n# define predictor and response variables in test set\ntest_x <- data.matrix(test[, -ncol(test)])\ntest_y <- test[, ncol(test)]\n\n# define final train, test and validation sets\n\nxgb_train <- xgboost::xgb.DMatrix(data = train_x, label = train_y)\nxgb_test <- xgboost::xgb.DMatrix(data = test_x, label = test_y)\n\n# define watchlist\nwatchlist <- list(train = xgb_train)\nwatchlist_test <- list(train = xgb_train, test = xgb_test)\n\n# fit XGBoost model and display training and validation data at each round\nxgb_model <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_test, nrounds = 70)\n\nxgboost_min <- which.min(xgb_model$evaluation_log$validation_rmse)\n\nxgb_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = xgb_model, newdata = train_x))\nxgb_train_RMSE_mean <- mean(xgb_train_RMSE)\nxgb_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = xgb_model, newdata = test_x))\nxgb_test_RMSE_mean <- mean(xgb_test_RMSE)\n\nxgb_holdout_RMSE[i] <- mean(xgb_test_RMSE_mean)\nxgb_holdout_RMSE_mean <- mean(xgb_holdout_RMSE)\n\n} # Closing brace for numresamples loop\n  return(xgb_holdout_RMSE_mean)\n  \n} # Closing brace for xgb1 function\n\nxgb1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1]  train-rmse:17.106138    test-rmse:17.366893 \n#> [2]  train-rmse:12.381786    test-rmse:12.791063 \n#> [3]  train-rmse:9.054376 test-rmse:9.622087 \n#> [4]  train-rmse:6.732554 test-rmse:7.565010 \n#> [5]  train-rmse:5.149285 test-rmse:6.266702 \n#> [6]  train-rmse:4.076553 test-rmse:5.494029 \n#> [7]  train-rmse:3.324973 test-rmse:4.903619 \n#> [8]  train-rmse:2.847707 test-rmse:4.542260 \n#> [9]  train-rmse:2.530861 test-rmse:4.413045 \n#> [10] train-rmse:2.307846 test-rmse:4.231765 \n#> [11] train-rmse:2.152082 test-rmse:4.184429 \n#> [12] train-rmse:2.055640 test-rmse:4.161266 \n#> [13] train-rmse:1.946959 test-rmse:4.136018 \n#> [14] train-rmse:1.869913 test-rmse:4.143107 \n#> [15] train-rmse:1.805064 test-rmse:4.058231 \n#> [16] train-rmse:1.758146 test-rmse:4.040621 \n#> [17] train-rmse:1.713256 test-rmse:3.986970 \n#> [18] train-rmse:1.681229 test-rmse:3.985656 \n#> [19] train-rmse:1.620191 test-rmse:3.996229 \n#> [20] train-rmse:1.564413 test-rmse:3.989849 \n#> [21] train-rmse:1.533410 test-rmse:3.992445 \n#> [22] train-rmse:1.512751 test-rmse:3.997344 \n#> [23] train-rmse:1.473980 test-rmse:3.991080 \n#> [24] train-rmse:1.450180 test-rmse:3.959146 \n#> [25] train-rmse:1.419758 test-rmse:3.971367 \n#> [26] train-rmse:1.381709 test-rmse:3.972488 \n#> [27] train-rmse:1.341355 test-rmse:3.967456 \n#> [28] train-rmse:1.327296 test-rmse:3.967204 \n#> [29] train-rmse:1.315809 test-rmse:3.963369 \n#> [30] train-rmse:1.276372 test-rmse:3.939496 \n#> [31] train-rmse:1.245258 test-rmse:3.941929 \n#> [32] train-rmse:1.227539 test-rmse:3.943494 \n#> [33] train-rmse:1.205570 test-rmse:3.944429 \n#> [34] train-rmse:1.178578 test-rmse:3.908206 \n#> [35] train-rmse:1.147782 test-rmse:3.890774 \n#> [36] train-rmse:1.130770 test-rmse:3.895550 \n#> [37] train-rmse:1.110949 test-rmse:3.892502 \n#> [38] train-rmse:1.086514 test-rmse:3.894933 \n#> [39] train-rmse:1.059807 test-rmse:3.895144 \n#> [40] train-rmse:1.037346 test-rmse:3.899691 \n#> [41] train-rmse:1.027192 test-rmse:3.906265 \n#> [42] train-rmse:1.008947 test-rmse:3.906913 \n#> [43] train-rmse:0.982172 test-rmse:3.895650 \n#> [44] train-rmse:0.955719 test-rmse:3.886293 \n#> [45] train-rmse:0.934445 test-rmse:3.885709 \n#> [46] train-rmse:0.922770 test-rmse:3.883157 \n#> [47] train-rmse:0.904506 test-rmse:3.880058 \n#> [48] train-rmse:0.888704 test-rmse:3.877216 \n#> [49] train-rmse:0.873289 test-rmse:3.881064 \n#> [50] train-rmse:0.863270 test-rmse:3.870378 \n#> [51] train-rmse:0.856263 test-rmse:3.870180 \n#> [52] train-rmse:0.849087 test-rmse:3.871422 \n#> [53] train-rmse:0.831005 test-rmse:3.864546 \n#> [54] train-rmse:0.812538 test-rmse:3.863428 \n#> [55] train-rmse:0.803447 test-rmse:3.860963 \n#> [56] train-rmse:0.792266 test-rmse:3.852648 \n#> [57] train-rmse:0.783392 test-rmse:3.848370 \n#> [58] train-rmse:0.778657 test-rmse:3.850442 \n#> [59] train-rmse:0.773384 test-rmse:3.853208 \n#> [60] train-rmse:0.766044 test-rmse:3.852157 \n#> [61] train-rmse:0.761048 test-rmse:3.849829 \n#> [62] train-rmse:0.752227 test-rmse:3.840653 \n#> [63] train-rmse:0.736047 test-rmse:3.852588 \n#> [64] train-rmse:0.724781 test-rmse:3.846104 \n#> [65] train-rmse:0.713466 test-rmse:3.837874 \n#> [66] train-rmse:0.703244 test-rmse:3.836673 \n#> [67] train-rmse:0.689611 test-rmse:3.836109 \n#> [68] train-rmse:0.684932 test-rmse:3.826088 \n#> [69] train-rmse:0.664265 test-rmse:3.818385 \n#> [70] train-rmse:0.652603 test-rmse:3.810074 \n#> [1]  train-rmse:16.924814    test-rmse:17.491308 \n#> [2]  train-rmse:12.224077    test-rmse:12.977083 \n#> [3]  train-rmse:8.935319 test-rmse:9.811723 \n#> [4]  train-rmse:6.643097 test-rmse:7.739571 \n#> [5]  train-rmse:5.062052 test-rmse:6.470779 \n#> [6]  train-rmse:3.994706 test-rmse:5.659490 \n#> [7]  train-rmse:3.290321 test-rmse:5.208584 \n#> [8]  train-rmse:2.811032 test-rmse:4.926356 \n#> [9]  train-rmse:2.511393 test-rmse:4.616665 \n#> [10] train-rmse:2.302334 test-rmse:4.464360 \n#> [11] train-rmse:2.133457 test-rmse:4.325686 \n#> [12] train-rmse:2.020290 test-rmse:4.268084 \n#> [13] train-rmse:1.952641 test-rmse:4.176074 \n#> [14] train-rmse:1.909818 test-rmse:4.157850 \n#> [15] train-rmse:1.828420 test-rmse:4.114806 \n#> [16] train-rmse:1.762910 test-rmse:4.090413 \n#> [17] train-rmse:1.716270 test-rmse:4.040192 \n#> [18] train-rmse:1.684788 test-rmse:3.999977 \n#> [19] train-rmse:1.629414 test-rmse:3.970050 \n#> [20] train-rmse:1.570881 test-rmse:3.975561 \n#> [21] train-rmse:1.539230 test-rmse:3.971791 \n#> [22] train-rmse:1.483834 test-rmse:3.966041 \n#> [23] train-rmse:1.455368 test-rmse:3.981049 \n#> [24] train-rmse:1.440889 test-rmse:3.923917 \n#> [25] train-rmse:1.417689 test-rmse:3.904749 \n#> [26] train-rmse:1.377943 test-rmse:3.927352 \n#> [27] train-rmse:1.351410 test-rmse:3.924684 \n#> [28] train-rmse:1.307542 test-rmse:3.927266 \n#> [29] train-rmse:1.289389 test-rmse:3.920995 \n#> [30] train-rmse:1.281154 test-rmse:3.913963 \n#> [31] train-rmse:1.262924 test-rmse:3.913638 \n#> [32] train-rmse:1.240791 test-rmse:3.907200 \n#> [33] train-rmse:1.225516 test-rmse:3.905394 \n#> [34] train-rmse:1.212163 test-rmse:3.912072 \n#> [35] train-rmse:1.195203 test-rmse:3.916352 \n#> [36] train-rmse:1.187114 test-rmse:3.919584 \n#> [37] train-rmse:1.169093 test-rmse:3.912839 \n#> [38] train-rmse:1.139805 test-rmse:3.898301 \n#> [39] train-rmse:1.128596 test-rmse:3.896211 \n#> [40] train-rmse:1.117456 test-rmse:3.875217 \n#> [41] train-rmse:1.082428 test-rmse:3.880888 \n#> [42] train-rmse:1.073824 test-rmse:3.878762 \n#> [43] train-rmse:1.057046 test-rmse:3.881334 \n#> [44] train-rmse:1.044397 test-rmse:3.879126 \n#> [45] train-rmse:1.039668 test-rmse:3.878866 \n#> [46] train-rmse:1.023359 test-rmse:3.881689 \n#> [47] train-rmse:0.999350 test-rmse:3.887873 \n#> [48] train-rmse:0.978395 test-rmse:3.896517 \n#> [49] train-rmse:0.968111 test-rmse:3.876398 \n#> [50] train-rmse:0.959579 test-rmse:3.874545 \n#> [51] train-rmse:0.940500 test-rmse:3.873539 \n#> [52] train-rmse:0.917210 test-rmse:3.863689 \n#> [53] train-rmse:0.912667 test-rmse:3.861802 \n#> [54] train-rmse:0.906458 test-rmse:3.864835 \n#> [55] train-rmse:0.899376 test-rmse:3.845328 \n#> [56] train-rmse:0.894771 test-rmse:3.848354 \n#> [57] train-rmse:0.888213 test-rmse:3.848686 \n#> [58] train-rmse:0.884812 test-rmse:3.846327 \n#> [59] train-rmse:0.868224 test-rmse:3.841791 \n#> [60] train-rmse:0.850141 test-rmse:3.839369 \n#> [61] train-rmse:0.838281 test-rmse:3.842239 \n#> [62] train-rmse:0.809893 test-rmse:3.845711 \n#> [63] train-rmse:0.801592 test-rmse:3.847079 \n#> [64] train-rmse:0.797066 test-rmse:3.846509 \n#> [65] train-rmse:0.783371 test-rmse:3.836707 \n#> [66] train-rmse:0.760317 test-rmse:3.839897 \n#> [67] train-rmse:0.755642 test-rmse:3.833284 \n#> [68] train-rmse:0.750886 test-rmse:3.834663 \n#> [69] train-rmse:0.737716 test-rmse:3.832866 \n#> [70] train-rmse:0.730757 test-rmse:3.833470 \n#> [1]  train-rmse:16.769430    test-rmse:17.894876 \n#> [2]  train-rmse:12.179074    test-rmse:13.314768 \n#> [3]  train-rmse:8.959736 test-rmse:9.939200 \n#> [4]  train-rmse:6.744677 test-rmse:7.862458 \n#> [5]  train-rmse:5.215327 test-rmse:6.289669 \n#> [6]  train-rmse:4.166223 test-rmse:5.344343 \n#> [7]  train-rmse:3.448274 test-rmse:4.808412 \n#> [8]  train-rmse:3.000724 test-rmse:4.346214 \n#> [9]  train-rmse:2.676890 test-rmse:4.018198 \n#> [10] train-rmse:2.454332 test-rmse:3.864414 \n#> [11] train-rmse:2.320214 test-rmse:3.706553 \n#> [12] train-rmse:2.168896 test-rmse:3.625093 \n#> [13] train-rmse:2.087479 test-rmse:3.566353 \n#> [14] train-rmse:1.988021 test-rmse:3.482312 \n#> [15] train-rmse:1.900084 test-rmse:3.473857 \n#> [16] train-rmse:1.830392 test-rmse:3.442453 \n#> [17] train-rmse:1.798654 test-rmse:3.423610 \n#> [18] train-rmse:1.735404 test-rmse:3.392003 \n#> [19] train-rmse:1.689292 test-rmse:3.344842 \n#> [20] train-rmse:1.658038 test-rmse:3.329957 \n#> [21] train-rmse:1.615931 test-rmse:3.323461 \n#> [22] train-rmse:1.597195 test-rmse:3.320635 \n#> [23] train-rmse:1.547681 test-rmse:3.315211 \n#> [24] train-rmse:1.522034 test-rmse:3.289021 \n#> [25] train-rmse:1.502220 test-rmse:3.273123 \n#> [26] train-rmse:1.452952 test-rmse:3.264563 \n#> [27] train-rmse:1.440006 test-rmse:3.266037 \n#> [28] train-rmse:1.407211 test-rmse:3.267691 \n#> [29] train-rmse:1.380584 test-rmse:3.252523 \n#> [30] train-rmse:1.347878 test-rmse:3.254423 \n#> [31] train-rmse:1.323356 test-rmse:3.247834 \n#> [32] train-rmse:1.307834 test-rmse:3.243761 \n#> [33] train-rmse:1.284736 test-rmse:3.229237 \n#> [34] train-rmse:1.261906 test-rmse:3.218660 \n#> [35] train-rmse:1.230486 test-rmse:3.216589 \n#> [36] train-rmse:1.201938 test-rmse:3.205829 \n#> [37] train-rmse:1.186259 test-rmse:3.204344 \n#> [38] train-rmse:1.179335 test-rmse:3.192806 \n#> [39] train-rmse:1.162139 test-rmse:3.192417 \n#> [40] train-rmse:1.147543 test-rmse:3.182245 \n#> [41] train-rmse:1.130188 test-rmse:3.175907 \n#> [42] train-rmse:1.111416 test-rmse:3.182595 \n#> [43] train-rmse:1.098352 test-rmse:3.190045 \n#> [44] train-rmse:1.093224 test-rmse:3.190668 \n#> [45] train-rmse:1.057084 test-rmse:3.183423 \n#> [46] train-rmse:1.049201 test-rmse:3.184489 \n#> [47] train-rmse:1.030767 test-rmse:3.184654 \n#> [48] train-rmse:1.005605 test-rmse:3.189390 \n#> [49] train-rmse:0.983366 test-rmse:3.196911 \n#> [50] train-rmse:0.964676 test-rmse:3.202144 \n#> [51] train-rmse:0.948305 test-rmse:3.189985 \n#> [52] train-rmse:0.920270 test-rmse:3.199393 \n#> [53] train-rmse:0.915890 test-rmse:3.199914 \n#> [54] train-rmse:0.892648 test-rmse:3.199218 \n#> [55] train-rmse:0.882845 test-rmse:3.196315 \n#> [56] train-rmse:0.872703 test-rmse:3.185576 \n#> [57] train-rmse:0.861601 test-rmse:3.193927 \n#> [58] train-rmse:0.838209 test-rmse:3.184323 \n#> [59] train-rmse:0.828727 test-rmse:3.182709 \n#> [60] train-rmse:0.810560 test-rmse:3.176715 \n#> [61] train-rmse:0.805448 test-rmse:3.176383 \n#> [62] train-rmse:0.798437 test-rmse:3.180692 \n#> [63] train-rmse:0.782868 test-rmse:3.188952 \n#> [64] train-rmse:0.775681 test-rmse:3.180862 \n#> [65] train-rmse:0.759254 test-rmse:3.183831 \n#> [66] train-rmse:0.745204 test-rmse:3.182465 \n#> [67] train-rmse:0.741159 test-rmse:3.183134 \n#> [68] train-rmse:0.736253 test-rmse:3.184094 \n#> [69] train-rmse:0.729998 test-rmse:3.191249 \n#> [70] train-rmse:0.716184 test-rmse:3.184404 \n#> [1]  train-rmse:17.087106    test-rmse:17.547975 \n#> [2]  train-rmse:12.433314    test-rmse:12.898015 \n#> [3]  train-rmse:9.181515 test-rmse:9.806146 \n#> [4]  train-rmse:6.907137 test-rmse:7.608362 \n#> [5]  train-rmse:5.315277 test-rmse:6.191429 \n#> [6]  train-rmse:4.277091 test-rmse:5.247489 \n#> [7]  train-rmse:3.555920 test-rmse:4.620634 \n#> [8]  train-rmse:3.091994 test-rmse:4.277427 \n#> [9]  train-rmse:2.773289 test-rmse:4.039136 \n#> [10] train-rmse:2.542963 test-rmse:3.837833 \n#> [11] train-rmse:2.358420 test-rmse:3.719522 \n#> [12] train-rmse:2.259905 test-rmse:3.657435 \n#> [13] train-rmse:2.143198 test-rmse:3.614771 \n#> [14] train-rmse:2.016831 test-rmse:3.509346 \n#> [15] train-rmse:1.926708 test-rmse:3.452235 \n#> [16] train-rmse:1.831159 test-rmse:3.424980 \n#> [17] train-rmse:1.764247 test-rmse:3.401318 \n#> [18] train-rmse:1.717560 test-rmse:3.386264 \n#> [19] train-rmse:1.686371 test-rmse:3.359821 \n#> [20] train-rmse:1.635582 test-rmse:3.344121 \n#> [21] train-rmse:1.597862 test-rmse:3.321821 \n#> [22] train-rmse:1.551536 test-rmse:3.314438 \n#> [23] train-rmse:1.493920 test-rmse:3.301745 \n#> [24] train-rmse:1.468887 test-rmse:3.307657 \n#> [25] train-rmse:1.440494 test-rmse:3.318599 \n#> [26] train-rmse:1.399639 test-rmse:3.308133 \n#> [27] train-rmse:1.371753 test-rmse:3.321056 \n#> [28] train-rmse:1.356785 test-rmse:3.311057 \n#> [29] train-rmse:1.334746 test-rmse:3.315460 \n#> [30] train-rmse:1.314431 test-rmse:3.313619 \n#> [31] train-rmse:1.286656 test-rmse:3.301253 \n#> [32] train-rmse:1.275029 test-rmse:3.300700 \n#> [33] train-rmse:1.261278 test-rmse:3.313080 \n#> [34] train-rmse:1.228144 test-rmse:3.307229 \n#> [35] train-rmse:1.209504 test-rmse:3.301115 \n#> [36] train-rmse:1.186792 test-rmse:3.290215 \n#> [37] train-rmse:1.168979 test-rmse:3.284940 \n#> [38] train-rmse:1.140622 test-rmse:3.289209 \n#> [39] train-rmse:1.125910 test-rmse:3.283581 \n#> [40] train-rmse:1.114858 test-rmse:3.286099 \n#> [41] train-rmse:1.090374 test-rmse:3.302602 \n#> [42] train-rmse:1.076803 test-rmse:3.301566 \n#> [43] train-rmse:1.068855 test-rmse:3.296318 \n#> [44] train-rmse:1.042066 test-rmse:3.292759 \n#> [45] train-rmse:1.028457 test-rmse:3.294081 \n#> [46] train-rmse:0.995959 test-rmse:3.270298 \n#> [47] train-rmse:0.980871 test-rmse:3.268985 \n#> [48] train-rmse:0.958533 test-rmse:3.274377 \n#> [49] train-rmse:0.938282 test-rmse:3.272335 \n#> [50] train-rmse:0.928156 test-rmse:3.275671 \n#> [51] train-rmse:0.921367 test-rmse:3.278549 \n#> [52] train-rmse:0.911435 test-rmse:3.271091 \n#> [53] train-rmse:0.903200 test-rmse:3.273346 \n#> [54] train-rmse:0.878155 test-rmse:3.275489 \n#> [55] train-rmse:0.856695 test-rmse:3.268017 \n#> [56] train-rmse:0.843457 test-rmse:3.263470 \n#> [57] train-rmse:0.825039 test-rmse:3.262049 \n#> [58] train-rmse:0.801914 test-rmse:3.260554 \n#> [59] train-rmse:0.790788 test-rmse:3.264901 \n#> [60] train-rmse:0.780473 test-rmse:3.265901 \n#> [61] train-rmse:0.764497 test-rmse:3.257577 \n#> [62] train-rmse:0.755715 test-rmse:3.257109 \n#> [63] train-rmse:0.743633 test-rmse:3.251935 \n#> [64] train-rmse:0.735593 test-rmse:3.251824 \n#> [65] train-rmse:0.716100 test-rmse:3.252355 \n#> [66] train-rmse:0.708078 test-rmse:3.256886 \n#> [67] train-rmse:0.701784 test-rmse:3.254948 \n#> [68] train-rmse:0.694658 test-rmse:3.253093 \n#> [69] train-rmse:0.692237 test-rmse:3.252560 \n#> [70] train-rmse:0.682665 test-rmse:3.252795 \n#> [1]  train-rmse:17.039314    test-rmse:17.372493 \n#> [2]  train-rmse:12.364930    test-rmse:12.737714 \n#> [3]  train-rmse:9.152790 test-rmse:9.706442 \n#> [4]  train-rmse:6.880103 test-rmse:7.469698 \n#> [5]  train-rmse:5.385033 test-rmse:6.221769 \n#> [6]  train-rmse:4.334779 test-rmse:5.418746 \n#> [7]  train-rmse:3.623685 test-rmse:4.894852 \n#> [8]  train-rmse:3.119375 test-rmse:4.530647 \n#> [9]  train-rmse:2.808541 test-rmse:4.299940 \n#> [10] train-rmse:2.608400 test-rmse:4.144907 \n#> [11] train-rmse:2.438045 test-rmse:4.021411 \n#> [12] train-rmse:2.284360 test-rmse:3.923246 \n#> [13] train-rmse:2.160649 test-rmse:3.878706 \n#> [14] train-rmse:2.074205 test-rmse:3.879430 \n#> [15] train-rmse:2.001493 test-rmse:3.882180 \n#> [16] train-rmse:1.931897 test-rmse:3.853905 \n#> [17] train-rmse:1.886886 test-rmse:3.839471 \n#> [18] train-rmse:1.842014 test-rmse:3.817174 \n#> [19] train-rmse:1.770499 test-rmse:3.809698 \n#> [20] train-rmse:1.741151 test-rmse:3.788846 \n#> [21] train-rmse:1.706989 test-rmse:3.786862 \n#> [22] train-rmse:1.682979 test-rmse:3.788694 \n#> [23] train-rmse:1.667438 test-rmse:3.801298 \n#> [24] train-rmse:1.644861 test-rmse:3.800593 \n#> [25] train-rmse:1.625596 test-rmse:3.807905 \n#> [26] train-rmse:1.588505 test-rmse:3.830860 \n#> [27] train-rmse:1.556805 test-rmse:3.830283 \n#> [28] train-rmse:1.504327 test-rmse:3.831677 \n#> [29] train-rmse:1.484864 test-rmse:3.827646 \n#> [30] train-rmse:1.436177 test-rmse:3.831639 \n#> [31] train-rmse:1.406546 test-rmse:3.843213 \n#> [32] train-rmse:1.386777 test-rmse:3.850847 \n#> [33] train-rmse:1.362030 test-rmse:3.850510 \n#> [34] train-rmse:1.337454 test-rmse:3.846228 \n#> [35] train-rmse:1.321338 test-rmse:3.831758 \n#> [36] train-rmse:1.296574 test-rmse:3.823850 \n#> [37] train-rmse:1.275097 test-rmse:3.802191 \n#> [38] train-rmse:1.258453 test-rmse:3.806852 \n#> [39] train-rmse:1.247943 test-rmse:3.816175 \n#> [40] train-rmse:1.208008 test-rmse:3.815497 \n#> [41] train-rmse:1.180333 test-rmse:3.810803 \n#> [42] train-rmse:1.166932 test-rmse:3.802153 \n#> [43] train-rmse:1.141532 test-rmse:3.800314 \n#> [44] train-rmse:1.125231 test-rmse:3.801838 \n#> [45] train-rmse:1.114843 test-rmse:3.801716 \n#> [46] train-rmse:1.103012 test-rmse:3.790983 \n#> [47] train-rmse:1.090691 test-rmse:3.789959 \n#> [48] train-rmse:1.059705 test-rmse:3.758672 \n#> [49] train-rmse:1.043280 test-rmse:3.760127 \n#> [50] train-rmse:1.021525 test-rmse:3.757465 \n#> [51] train-rmse:1.009641 test-rmse:3.761157 \n#> [52] train-rmse:0.995215 test-rmse:3.761631 \n#> [53] train-rmse:0.987668 test-rmse:3.767853 \n#> [54] train-rmse:0.976287 test-rmse:3.764637 \n#> [55] train-rmse:0.961583 test-rmse:3.763548 \n#> [56] train-rmse:0.948269 test-rmse:3.765997 \n#> [57] train-rmse:0.939112 test-rmse:3.760699 \n#> [58] train-rmse:0.914079 test-rmse:3.762832 \n#> [59] train-rmse:0.894459 test-rmse:3.750531 \n#> [60] train-rmse:0.871483 test-rmse:3.749053 \n#> [61] train-rmse:0.861584 test-rmse:3.756978 \n#> [62] train-rmse:0.844814 test-rmse:3.756580 \n#> [63] train-rmse:0.829690 test-rmse:3.756497 \n#> [64] train-rmse:0.810550 test-rmse:3.755744 \n#> [65] train-rmse:0.804361 test-rmse:3.762233 \n#> [66] train-rmse:0.800803 test-rmse:3.762128 \n#> [67] train-rmse:0.776651 test-rmse:3.751693 \n#> [68] train-rmse:0.772505 test-rmse:3.748052 \n#> [69] train-rmse:0.760311 test-rmse:3.744858 \n#> [70] train-rmse:0.750680 test-rmse:3.737034 \n#> [1]  train-rmse:17.101483    test-rmse:17.221152 \n#> [2]  train-rmse:12.444646    test-rmse:12.615811 \n#> [3]  train-rmse:9.172284 test-rmse:9.598002 \n#> [4]  train-rmse:6.884956 test-rmse:7.412542 \n#> [5]  train-rmse:5.322263 test-rmse:5.931355 \n#> [6]  train-rmse:4.280939 test-rmse:5.076201 \n#> [7]  train-rmse:3.569942 test-rmse:4.549952 \n#> [8]  train-rmse:3.081976 test-rmse:4.256225 \n#> [9]  train-rmse:2.781709 test-rmse:4.040227 \n#> [10] train-rmse:2.530248 test-rmse:3.816286 \n#> [11] train-rmse:2.366431 test-rmse:3.756852 \n#> [12] train-rmse:2.261999 test-rmse:3.717622 \n#> [13] train-rmse:2.164082 test-rmse:3.678948 \n#> [14] train-rmse:2.076715 test-rmse:3.604042 \n#> [15] train-rmse:2.014051 test-rmse:3.578147 \n#> [16] train-rmse:1.969042 test-rmse:3.556411 \n#> [17] train-rmse:1.883153 test-rmse:3.516556 \n#> [18] train-rmse:1.834229 test-rmse:3.496479 \n#> [19] train-rmse:1.792077 test-rmse:3.487370 \n#> [20] train-rmse:1.724855 test-rmse:3.454408 \n#> [21] train-rmse:1.697145 test-rmse:3.424916 \n#> [22] train-rmse:1.660443 test-rmse:3.425995 \n#> [23] train-rmse:1.615386 test-rmse:3.415980 \n#> [24] train-rmse:1.596917 test-rmse:3.384274 \n#> [25] train-rmse:1.569208 test-rmse:3.387216 \n#> [26] train-rmse:1.522134 test-rmse:3.372645 \n#> [27] train-rmse:1.498584 test-rmse:3.373395 \n#> [28] train-rmse:1.465949 test-rmse:3.366745 \n#> [29] train-rmse:1.457054 test-rmse:3.369736 \n#> [30] train-rmse:1.429412 test-rmse:3.367073 \n#> [31] train-rmse:1.385224 test-rmse:3.361896 \n#> [32] train-rmse:1.365340 test-rmse:3.369315 \n#> [33] train-rmse:1.348392 test-rmse:3.338163 \n#> [34] train-rmse:1.314974 test-rmse:3.347388 \n#> [35] train-rmse:1.279139 test-rmse:3.354428 \n#> [36] train-rmse:1.271114 test-rmse:3.360353 \n#> [37] train-rmse:1.255547 test-rmse:3.370428 \n#> [38] train-rmse:1.244996 test-rmse:3.364618 \n#> [39] train-rmse:1.233660 test-rmse:3.355773 \n#> [40] train-rmse:1.204007 test-rmse:3.354770 \n#> [41] train-rmse:1.176796 test-rmse:3.350602 \n#> [42] train-rmse:1.158870 test-rmse:3.351266 \n#> [43] train-rmse:1.142927 test-rmse:3.354656 \n#> [44] train-rmse:1.127056 test-rmse:3.359469 \n#> [45] train-rmse:1.122091 test-rmse:3.365171 \n#> [46] train-rmse:1.105127 test-rmse:3.363288 \n#> [47] train-rmse:1.092647 test-rmse:3.366496 \n#> [48] train-rmse:1.076022 test-rmse:3.364519 \n#> [49] train-rmse:1.061236 test-rmse:3.363286 \n#> [50] train-rmse:1.049585 test-rmse:3.369344 \n#> [51] train-rmse:1.029361 test-rmse:3.373790 \n#> [52] train-rmse:1.014524 test-rmse:3.370027 \n#> [53] train-rmse:1.004017 test-rmse:3.368937 \n#> [54] train-rmse:0.994354 test-rmse:3.369895 \n#> [55] train-rmse:0.991787 test-rmse:3.370348 \n#> [56] train-rmse:0.968377 test-rmse:3.365827 \n#> [57] train-rmse:0.944894 test-rmse:3.368889 \n#> [58] train-rmse:0.940092 test-rmse:3.365939 \n#> [59] train-rmse:0.930072 test-rmse:3.365950 \n#> [60] train-rmse:0.913780 test-rmse:3.371885 \n#> [61] train-rmse:0.894146 test-rmse:3.370491 \n#> [62] train-rmse:0.882759 test-rmse:3.372006 \n#> [63] train-rmse:0.869799 test-rmse:3.380177 \n#> [64] train-rmse:0.859722 test-rmse:3.369491 \n#> [65] train-rmse:0.844869 test-rmse:3.376588 \n#> [66] train-rmse:0.827649 test-rmse:3.382764 \n#> [67] train-rmse:0.819069 test-rmse:3.384356 \n#> [68] train-rmse:0.800752 test-rmse:3.387998 \n#> [69] train-rmse:0.789417 test-rmse:3.393218 \n#> [70] train-rmse:0.779049 test-rmse:3.397949 \n#> [1]  train-rmse:17.526194    test-rmse:16.532628 \n#> [2]  train-rmse:12.705034    test-rmse:12.224050 \n#> [3]  train-rmse:9.338710 test-rmse:9.098551 \n#> [4]  train-rmse:7.012505 test-rmse:7.200110 \n#> [5]  train-rmse:5.414610 test-rmse:5.915792 \n#> [6]  train-rmse:4.332967 test-rmse:5.032077 \n#> [7]  train-rmse:3.629258 test-rmse:4.617346 \n#> [8]  train-rmse:3.135208 test-rmse:4.254942 \n#> [9]  train-rmse:2.795479 test-rmse:4.046990 \n#> [10] train-rmse:2.560827 test-rmse:3.883091 \n#> [11] train-rmse:2.386630 test-rmse:3.803737 \n#> [12] train-rmse:2.279182 test-rmse:3.746729 \n#> [13] train-rmse:2.172721 test-rmse:3.744097 \n#> [14] train-rmse:2.107357 test-rmse:3.706228 \n#> [15] train-rmse:2.030903 test-rmse:3.674772 \n#> [16] train-rmse:1.968384 test-rmse:3.674313 \n#> [17] train-rmse:1.939995 test-rmse:3.661536 \n#> [18] train-rmse:1.898635 test-rmse:3.636892 \n#> [19] train-rmse:1.870243 test-rmse:3.640828 \n#> [20] train-rmse:1.837476 test-rmse:3.620710 \n#> [21] train-rmse:1.770018 test-rmse:3.621513 \n#> [22] train-rmse:1.705829 test-rmse:3.569995 \n#> [23] train-rmse:1.685498 test-rmse:3.568959 \n#> [24] train-rmse:1.642241 test-rmse:3.541168 \n#> [25] train-rmse:1.618518 test-rmse:3.535743 \n#> [26] train-rmse:1.601418 test-rmse:3.541819 \n#> [27] train-rmse:1.570618 test-rmse:3.513438 \n#> [28] train-rmse:1.535657 test-rmse:3.505832 \n#> [29] train-rmse:1.509630 test-rmse:3.493689 \n#> [30] train-rmse:1.492543 test-rmse:3.497684 \n#> [31] train-rmse:1.452068 test-rmse:3.493939 \n#> [32] train-rmse:1.427910 test-rmse:3.486587 \n#> [33] train-rmse:1.418973 test-rmse:3.491128 \n#> [34] train-rmse:1.398376 test-rmse:3.490988 \n#> [35] train-rmse:1.372867 test-rmse:3.486871 \n#> [36] train-rmse:1.335636 test-rmse:3.477362 \n#> [37] train-rmse:1.313383 test-rmse:3.455708 \n#> [38] train-rmse:1.281301 test-rmse:3.431872 \n#> [39] train-rmse:1.267660 test-rmse:3.424936 \n#> [40] train-rmse:1.245438 test-rmse:3.427774 \n#> [41] train-rmse:1.228806 test-rmse:3.413491 \n#> [42] train-rmse:1.200659 test-rmse:3.411115 \n#> [43] train-rmse:1.183586 test-rmse:3.410945 \n#> [44] train-rmse:1.161724 test-rmse:3.413806 \n#> [45] train-rmse:1.144605 test-rmse:3.407710 \n#> [46] train-rmse:1.129759 test-rmse:3.406934 \n#> [47] train-rmse:1.093113 test-rmse:3.395997 \n#> [48] train-rmse:1.068301 test-rmse:3.397111 \n#> [49] train-rmse:1.044730 test-rmse:3.402970 \n#> [50] train-rmse:1.030032 test-rmse:3.404501 \n#> [51] train-rmse:1.020507 test-rmse:3.401476 \n#> [52] train-rmse:1.002199 test-rmse:3.400382 \n#> [53] train-rmse:0.991375 test-rmse:3.400221 \n#> [54] train-rmse:0.980811 test-rmse:3.394219 \n#> [55] train-rmse:0.969603 test-rmse:3.387702 \n#> [56] train-rmse:0.960491 test-rmse:3.384092 \n#> [57] train-rmse:0.931411 test-rmse:3.376298 \n#> [58] train-rmse:0.906902 test-rmse:3.379070 \n#> [59] train-rmse:0.892301 test-rmse:3.378847 \n#> [60] train-rmse:0.870655 test-rmse:3.379813 \n#> [61] train-rmse:0.862239 test-rmse:3.373054 \n#> [62] train-rmse:0.852125 test-rmse:3.362553 \n#> [63] train-rmse:0.847975 test-rmse:3.363875 \n#> [64] train-rmse:0.833654 test-rmse:3.361672 \n#> [65] train-rmse:0.825251 test-rmse:3.358061 \n#> [66] train-rmse:0.810980 test-rmse:3.361093 \n#> [67] train-rmse:0.796412 test-rmse:3.357863 \n#> [68] train-rmse:0.780503 test-rmse:3.360487 \n#> [69] train-rmse:0.773635 test-rmse:3.354451 \n#> [70] train-rmse:0.754839 test-rmse:3.344973 \n#> [1]  train-rmse:17.483723    test-rmse:16.602402 \n#> [2]  train-rmse:12.677203    test-rmse:11.971884 \n#> [3]  train-rmse:9.334857 test-rmse:8.970090 \n#> [4]  train-rmse:6.974731 test-rmse:6.865298 \n#> [5]  train-rmse:5.294664 test-rmse:5.452871 \n#> [6]  train-rmse:4.147150 test-rmse:4.587184 \n#> [7]  train-rmse:3.400253 test-rmse:4.028282 \n#> [8]  train-rmse:2.909693 test-rmse:3.673200 \n#> [9]  train-rmse:2.595444 test-rmse:3.490276 \n#> [10] train-rmse:2.391398 test-rmse:3.390995 \n#> [11] train-rmse:2.223429 test-rmse:3.317579 \n#> [12] train-rmse:2.114670 test-rmse:3.244576 \n#> [13] train-rmse:2.040906 test-rmse:3.243893 \n#> [14] train-rmse:1.961224 test-rmse:3.237762 \n#> [15] train-rmse:1.882405 test-rmse:3.230505 \n#> [16] train-rmse:1.804731 test-rmse:3.211189 \n#> [17] train-rmse:1.754230 test-rmse:3.187206 \n#> [18] train-rmse:1.705020 test-rmse:3.164158 \n#> [19] train-rmse:1.680249 test-rmse:3.155934 \n#> [20] train-rmse:1.650263 test-rmse:3.148356 \n#> [21] train-rmse:1.592729 test-rmse:3.138569 \n#> [22] train-rmse:1.557427 test-rmse:3.137305 \n#> [23] train-rmse:1.529177 test-rmse:3.140099 \n#> [24] train-rmse:1.493994 test-rmse:3.130350 \n#> [25] train-rmse:1.470884 test-rmse:3.131758 \n#> [26] train-rmse:1.448832 test-rmse:3.132505 \n#> [27] train-rmse:1.415059 test-rmse:3.131703 \n#> [28] train-rmse:1.377509 test-rmse:3.121365 \n#> [29] train-rmse:1.356290 test-rmse:3.122179 \n#> [30] train-rmse:1.322679 test-rmse:3.120752 \n#> [31] train-rmse:1.297073 test-rmse:3.114314 \n#> [32] train-rmse:1.277549 test-rmse:3.120934 \n#> [33] train-rmse:1.251928 test-rmse:3.113675 \n#> [34] train-rmse:1.231383 test-rmse:3.110015 \n#> [35] train-rmse:1.199163 test-rmse:3.099596 \n#> [36] train-rmse:1.178040 test-rmse:3.091530 \n#> [37] train-rmse:1.161323 test-rmse:3.096724 \n#> [38] train-rmse:1.138615 test-rmse:3.096789 \n#> [39] train-rmse:1.125595 test-rmse:3.096618 \n#> [40] train-rmse:1.111898 test-rmse:3.094366 \n#> [41] train-rmse:1.094701 test-rmse:3.097805 \n#> [42] train-rmse:1.085883 test-rmse:3.092502 \n#> [43] train-rmse:1.072696 test-rmse:3.087899 \n#> [44] train-rmse:1.064667 test-rmse:3.088633 \n#> [45] train-rmse:1.047441 test-rmse:3.090840 \n#> [46] train-rmse:1.021197 test-rmse:3.073791 \n#> [47] train-rmse:1.010193 test-rmse:3.078463 \n#> [48] train-rmse:0.985304 test-rmse:3.073091 \n#> [49] train-rmse:0.977418 test-rmse:3.076953 \n#> [50] train-rmse:0.967475 test-rmse:3.077545 \n#> [51] train-rmse:0.942087 test-rmse:3.083860 \n#> [52] train-rmse:0.911916 test-rmse:3.081977 \n#> [53] train-rmse:0.886563 test-rmse:3.078180 \n#> [54] train-rmse:0.855724 test-rmse:3.078831 \n#> [55] train-rmse:0.845150 test-rmse:3.077916 \n#> [56] train-rmse:0.826518 test-rmse:3.080387 \n#> [57] train-rmse:0.821122 test-rmse:3.081350 \n#> [58] train-rmse:0.813287 test-rmse:3.083647 \n#> [59] train-rmse:0.795808 test-rmse:3.085114 \n#> [60] train-rmse:0.790514 test-rmse:3.083795 \n#> [61] train-rmse:0.783901 test-rmse:3.070323 \n#> [62] train-rmse:0.773177 test-rmse:3.068162 \n#> [63] train-rmse:0.762865 test-rmse:3.055478 \n#> [64] train-rmse:0.749724 test-rmse:3.054018 \n#> [65] train-rmse:0.734997 test-rmse:3.053921 \n#> [66] train-rmse:0.725137 test-rmse:3.063874 \n#> [67] train-rmse:0.716506 test-rmse:3.065351 \n#> [68] train-rmse:0.709745 test-rmse:3.067690 \n#> [69] train-rmse:0.701490 test-rmse:3.067873 \n#> [70] train-rmse:0.696366 test-rmse:3.066660 \n#> [1]  train-rmse:17.103693    test-rmse:17.252991 \n#> [2]  train-rmse:12.348859    test-rmse:12.743414 \n#> [3]  train-rmse:9.045874 test-rmse:9.692068 \n#> [4]  train-rmse:6.783909 test-rmse:7.692183 \n#> [5]  train-rmse:5.199627 test-rmse:6.321737 \n#> [6]  train-rmse:4.150670 test-rmse:5.533554 \n#> [7]  train-rmse:3.470625 test-rmse:5.048434 \n#> [8]  train-rmse:3.040186 test-rmse:4.743094 \n#> [9]  train-rmse:2.743806 test-rmse:4.468214 \n#> [10] train-rmse:2.512907 test-rmse:4.274009 \n#> [11] train-rmse:2.335801 test-rmse:4.174132 \n#> [12] train-rmse:2.216330 test-rmse:4.133427 \n#> [13] train-rmse:2.159221 test-rmse:4.105270 \n#> [14] train-rmse:2.083429 test-rmse:4.084567 \n#> [15] train-rmse:2.021866 test-rmse:4.010781 \n#> [16] train-rmse:1.937201 test-rmse:3.993215 \n#> [17] train-rmse:1.874537 test-rmse:3.959648 \n#> [18] train-rmse:1.830324 test-rmse:3.939004 \n#> [19] train-rmse:1.795591 test-rmse:3.903961 \n#> [20] train-rmse:1.762106 test-rmse:3.888590 \n#> [21] train-rmse:1.744185 test-rmse:3.883671 \n#> [22] train-rmse:1.710589 test-rmse:3.892285 \n#> [23] train-rmse:1.690421 test-rmse:3.863746 \n#> [24] train-rmse:1.643268 test-rmse:3.858944 \n#> [25] train-rmse:1.606434 test-rmse:3.845628 \n#> [26] train-rmse:1.557324 test-rmse:3.858439 \n#> [27] train-rmse:1.535222 test-rmse:3.865503 \n#> [28] train-rmse:1.510793 test-rmse:3.855497 \n#> [29] train-rmse:1.455015 test-rmse:3.858603 \n#> [30] train-rmse:1.437971 test-rmse:3.863025 \n#> [31] train-rmse:1.410683 test-rmse:3.859476 \n#> [32] train-rmse:1.378122 test-rmse:3.852221 \n#> [33] train-rmse:1.349468 test-rmse:3.853856 \n#> [34] train-rmse:1.337037 test-rmse:3.854812 \n#> [35] train-rmse:1.315696 test-rmse:3.858581 \n#> [36] train-rmse:1.297927 test-rmse:3.864516 \n#> [37] train-rmse:1.282889 test-rmse:3.852512 \n#> [38] train-rmse:1.268213 test-rmse:3.846431 \n#> [39] train-rmse:1.249264 test-rmse:3.852976 \n#> [40] train-rmse:1.226594 test-rmse:3.850417 \n#> [41] train-rmse:1.193590 test-rmse:3.836281 \n#> [42] train-rmse:1.168915 test-rmse:3.819372 \n#> [43] train-rmse:1.151118 test-rmse:3.815312 \n#> [44] train-rmse:1.127839 test-rmse:3.812875 \n#> [45] train-rmse:1.099423 test-rmse:3.813757 \n#> [46] train-rmse:1.081614 test-rmse:3.816851 \n#> [47] train-rmse:1.071098 test-rmse:3.813645 \n#> [48] train-rmse:1.051975 test-rmse:3.811646 \n#> [49] train-rmse:1.034362 test-rmse:3.808453 \n#> [50] train-rmse:1.016399 test-rmse:3.810196 \n#> [51] train-rmse:1.005012 test-rmse:3.813361 \n#> [52] train-rmse:0.983274 test-rmse:3.823564 \n#> [53] train-rmse:0.957256 test-rmse:3.809035 \n#> [54] train-rmse:0.952174 test-rmse:3.810448 \n#> [55] train-rmse:0.942637 test-rmse:3.807311 \n#> [56] train-rmse:0.934262 test-rmse:3.805659 \n#> [57] train-rmse:0.913344 test-rmse:3.794403 \n#> [58] train-rmse:0.892867 test-rmse:3.775166 \n#> [59] train-rmse:0.881712 test-rmse:3.779293 \n#> [60] train-rmse:0.866893 test-rmse:3.776759 \n#> [61] train-rmse:0.848013 test-rmse:3.767446 \n#> [62] train-rmse:0.838068 test-rmse:3.769425 \n#> [63] train-rmse:0.820571 test-rmse:3.764812 \n#> [64] train-rmse:0.800530 test-rmse:3.769180 \n#> [65] train-rmse:0.788218 test-rmse:3.771906 \n#> [66] train-rmse:0.763266 test-rmse:3.774454 \n#> [67] train-rmse:0.753960 test-rmse:3.771909 \n#> [68] train-rmse:0.742168 test-rmse:3.771918 \n#> [69] train-rmse:0.725973 test-rmse:3.775693 \n#> [70] train-rmse:0.718817 test-rmse:3.774767 \n#> [1]  train-rmse:17.086836    test-rmse:17.271259 \n#> [2]  train-rmse:12.423159    test-rmse:12.482463 \n#> [3]  train-rmse:9.137312 test-rmse:9.356235 \n#> [4]  train-rmse:6.844112 test-rmse:7.146619 \n#> [5]  train-rmse:5.306246 test-rmse:5.573422 \n#> [6]  train-rmse:4.189220 test-rmse:4.663308 \n#> [7]  train-rmse:3.462537 test-rmse:4.120639 \n#> [8]  train-rmse:2.944524 test-rmse:3.822552 \n#> [9]  train-rmse:2.606719 test-rmse:3.610832 \n#> [10] train-rmse:2.366434 test-rmse:3.531172 \n#> [11] train-rmse:2.204029 test-rmse:3.501713 \n#> [12] train-rmse:2.089627 test-rmse:3.483928 \n#> [13] train-rmse:1.995011 test-rmse:3.479363 \n#> [14] train-rmse:1.933802 test-rmse:3.494651 \n#> [15] train-rmse:1.870223 test-rmse:3.485901 \n#> [16] train-rmse:1.830508 test-rmse:3.495230 \n#> [17] train-rmse:1.786605 test-rmse:3.484070 \n#> [18] train-rmse:1.720169 test-rmse:3.456908 \n#> [19] train-rmse:1.670580 test-rmse:3.443953 \n#> [20] train-rmse:1.620001 test-rmse:3.443137 \n#> [21] train-rmse:1.604795 test-rmse:3.445152 \n#> [22] train-rmse:1.578824 test-rmse:3.445412 \n#> [23] train-rmse:1.548743 test-rmse:3.441911 \n#> [24] train-rmse:1.518229 test-rmse:3.438313 \n#> [25] train-rmse:1.504679 test-rmse:3.458323 \n#> [26] train-rmse:1.484650 test-rmse:3.451733 \n#> [27] train-rmse:1.450388 test-rmse:3.432473 \n#> [28] train-rmse:1.427110 test-rmse:3.416934 \n#> [29] train-rmse:1.412530 test-rmse:3.415223 \n#> [30] train-rmse:1.376340 test-rmse:3.411791 \n#> [31] train-rmse:1.363297 test-rmse:3.417721 \n#> [32] train-rmse:1.345455 test-rmse:3.423720 \n#> [33] train-rmse:1.308462 test-rmse:3.436598 \n#> [34] train-rmse:1.289718 test-rmse:3.438217 \n#> [35] train-rmse:1.266830 test-rmse:3.432317 \n#> [36] train-rmse:1.249031 test-rmse:3.443272 \n#> [37] train-rmse:1.212633 test-rmse:3.454168 \n#> [38] train-rmse:1.178920 test-rmse:3.454109 \n#> [39] train-rmse:1.145651 test-rmse:3.452442 \n#> [40] train-rmse:1.131976 test-rmse:3.465035 \n#> [41] train-rmse:1.108406 test-rmse:3.468541 \n#> [42] train-rmse:1.089188 test-rmse:3.463397 \n#> [43] train-rmse:1.083508 test-rmse:3.465469 \n#> [44] train-rmse:1.076285 test-rmse:3.463153 \n#> [45] train-rmse:1.055508 test-rmse:3.469290 \n#> [46] train-rmse:1.046914 test-rmse:3.470375 \n#> [47] train-rmse:1.033266 test-rmse:3.467631 \n#> [48] train-rmse:1.013558 test-rmse:3.475531 \n#> [49] train-rmse:0.996832 test-rmse:3.475539 \n#> [50] train-rmse:0.980940 test-rmse:3.457934 \n#> [51] train-rmse:0.965369 test-rmse:3.466527 \n#> [52] train-rmse:0.953598 test-rmse:3.461404 \n#> [53] train-rmse:0.947059 test-rmse:3.462820 \n#> [54] train-rmse:0.925900 test-rmse:3.454268 \n#> [55] train-rmse:0.919322 test-rmse:3.460876 \n#> [56] train-rmse:0.912147 test-rmse:3.459374 \n#> [57] train-rmse:0.903527 test-rmse:3.456502 \n#> [58] train-rmse:0.894283 test-rmse:3.450772 \n#> [59] train-rmse:0.888327 test-rmse:3.449615 \n#> [60] train-rmse:0.863139 test-rmse:3.459265 \n#> [61] train-rmse:0.841046 test-rmse:3.468838 \n#> [62] train-rmse:0.835520 test-rmse:3.473780 \n#> [63] train-rmse:0.832331 test-rmse:3.475061 \n#> [64] train-rmse:0.829117 test-rmse:3.476901 \n#> [65] train-rmse:0.823743 test-rmse:3.472604 \n#> [66] train-rmse:0.811426 test-rmse:3.480739 \n#> [67] train-rmse:0.795589 test-rmse:3.479755 \n#> [68] train-rmse:0.786904 test-rmse:3.481300 \n#> [69] train-rmse:0.767472 test-rmse:3.485711 \n#> [70] train-rmse:0.760890 test-rmse:3.494196 \n#> [1]  train-rmse:17.132009    test-rmse:17.251055 \n#> [2]  train-rmse:12.474579    test-rmse:12.813663 \n#> [3]  train-rmse:9.196238 test-rmse:9.473038 \n#> [4]  train-rmse:6.926752 test-rmse:7.332922 \n#> [5]  train-rmse:5.335834 test-rmse:5.797878 \n#> [6]  train-rmse:4.291225 test-rmse:4.941513 \n#> [7]  train-rmse:3.591254 test-rmse:4.424441 \n#> [8]  train-rmse:3.083757 test-rmse:4.016705 \n#> [9]  train-rmse:2.779699 test-rmse:3.772171 \n#> [10] train-rmse:2.584003 test-rmse:3.615159 \n#> [11] train-rmse:2.422954 test-rmse:3.545441 \n#> [12] train-rmse:2.304330 test-rmse:3.491544 \n#> [13] train-rmse:2.186087 test-rmse:3.436542 \n#> [14] train-rmse:2.122891 test-rmse:3.391602 \n#> [15] train-rmse:2.068880 test-rmse:3.389993 \n#> [16] train-rmse:2.031223 test-rmse:3.356006 \n#> [17] train-rmse:1.928655 test-rmse:3.314187 \n#> [18] train-rmse:1.890378 test-rmse:3.295842 \n#> [19] train-rmse:1.851579 test-rmse:3.278900 \n#> [20] train-rmse:1.804133 test-rmse:3.283719 \n#> [21] train-rmse:1.752445 test-rmse:3.292249 \n#> [22] train-rmse:1.726916 test-rmse:3.280337 \n#> [23] train-rmse:1.691543 test-rmse:3.255034 \n#> [24] train-rmse:1.660882 test-rmse:3.244141 \n#> [25] train-rmse:1.586876 test-rmse:3.253855 \n#> [26] train-rmse:1.544081 test-rmse:3.226763 \n#> [27] train-rmse:1.516795 test-rmse:3.211534 \n#> [28] train-rmse:1.474672 test-rmse:3.209140 \n#> [29] train-rmse:1.434904 test-rmse:3.204160 \n#> [30] train-rmse:1.415943 test-rmse:3.199675 \n#> [31] train-rmse:1.398198 test-rmse:3.209786 \n#> [32] train-rmse:1.355882 test-rmse:3.195988 \n#> [33] train-rmse:1.333300 test-rmse:3.198368 \n#> [34] train-rmse:1.318887 test-rmse:3.192620 \n#> [35] train-rmse:1.299203 test-rmse:3.186128 \n#> [36] train-rmse:1.259877 test-rmse:3.193987 \n#> [37] train-rmse:1.244086 test-rmse:3.192305 \n#> [38] train-rmse:1.214626 test-rmse:3.184889 \n#> [39] train-rmse:1.176503 test-rmse:3.189001 \n#> [40] train-rmse:1.147220 test-rmse:3.184795 \n#> [41] train-rmse:1.133412 test-rmse:3.178203 \n#> [42] train-rmse:1.107119 test-rmse:3.176926 \n#> [43] train-rmse:1.096034 test-rmse:3.179563 \n#> [44] train-rmse:1.084785 test-rmse:3.180497 \n#> [45] train-rmse:1.073845 test-rmse:3.174839 \n#> [46] train-rmse:1.063945 test-rmse:3.164721 \n#> [47] train-rmse:1.040868 test-rmse:3.161488 \n#> [48] train-rmse:1.008156 test-rmse:3.173606 \n#> [49] train-rmse:0.996106 test-rmse:3.177930 \n#> [50] train-rmse:0.990406 test-rmse:3.181367 \n#> [51] train-rmse:0.963512 test-rmse:3.186441 \n#> [52] train-rmse:0.954483 test-rmse:3.171163 \n#> [53] train-rmse:0.929998 test-rmse:3.170383 \n#> [54] train-rmse:0.912704 test-rmse:3.153999 \n#> [55] train-rmse:0.897730 test-rmse:3.148199 \n#> [56] train-rmse:0.890404 test-rmse:3.148111 \n#> [57] train-rmse:0.877286 test-rmse:3.139555 \n#> [58] train-rmse:0.855869 test-rmse:3.144100 \n#> [59] train-rmse:0.846142 test-rmse:3.148114 \n#> [60] train-rmse:0.838000 test-rmse:3.147561 \n#> [61] train-rmse:0.824123 test-rmse:3.147864 \n#> [62] train-rmse:0.802475 test-rmse:3.158312 \n#> [63] train-rmse:0.790599 test-rmse:3.156652 \n#> [64] train-rmse:0.781960 test-rmse:3.158303 \n#> [65] train-rmse:0.776998 test-rmse:3.158212 \n#> [66] train-rmse:0.771446 test-rmse:3.157373 \n#> [67] train-rmse:0.756462 test-rmse:3.156132 \n#> [68] train-rmse:0.747627 test-rmse:3.155212 \n#> [69] train-rmse:0.731652 test-rmse:3.163408 \n#> [70] train-rmse:0.728644 test-rmse:3.163302 \n#> [1]  train-rmse:17.278292    test-rmse:17.131271 \n#> [2]  train-rmse:12.556230    test-rmse:12.753764 \n#> [3]  train-rmse:9.281087 test-rmse:9.710914 \n#> [4]  train-rmse:6.964630 test-rmse:7.441855 \n#> [5]  train-rmse:5.380251 test-rmse:6.080617 \n#> [6]  train-rmse:4.302356 test-rmse:5.150695 \n#> [7]  train-rmse:3.597071 test-rmse:4.589758 \n#> [8]  train-rmse:3.156724 test-rmse:4.261594 \n#> [9]  train-rmse:2.860281 test-rmse:4.087005 \n#> [10] train-rmse:2.670655 test-rmse:4.006988 \n#> [11] train-rmse:2.486593 test-rmse:3.949593 \n#> [12] train-rmse:2.355701 test-rmse:3.857403 \n#> [13] train-rmse:2.284873 test-rmse:3.850330 \n#> [14] train-rmse:2.147371 test-rmse:3.810094 \n#> [15] train-rmse:2.072834 test-rmse:3.798598 \n#> [16] train-rmse:1.991673 test-rmse:3.761367 \n#> [17] train-rmse:1.946457 test-rmse:3.704476 \n#> [18] train-rmse:1.894600 test-rmse:3.707522 \n#> [19] train-rmse:1.851904 test-rmse:3.692939 \n#> [20] train-rmse:1.779696 test-rmse:3.674443 \n#> [21] train-rmse:1.747371 test-rmse:3.630174 \n#> [22] train-rmse:1.691118 test-rmse:3.579103 \n#> [23] train-rmse:1.653306 test-rmse:3.547489 \n#> [24] train-rmse:1.615739 test-rmse:3.547281 \n#> [25] train-rmse:1.583314 test-rmse:3.524839 \n#> [26] train-rmse:1.535739 test-rmse:3.516026 \n#> [27] train-rmse:1.499918 test-rmse:3.527197 \n#> [28] train-rmse:1.482618 test-rmse:3.543929 \n#> [29] train-rmse:1.445713 test-rmse:3.535451 \n#> [30] train-rmse:1.412574 test-rmse:3.518838 \n#> [31] train-rmse:1.396444 test-rmse:3.524303 \n#> [32] train-rmse:1.376443 test-rmse:3.515206 \n#> [33] train-rmse:1.336527 test-rmse:3.489955 \n#> [34] train-rmse:1.312602 test-rmse:3.490443 \n#> [35] train-rmse:1.288183 test-rmse:3.488138 \n#> [36] train-rmse:1.264677 test-rmse:3.498917 \n#> [37] train-rmse:1.244928 test-rmse:3.489153 \n#> [38] train-rmse:1.214395 test-rmse:3.470848 \n#> [39] train-rmse:1.199964 test-rmse:3.465575 \n#> [40] train-rmse:1.166921 test-rmse:3.465804 \n#> [41] train-rmse:1.133459 test-rmse:3.471084 \n#> [42] train-rmse:1.105368 test-rmse:3.475036 \n#> [43] train-rmse:1.083675 test-rmse:3.476203 \n#> [44] train-rmse:1.069402 test-rmse:3.464823 \n#> [45] train-rmse:1.060441 test-rmse:3.448605 \n#> [46] train-rmse:1.044142 test-rmse:3.440452 \n#> [47] train-rmse:1.024233 test-rmse:3.447371 \n#> [48] train-rmse:1.011220 test-rmse:3.449913 \n#> [49] train-rmse:1.000908 test-rmse:3.446062 \n#> [50] train-rmse:0.982938 test-rmse:3.441698 \n#> [51] train-rmse:0.977451 test-rmse:3.451431 \n#> [52] train-rmse:0.970785 test-rmse:3.447929 \n#> [53] train-rmse:0.939961 test-rmse:3.448962 \n#> [54] train-rmse:0.929331 test-rmse:3.447889 \n#> [55] train-rmse:0.909236 test-rmse:3.458525 \n#> [56] train-rmse:0.895298 test-rmse:3.446665 \n#> [57] train-rmse:0.888991 test-rmse:3.448505 \n#> [58] train-rmse:0.881190 test-rmse:3.449063 \n#> [59] train-rmse:0.870374 test-rmse:3.446085 \n#> [60] train-rmse:0.852684 test-rmse:3.443754 \n#> [61] train-rmse:0.834644 test-rmse:3.435974 \n#> [62] train-rmse:0.815564 test-rmse:3.426854 \n#> [63] train-rmse:0.799357 test-rmse:3.423715 \n#> [64] train-rmse:0.793762 test-rmse:3.428804 \n#> [65] train-rmse:0.782843 test-rmse:3.428723 \n#> [66] train-rmse:0.771491 test-rmse:3.432789 \n#> [67] train-rmse:0.762924 test-rmse:3.433431 \n#> [68] train-rmse:0.746974 test-rmse:3.427707 \n#> [69] train-rmse:0.728521 test-rmse:3.430997 \n#> [70] train-rmse:0.715470 test-rmse:3.432857 \n#> [1]  train-rmse:17.395387    test-rmse:16.807481 \n#> [2]  train-rmse:12.607171    test-rmse:12.209140 \n#> [3]  train-rmse:9.287325 test-rmse:8.937328 \n#> [4]  train-rmse:7.013758 test-rmse:6.761207 \n#> [5]  train-rmse:5.418121 test-rmse:5.271652 \n#> [6]  train-rmse:4.309007 test-rmse:4.362471 \n#> [7]  train-rmse:3.593338 test-rmse:3.809283 \n#> [8]  train-rmse:3.125462 test-rmse:3.488303 \n#> [9]  train-rmse:2.808215 test-rmse:3.257830 \n#> [10] train-rmse:2.613578 test-rmse:3.154585 \n#> [11] train-rmse:2.471688 test-rmse:3.079359 \n#> [12] train-rmse:2.326925 test-rmse:2.990096 \n#> [13] train-rmse:2.243460 test-rmse:2.929664 \n#> [14] train-rmse:2.134441 test-rmse:2.890038 \n#> [15] train-rmse:2.068489 test-rmse:2.834206 \n#> [16] train-rmse:2.009764 test-rmse:2.824391 \n#> [17] train-rmse:1.972183 test-rmse:2.825643 \n#> [18] train-rmse:1.915850 test-rmse:2.827552 \n#> [19] train-rmse:1.860962 test-rmse:2.804798 \n#> [20] train-rmse:1.825913 test-rmse:2.805106 \n#> [21] train-rmse:1.795406 test-rmse:2.794276 \n#> [22] train-rmse:1.755088 test-rmse:2.794035 \n#> [23] train-rmse:1.721797 test-rmse:2.775509 \n#> [24] train-rmse:1.676102 test-rmse:2.762334 \n#> [25] train-rmse:1.645750 test-rmse:2.755726 \n#> [26] train-rmse:1.606884 test-rmse:2.752308 \n#> [27] train-rmse:1.554420 test-rmse:2.725800 \n#> [28] train-rmse:1.533663 test-rmse:2.709923 \n#> [29] train-rmse:1.515497 test-rmse:2.715273 \n#> [30] train-rmse:1.487916 test-rmse:2.688154 \n#> [31] train-rmse:1.438739 test-rmse:2.688176 \n#> [32] train-rmse:1.423625 test-rmse:2.672790 \n#> [33] train-rmse:1.390451 test-rmse:2.663642 \n#> [34] train-rmse:1.375332 test-rmse:2.664944 \n#> [35] train-rmse:1.338564 test-rmse:2.667463 \n#> [36] train-rmse:1.303356 test-rmse:2.654060 \n#> [37] train-rmse:1.275568 test-rmse:2.665540 \n#> [38] train-rmse:1.261996 test-rmse:2.659455 \n#> [39] train-rmse:1.251079 test-rmse:2.665207 \n#> [40] train-rmse:1.227448 test-rmse:2.653628 \n#> [41] train-rmse:1.213734 test-rmse:2.646677 \n#> [42] train-rmse:1.176958 test-rmse:2.649511 \n#> [43] train-rmse:1.158550 test-rmse:2.659746 \n#> [44] train-rmse:1.146393 test-rmse:2.661095 \n#> [45] train-rmse:1.112010 test-rmse:2.650501 \n#> [46] train-rmse:1.092421 test-rmse:2.659812 \n#> [47] train-rmse:1.075534 test-rmse:2.660230 \n#> [48] train-rmse:1.049945 test-rmse:2.655150 \n#> [49] train-rmse:1.035587 test-rmse:2.660354 \n#> [50] train-rmse:1.027057 test-rmse:2.658988 \n#> [51] train-rmse:1.005431 test-rmse:2.649592 \n#> [52] train-rmse:0.987424 test-rmse:2.645430 \n#> [53] train-rmse:0.971436 test-rmse:2.647272 \n#> [54] train-rmse:0.951233 test-rmse:2.637944 \n#> [55] train-rmse:0.934642 test-rmse:2.638550 \n#> [56] train-rmse:0.914529 test-rmse:2.639281 \n#> [57] train-rmse:0.905744 test-rmse:2.634091 \n#> [58] train-rmse:0.897733 test-rmse:2.635307 \n#> [59] train-rmse:0.883033 test-rmse:2.625912 \n#> [60] train-rmse:0.865160 test-rmse:2.627433 \n#> [61] train-rmse:0.853266 test-rmse:2.628132 \n#> [62] train-rmse:0.848135 test-rmse:2.625140 \n#> [63] train-rmse:0.835240 test-rmse:2.620551 \n#> [64] train-rmse:0.819862 test-rmse:2.629525 \n#> [65] train-rmse:0.816406 test-rmse:2.631767 \n#> [66] train-rmse:0.797353 test-rmse:2.636911 \n#> [67] train-rmse:0.789818 test-rmse:2.636512 \n#> [68] train-rmse:0.775842 test-rmse:2.636128 \n#> [69] train-rmse:0.762010 test-rmse:2.640448 \n#> [70] train-rmse:0.755448 test-rmse:2.634867 \n#> [1]  train-rmse:17.369700    test-rmse:17.163387 \n#> [2]  train-rmse:12.538664    test-rmse:12.632404 \n#> [3]  train-rmse:9.180276 test-rmse:9.463454 \n#> [4]  train-rmse:6.822798 test-rmse:7.408056 \n#> [5]  train-rmse:5.157894 test-rmse:6.144156 \n#> [6]  train-rmse:4.053398 test-rmse:5.392924 \n#> [7]  train-rmse:3.282756 test-rmse:4.813340 \n#> [8]  train-rmse:2.763495 test-rmse:4.541041 \n#> [9]  train-rmse:2.432271 test-rmse:4.383377 \n#> [10] train-rmse:2.200080 test-rmse:4.191137 \n#> [11] train-rmse:2.020068 test-rmse:4.085083 \n#> [12] train-rmse:1.924612 test-rmse:4.041232 \n#> [13] train-rmse:1.848469 test-rmse:4.027526 \n#> [14] train-rmse:1.782436 test-rmse:4.080986 \n#> [15] train-rmse:1.730807 test-rmse:4.065269 \n#> [16] train-rmse:1.664313 test-rmse:4.020690 \n#> [17] train-rmse:1.614103 test-rmse:3.966595 \n#> [18] train-rmse:1.575948 test-rmse:3.973121 \n#> [19] train-rmse:1.547222 test-rmse:3.971151 \n#> [20] train-rmse:1.514206 test-rmse:3.951577 \n#> [21] train-rmse:1.462316 test-rmse:3.959171 \n#> [22] train-rmse:1.440970 test-rmse:3.968296 \n#> [23] train-rmse:1.399896 test-rmse:3.967540 \n#> [24] train-rmse:1.368519 test-rmse:3.964019 \n#> [25] train-rmse:1.353368 test-rmse:3.948828 \n#> [26] train-rmse:1.327715 test-rmse:3.951640 \n#> [27] train-rmse:1.293445 test-rmse:3.937234 \n#> [28] train-rmse:1.272693 test-rmse:3.912807 \n#> [29] train-rmse:1.240026 test-rmse:3.901195 \n#> [30] train-rmse:1.206814 test-rmse:3.899558 \n#> [31] train-rmse:1.193308 test-rmse:3.902120 \n#> [32] train-rmse:1.179732 test-rmse:3.887835 \n#> [33] train-rmse:1.130219 test-rmse:3.853357 \n#> [34] train-rmse:1.108433 test-rmse:3.837914 \n#> [35] train-rmse:1.082396 test-rmse:3.830936 \n#> [36] train-rmse:1.072405 test-rmse:3.831802 \n#> [37] train-rmse:1.059950 test-rmse:3.836254 \n#> [38] train-rmse:1.040796 test-rmse:3.836272 \n#> [39] train-rmse:1.034363 test-rmse:3.835807 \n#> [40] train-rmse:1.009050 test-rmse:3.830396 \n#> [41] train-rmse:0.992680 test-rmse:3.829704 \n#> [42] train-rmse:0.979284 test-rmse:3.818401 \n#> [43] train-rmse:0.959056 test-rmse:3.817649 \n#> [44] train-rmse:0.931938 test-rmse:3.802608 \n#> [45] train-rmse:0.915750 test-rmse:3.806075 \n#> [46] train-rmse:0.902265 test-rmse:3.805916 \n#> [47] train-rmse:0.874482 test-rmse:3.802125 \n#> [48] train-rmse:0.864981 test-rmse:3.801639 \n#> [49] train-rmse:0.852830 test-rmse:3.806106 \n#> [50] train-rmse:0.835721 test-rmse:3.797447 \n#> [51] train-rmse:0.824056 test-rmse:3.802764 \n#> [52] train-rmse:0.808376 test-rmse:3.800457 \n#> [53] train-rmse:0.800746 test-rmse:3.802641 \n#> [54] train-rmse:0.783422 test-rmse:3.796594 \n#> [55] train-rmse:0.771401 test-rmse:3.804255 \n#> [56] train-rmse:0.762217 test-rmse:3.798605 \n#> [57] train-rmse:0.749706 test-rmse:3.799337 \n#> [58] train-rmse:0.739627 test-rmse:3.796423 \n#> [59] train-rmse:0.721745 test-rmse:3.796050 \n#> [60] train-rmse:0.709733 test-rmse:3.789439 \n#> [61] train-rmse:0.702526 test-rmse:3.787663 \n#> [62] train-rmse:0.696618 test-rmse:3.785630 \n#> [63] train-rmse:0.690819 test-rmse:3.780841 \n#> [64] train-rmse:0.679432 test-rmse:3.782988 \n#> [65] train-rmse:0.674282 test-rmse:3.784567 \n#> [66] train-rmse:0.664273 test-rmse:3.784069 \n#> [67] train-rmse:0.651228 test-rmse:3.785054 \n#> [68] train-rmse:0.644693 test-rmse:3.786495 \n#> [69] train-rmse:0.638943 test-rmse:3.788826 \n#> [70] train-rmse:0.631541 test-rmse:3.779159 \n#> [1]  train-rmse:16.851618    test-rmse:17.689038 \n#> [2]  train-rmse:12.247046    test-rmse:13.146214 \n#> [3]  train-rmse:9.055132 test-rmse:9.992292 \n#> [4]  train-rmse:6.811049 test-rmse:7.597927 \n#> [5]  train-rmse:5.287718 test-rmse:6.017995 \n#> [6]  train-rmse:4.287591 test-rmse:5.168742 \n#> [7]  train-rmse:3.604025 test-rmse:4.545065 \n#> [8]  train-rmse:3.144249 test-rmse:4.192109 \n#> [9]  train-rmse:2.836724 test-rmse:3.936045 \n#> [10] train-rmse:2.629665 test-rmse:3.844183 \n#> [11] train-rmse:2.446611 test-rmse:3.774374 \n#> [12] train-rmse:2.350734 test-rmse:3.656684 \n#> [13] train-rmse:2.246979 test-rmse:3.608158 \n#> [14] train-rmse:2.199923 test-rmse:3.614383 \n#> [15] train-rmse:2.130916 test-rmse:3.566512 \n#> [16] train-rmse:2.092222 test-rmse:3.569966 \n#> [17] train-rmse:2.056247 test-rmse:3.531677 \n#> [18] train-rmse:1.989196 test-rmse:3.509704 \n#> [19] train-rmse:1.904293 test-rmse:3.526812 \n#> [20] train-rmse:1.874183 test-rmse:3.542455 \n#> [21] train-rmse:1.836572 test-rmse:3.529170 \n#> [22] train-rmse:1.803295 test-rmse:3.528720 \n#> [23] train-rmse:1.751647 test-rmse:3.509608 \n#> [24] train-rmse:1.733535 test-rmse:3.533424 \n#> [25] train-rmse:1.690835 test-rmse:3.526375 \n#> [26] train-rmse:1.675032 test-rmse:3.528541 \n#> [27] train-rmse:1.637364 test-rmse:3.531943 \n#> [28] train-rmse:1.595909 test-rmse:3.532742 \n#> [29] train-rmse:1.530506 test-rmse:3.529922 \n#> [30] train-rmse:1.495419 test-rmse:3.524897 \n#> [31] train-rmse:1.482784 test-rmse:3.530953 \n#> [32] train-rmse:1.459573 test-rmse:3.524708 \n#> [33] train-rmse:1.443076 test-rmse:3.540299 \n#> [34] train-rmse:1.419467 test-rmse:3.559708 \n#> [35] train-rmse:1.393598 test-rmse:3.551996 \n#> [36] train-rmse:1.381127 test-rmse:3.549354 \n#> [37] train-rmse:1.343972 test-rmse:3.546839 \n#> [38] train-rmse:1.293013 test-rmse:3.547004 \n#> [39] train-rmse:1.258523 test-rmse:3.550533 \n#> [40] train-rmse:1.227583 test-rmse:3.552073 \n#> [41] train-rmse:1.218001 test-rmse:3.565416 \n#> [42] train-rmse:1.191352 test-rmse:3.555782 \n#> [43] train-rmse:1.167781 test-rmse:3.555559 \n#> [44] train-rmse:1.154672 test-rmse:3.560110 \n#> [45] train-rmse:1.143491 test-rmse:3.561748 \n#> [46] train-rmse:1.112355 test-rmse:3.573442 \n#> [47] train-rmse:1.104252 test-rmse:3.576608 \n#> [48] train-rmse:1.086042 test-rmse:3.564718 \n#> [49] train-rmse:1.065225 test-rmse:3.573229 \n#> [50] train-rmse:1.055505 test-rmse:3.572608 \n#> [51] train-rmse:1.050356 test-rmse:3.574039 \n#> [52] train-rmse:1.024839 test-rmse:3.582760 \n#> [53] train-rmse:1.007063 test-rmse:3.594317 \n#> [54] train-rmse:0.989760 test-rmse:3.596190 \n#> [55] train-rmse:0.985288 test-rmse:3.589626 \n#> [56] train-rmse:0.968332 test-rmse:3.587761 \n#> [57] train-rmse:0.955451 test-rmse:3.588921 \n#> [58] train-rmse:0.935644 test-rmse:3.581341 \n#> [59] train-rmse:0.915201 test-rmse:3.576290 \n#> [60] train-rmse:0.894053 test-rmse:3.575263 \n#> [61] train-rmse:0.886351 test-rmse:3.578476 \n#> [62] train-rmse:0.868200 test-rmse:3.574945 \n#> [63] train-rmse:0.864606 test-rmse:3.570811 \n#> [64] train-rmse:0.843844 test-rmse:3.571999 \n#> [65] train-rmse:0.825551 test-rmse:3.578927 \n#> [66] train-rmse:0.822725 test-rmse:3.575870 \n#> [67] train-rmse:0.814655 test-rmse:3.573540 \n#> [68] train-rmse:0.803614 test-rmse:3.577983 \n#> [69] train-rmse:0.797038 test-rmse:3.584130 \n#> [70] train-rmse:0.791149 test-rmse:3.583348 \n#> [1]  train-rmse:17.477357    test-rmse:16.527321 \n#> [2]  train-rmse:12.664641    test-rmse:11.995947 \n#> [3]  train-rmse:9.302649 test-rmse:8.958430 \n#> [4]  train-rmse:6.986884 test-rmse:6.865748 \n#> [5]  train-rmse:5.376294 test-rmse:5.499326 \n#> [6]  train-rmse:4.299956 test-rmse:4.772482 \n#> [7]  train-rmse:3.560704 test-rmse:4.274495 \n#> [8]  train-rmse:3.081282 test-rmse:3.934724 \n#> [9]  train-rmse:2.734935 test-rmse:3.754312 \n#> [10] train-rmse:2.515792 test-rmse:3.590660 \n#> [11] train-rmse:2.348332 test-rmse:3.475234 \n#> [12] train-rmse:2.250497 test-rmse:3.442904 \n#> [13] train-rmse:2.157210 test-rmse:3.383382 \n#> [14] train-rmse:2.092833 test-rmse:3.351163 \n#> [15] train-rmse:2.019703 test-rmse:3.355036 \n#> [16] train-rmse:1.964612 test-rmse:3.332666 \n#> [17] train-rmse:1.907572 test-rmse:3.299823 \n#> [18] train-rmse:1.874738 test-rmse:3.292053 \n#> [19] train-rmse:1.822290 test-rmse:3.308171 \n#> [20] train-rmse:1.774891 test-rmse:3.277486 \n#> [21] train-rmse:1.727159 test-rmse:3.252848 \n#> [22] train-rmse:1.705368 test-rmse:3.217827 \n#> [23] train-rmse:1.684295 test-rmse:3.203909 \n#> [24] train-rmse:1.653245 test-rmse:3.201445 \n#> [25] train-rmse:1.617504 test-rmse:3.202111 \n#> [26] train-rmse:1.580065 test-rmse:3.188726 \n#> [27] train-rmse:1.546369 test-rmse:3.184266 \n#> [28] train-rmse:1.516384 test-rmse:3.181718 \n#> [29] train-rmse:1.476499 test-rmse:3.170822 \n#> [30] train-rmse:1.449459 test-rmse:3.162337 \n#> [31] train-rmse:1.416426 test-rmse:3.176795 \n#> [32] train-rmse:1.398788 test-rmse:3.177891 \n#> [33] train-rmse:1.367230 test-rmse:3.175756 \n#> [34] train-rmse:1.337373 test-rmse:3.174424 \n#> [35] train-rmse:1.314864 test-rmse:3.169255 \n#> [36] train-rmse:1.294271 test-rmse:3.176421 \n#> [37] train-rmse:1.282166 test-rmse:3.171310 \n#> [38] train-rmse:1.263780 test-rmse:3.168343 \n#> [39] train-rmse:1.250331 test-rmse:3.168549 \n#> [40] train-rmse:1.239088 test-rmse:3.170896 \n#> [41] train-rmse:1.227630 test-rmse:3.165811 \n#> [42] train-rmse:1.219705 test-rmse:3.167467 \n#> [43] train-rmse:1.197678 test-rmse:3.158448 \n#> [44] train-rmse:1.189214 test-rmse:3.150690 \n#> [45] train-rmse:1.174734 test-rmse:3.144025 \n#> [46] train-rmse:1.167863 test-rmse:3.140134 \n#> [47] train-rmse:1.153933 test-rmse:3.138493 \n#> [48] train-rmse:1.144011 test-rmse:3.136328 \n#> [49] train-rmse:1.117278 test-rmse:3.118836 \n#> [50] train-rmse:1.094564 test-rmse:3.093019 \n#> [51] train-rmse:1.078295 test-rmse:3.092524 \n#> [52] train-rmse:1.059747 test-rmse:3.090600 \n#> [53] train-rmse:1.055997 test-rmse:3.091937 \n#> [54] train-rmse:1.038776 test-rmse:3.088562 \n#> [55] train-rmse:1.024767 test-rmse:3.092815 \n#> [56] train-rmse:1.009588 test-rmse:3.096786 \n#> [57] train-rmse:0.987335 test-rmse:3.099280 \n#> [58] train-rmse:0.977918 test-rmse:3.100591 \n#> [59] train-rmse:0.965779 test-rmse:3.105325 \n#> [60] train-rmse:0.959166 test-rmse:3.106332 \n#> [61] train-rmse:0.941844 test-rmse:3.110665 \n#> [62] train-rmse:0.927847 test-rmse:3.097986 \n#> [63] train-rmse:0.908501 test-rmse:3.103467 \n#> [64] train-rmse:0.897267 test-rmse:3.105768 \n#> [65] train-rmse:0.885763 test-rmse:3.104582 \n#> [66] train-rmse:0.877413 test-rmse:3.109884 \n#> [67] train-rmse:0.858192 test-rmse:3.113015 \n#> [68] train-rmse:0.841577 test-rmse:3.112886 \n#> [69] train-rmse:0.827335 test-rmse:3.111785 \n#> [70] train-rmse:0.810480 test-rmse:3.102859 \n#> [1]  train-rmse:17.516712    test-rmse:16.439532 \n#> [2]  train-rmse:12.704872    test-rmse:11.972499 \n#> [3]  train-rmse:9.309501 test-rmse:8.962723 \n#> [4]  train-rmse:6.952581 test-rmse:6.891552 \n#> [5]  train-rmse:5.359371 test-rmse:5.631844 \n#> [6]  train-rmse:4.216955 test-rmse:4.851716 \n#> [7]  train-rmse:3.465746 test-rmse:4.414256 \n#> [8]  train-rmse:2.948136 test-rmse:4.126199 \n#> [9]  train-rmse:2.632189 test-rmse:3.988165 \n#> [10] train-rmse:2.396595 test-rmse:3.910492 \n#> [11] train-rmse:2.235672 test-rmse:3.861430 \n#> [12] train-rmse:2.128849 test-rmse:3.879963 \n#> [13] train-rmse:2.007587 test-rmse:3.839521 \n#> [14] train-rmse:1.917538 test-rmse:3.819519 \n#> [15] train-rmse:1.834964 test-rmse:3.834588 \n#> [16] train-rmse:1.799470 test-rmse:3.846834 \n#> [17] train-rmse:1.732873 test-rmse:3.842931 \n#> [18] train-rmse:1.666987 test-rmse:3.819122 \n#> [19] train-rmse:1.625099 test-rmse:3.788526 \n#> [20] train-rmse:1.588039 test-rmse:3.763336 \n#> [21] train-rmse:1.558551 test-rmse:3.747225 \n#> [22] train-rmse:1.510424 test-rmse:3.756382 \n#> [23] train-rmse:1.497681 test-rmse:3.734523 \n#> [24] train-rmse:1.482040 test-rmse:3.733861 \n#> [25] train-rmse:1.463634 test-rmse:3.753645 \n#> [26] train-rmse:1.448568 test-rmse:3.741722 \n#> [27] train-rmse:1.419823 test-rmse:3.727728 \n#> [28] train-rmse:1.399649 test-rmse:3.719883 \n#> [29] train-rmse:1.353190 test-rmse:3.690821 \n#> [30] train-rmse:1.319409 test-rmse:3.697237 \n#> [31] train-rmse:1.305254 test-rmse:3.692799 \n#> [32] train-rmse:1.284274 test-rmse:3.683033 \n#> [33] train-rmse:1.260786 test-rmse:3.694678 \n#> [34] train-rmse:1.252847 test-rmse:3.687495 \n#> [35] train-rmse:1.232505 test-rmse:3.690181 \n#> [36] train-rmse:1.216862 test-rmse:3.689011 \n#> [37] train-rmse:1.180909 test-rmse:3.689362 \n#> [38] train-rmse:1.163879 test-rmse:3.686103 \n#> [39] train-rmse:1.146814 test-rmse:3.695191 \n#> [40] train-rmse:1.118845 test-rmse:3.702853 \n#> [41] train-rmse:1.091918 test-rmse:3.707081 \n#> [42] train-rmse:1.076218 test-rmse:3.704513 \n#> [43] train-rmse:1.056559 test-rmse:3.704579 \n#> [44] train-rmse:1.034916 test-rmse:3.696735 \n#> [45] train-rmse:1.017518 test-rmse:3.686453 \n#> [46] train-rmse:1.010944 test-rmse:3.672369 \n#> [47] train-rmse:0.998823 test-rmse:3.671160 \n#> [48] train-rmse:0.985772 test-rmse:3.669297 \n#> [49] train-rmse:0.967363 test-rmse:3.678855 \n#> [50] train-rmse:0.944085 test-rmse:3.677862 \n#> [51] train-rmse:0.925227 test-rmse:3.675951 \n#> [52] train-rmse:0.921101 test-rmse:3.666867 \n#> [53] train-rmse:0.902544 test-rmse:3.658930 \n#> [54] train-rmse:0.881444 test-rmse:3.660465 \n#> [55] train-rmse:0.872768 test-rmse:3.665478 \n#> [56] train-rmse:0.864121 test-rmse:3.662788 \n#> [57] train-rmse:0.848972 test-rmse:3.651639 \n#> [58] train-rmse:0.828335 test-rmse:3.641569 \n#> [59] train-rmse:0.816252 test-rmse:3.636778 \n#> [60] train-rmse:0.806125 test-rmse:3.639459 \n#> [61] train-rmse:0.788547 test-rmse:3.634627 \n#> [62] train-rmse:0.773397 test-rmse:3.629928 \n#> [63] train-rmse:0.757270 test-rmse:3.619143 \n#> [64] train-rmse:0.743927 test-rmse:3.622712 \n#> [65] train-rmse:0.734305 test-rmse:3.624204 \n#> [66] train-rmse:0.723893 test-rmse:3.626602 \n#> [67] train-rmse:0.715963 test-rmse:3.626080 \n#> [68] train-rmse:0.708651 test-rmse:3.625414 \n#> [69] train-rmse:0.692336 test-rmse:3.621810 \n#> [70] train-rmse:0.684827 test-rmse:3.617717 \n#> [1]  train-rmse:16.922243    test-rmse:17.651039 \n#> [2]  train-rmse:12.250031    test-rmse:12.887329 \n#> [3]  train-rmse:9.013010 test-rmse:9.756794 \n#> [4]  train-rmse:6.793117 test-rmse:7.768504 \n#> [5]  train-rmse:5.235041 test-rmse:6.281144 \n#> [6]  train-rmse:4.170303 test-rmse:5.362771 \n#> [7]  train-rmse:3.486009 test-rmse:4.830497 \n#> [8]  train-rmse:3.051639 test-rmse:4.501959 \n#> [9]  train-rmse:2.750394 test-rmse:4.297157 \n#> [10] train-rmse:2.528267 test-rmse:4.143705 \n#> [11] train-rmse:2.385107 test-rmse:4.056694 \n#> [12] train-rmse:2.282677 test-rmse:4.011703 \n#> [13] train-rmse:2.206975 test-rmse:3.983616 \n#> [14] train-rmse:2.131304 test-rmse:3.921142 \n#> [15] train-rmse:2.077097 test-rmse:3.898018 \n#> [16] train-rmse:1.992156 test-rmse:3.874507 \n#> [17] train-rmse:1.946122 test-rmse:3.832192 \n#> [18] train-rmse:1.879951 test-rmse:3.791515 \n#> [19] train-rmse:1.814209 test-rmse:3.778795 \n#> [20] train-rmse:1.777613 test-rmse:3.767908 \n#> [21] train-rmse:1.730406 test-rmse:3.749057 \n#> [22] train-rmse:1.710667 test-rmse:3.742828 \n#> [23] train-rmse:1.672707 test-rmse:3.727354 \n#> [24] train-rmse:1.624492 test-rmse:3.710978 \n#> [25] train-rmse:1.585969 test-rmse:3.717527 \n#> [26] train-rmse:1.539809 test-rmse:3.718164 \n#> [27] train-rmse:1.502219 test-rmse:3.711564 \n#> [28] train-rmse:1.487363 test-rmse:3.716270 \n#> [29] train-rmse:1.472990 test-rmse:3.729529 \n#> [30] train-rmse:1.456847 test-rmse:3.730548 \n#> [31] train-rmse:1.438577 test-rmse:3.720914 \n#> [32] train-rmse:1.414038 test-rmse:3.713137 \n#> [33] train-rmse:1.393750 test-rmse:3.698787 \n#> [34] train-rmse:1.377710 test-rmse:3.685371 \n#> [35] train-rmse:1.366862 test-rmse:3.677508 \n#> [36] train-rmse:1.353890 test-rmse:3.680680 \n#> [37] train-rmse:1.318616 test-rmse:3.679199 \n#> [38] train-rmse:1.292043 test-rmse:3.676492 \n#> [39] train-rmse:1.269161 test-rmse:3.670951 \n#> [40] train-rmse:1.256788 test-rmse:3.679901 \n#> [41] train-rmse:1.237353 test-rmse:3.677076 \n#> [42] train-rmse:1.217982 test-rmse:3.671050 \n#> [43] train-rmse:1.182463 test-rmse:3.674857 \n#> [44] train-rmse:1.157537 test-rmse:3.667582 \n#> [45] train-rmse:1.145929 test-rmse:3.660398 \n#> [46] train-rmse:1.124955 test-rmse:3.661673 \n#> [47] train-rmse:1.108189 test-rmse:3.659959 \n#> [48] train-rmse:1.102524 test-rmse:3.661394 \n#> [49] train-rmse:1.064052 test-rmse:3.663788 \n#> [50] train-rmse:1.034119 test-rmse:3.665225 \n#> [51] train-rmse:1.010867 test-rmse:3.658235 \n#> [52] train-rmse:1.002326 test-rmse:3.648886 \n#> [53] train-rmse:0.982503 test-rmse:3.663341 \n#> [54] train-rmse:0.970714 test-rmse:3.660239 \n#> [55] train-rmse:0.955684 test-rmse:3.655650 \n#> [56] train-rmse:0.933728 test-rmse:3.649022 \n#> [57] train-rmse:0.921975 test-rmse:3.646999 \n#> [58] train-rmse:0.907513 test-rmse:3.651628 \n#> [59] train-rmse:0.886558 test-rmse:3.654107 \n#> [60] train-rmse:0.866756 test-rmse:3.662212 \n#> [61] train-rmse:0.862832 test-rmse:3.658910 \n#> [62] train-rmse:0.847068 test-rmse:3.651673 \n#> [63] train-rmse:0.837279 test-rmse:3.652087 \n#> [64] train-rmse:0.822307 test-rmse:3.651748 \n#> [65] train-rmse:0.807431 test-rmse:3.652063 \n#> [66] train-rmse:0.797902 test-rmse:3.655239 \n#> [67] train-rmse:0.791800 test-rmse:3.658549 \n#> [68] train-rmse:0.776808 test-rmse:3.658271 \n#> [69] train-rmse:0.770397 test-rmse:3.659264 \n#> [70] train-rmse:0.767124 test-rmse:3.662660 \n#> [1]  train-rmse:17.152837    test-rmse:17.312627 \n#> [2]  train-rmse:12.417863    test-rmse:12.623049 \n#> [3]  train-rmse:9.109260 test-rmse:9.430341 \n#> [4]  train-rmse:6.806087 test-rmse:7.340749 \n#> [5]  train-rmse:5.236341 test-rmse:5.969851 \n#> [6]  train-rmse:4.124409 test-rmse:5.180520 \n#> [7]  train-rmse:3.409541 test-rmse:4.725769 \n#> [8]  train-rmse:2.907675 test-rmse:4.370898 \n#> [9]  train-rmse:2.616327 test-rmse:4.193904 \n#> [10] train-rmse:2.403490 test-rmse:4.063455 \n#> [11] train-rmse:2.245480 test-rmse:3.954606 \n#> [12] train-rmse:2.131974 test-rmse:3.909208 \n#> [13] train-rmse:2.056478 test-rmse:3.883845 \n#> [14] train-rmse:1.988316 test-rmse:3.829907 \n#> [15] train-rmse:1.918525 test-rmse:3.846480 \n#> [16] train-rmse:1.853409 test-rmse:3.844287 \n#> [17] train-rmse:1.764102 test-rmse:3.808609 \n#> [18] train-rmse:1.728920 test-rmse:3.798865 \n#> [19] train-rmse:1.686462 test-rmse:3.801505 \n#> [20] train-rmse:1.643720 test-rmse:3.768817 \n#> [21] train-rmse:1.615638 test-rmse:3.772158 \n#> [22] train-rmse:1.581222 test-rmse:3.754727 \n#> [23] train-rmse:1.532340 test-rmse:3.742759 \n#> [24] train-rmse:1.511468 test-rmse:3.754419 \n#> [25] train-rmse:1.464715 test-rmse:3.738069 \n#> [26] train-rmse:1.425049 test-rmse:3.750589 \n#> [27] train-rmse:1.403122 test-rmse:3.755480 \n#> [28] train-rmse:1.358186 test-rmse:3.725604 \n#> [29] train-rmse:1.339987 test-rmse:3.718350 \n#> [30] train-rmse:1.315209 test-rmse:3.706850 \n#> [31] train-rmse:1.283096 test-rmse:3.697370 \n#> [32] train-rmse:1.253204 test-rmse:3.704360 \n#> [33] train-rmse:1.239898 test-rmse:3.701379 \n#> [34] train-rmse:1.218439 test-rmse:3.705304 \n#> [35] train-rmse:1.202165 test-rmse:3.717953 \n#> [36] train-rmse:1.188075 test-rmse:3.712864 \n#> [37] train-rmse:1.172806 test-rmse:3.706612 \n#> [38] train-rmse:1.130514 test-rmse:3.693831 \n#> [39] train-rmse:1.123755 test-rmse:3.687604 \n#> [40] train-rmse:1.110395 test-rmse:3.683130 \n#> [41] train-rmse:1.088629 test-rmse:3.684874 \n#> [42] train-rmse:1.057146 test-rmse:3.673015 \n#> [43] train-rmse:1.037357 test-rmse:3.669375 \n#> [44] train-rmse:1.012744 test-rmse:3.667050 \n#> [45] train-rmse:0.995983 test-rmse:3.663645 \n#> [46] train-rmse:0.987530 test-rmse:3.662034 \n#> [47] train-rmse:0.982015 test-rmse:3.661063 \n#> [48] train-rmse:0.963825 test-rmse:3.663633 \n#> [49] train-rmse:0.949556 test-rmse:3.656571 \n#> [50] train-rmse:0.929863 test-rmse:3.654831 \n#> [51] train-rmse:0.917359 test-rmse:3.656136 \n#> [52] train-rmse:0.906694 test-rmse:3.650071 \n#> [53] train-rmse:0.896932 test-rmse:3.651673 \n#> [54] train-rmse:0.886899 test-rmse:3.646640 \n#> [55] train-rmse:0.881587 test-rmse:3.644913 \n#> [56] train-rmse:0.865127 test-rmse:3.649565 \n#> [57] train-rmse:0.852795 test-rmse:3.653401 \n#> [58] train-rmse:0.832879 test-rmse:3.655664 \n#> [59] train-rmse:0.828237 test-rmse:3.649560 \n#> [60] train-rmse:0.813814 test-rmse:3.649660 \n#> [61] train-rmse:0.801674 test-rmse:3.646815 \n#> [62] train-rmse:0.795732 test-rmse:3.652086 \n#> [63] train-rmse:0.792014 test-rmse:3.650749 \n#> [64] train-rmse:0.780086 test-rmse:3.654672 \n#> [65] train-rmse:0.768030 test-rmse:3.653059 \n#> [66] train-rmse:0.759222 test-rmse:3.653827 \n#> [67] train-rmse:0.751381 test-rmse:3.647739 \n#> [68] train-rmse:0.733078 test-rmse:3.651626 \n#> [69] train-rmse:0.715336 test-rmse:3.653775 \n#> [70] train-rmse:0.703349 test-rmse:3.657565 \n#> [1]  train-rmse:17.431329    test-rmse:16.703654 \n#> [2]  train-rmse:12.696194    test-rmse:12.218873 \n#> [3]  train-rmse:9.337002 test-rmse:9.118414 \n#> [4]  train-rmse:7.018587 test-rmse:7.088290 \n#> [5]  train-rmse:5.415362 test-rmse:5.711857 \n#> [6]  train-rmse:4.329083 test-rmse:4.831852 \n#> [7]  train-rmse:3.601945 test-rmse:4.270706 \n#> [8]  train-rmse:3.149155 test-rmse:4.009935 \n#> [9]  train-rmse:2.825517 test-rmse:3.852722 \n#> [10] train-rmse:2.529904 test-rmse:3.700324 \n#> [11] train-rmse:2.371822 test-rmse:3.619504 \n#> [12] train-rmse:2.248299 test-rmse:3.526853 \n#> [13] train-rmse:2.148486 test-rmse:3.510621 \n#> [14] train-rmse:2.083448 test-rmse:3.522890 \n#> [15] train-rmse:2.014768 test-rmse:3.492488 \n#> [16] train-rmse:1.969852 test-rmse:3.492779 \n#> [17] train-rmse:1.919457 test-rmse:3.484013 \n#> [18] train-rmse:1.836285 test-rmse:3.480110 \n#> [19] train-rmse:1.789915 test-rmse:3.494580 \n#> [20] train-rmse:1.745980 test-rmse:3.485723 \n#> [21] train-rmse:1.688302 test-rmse:3.466251 \n#> [22] train-rmse:1.651237 test-rmse:3.448205 \n#> [23] train-rmse:1.624050 test-rmse:3.451121 \n#> [24] train-rmse:1.544799 test-rmse:3.454887 \n#> [25] train-rmse:1.508494 test-rmse:3.443285 \n#> [26] train-rmse:1.494843 test-rmse:3.448496 \n#> [27] train-rmse:1.470220 test-rmse:3.435564 \n#> [28] train-rmse:1.425408 test-rmse:3.425121 \n#> [29] train-rmse:1.380475 test-rmse:3.413315 \n#> [30] train-rmse:1.340690 test-rmse:3.408225 \n#> [31] train-rmse:1.318358 test-rmse:3.410019 \n#> [32] train-rmse:1.305773 test-rmse:3.408686 \n#> [33] train-rmse:1.281862 test-rmse:3.411578 \n#> [34] train-rmse:1.269213 test-rmse:3.401081 \n#> [35] train-rmse:1.243276 test-rmse:3.401679 \n#> [36] train-rmse:1.230148 test-rmse:3.400291 \n#> [37] train-rmse:1.212566 test-rmse:3.396719 \n#> [38] train-rmse:1.199863 test-rmse:3.398044 \n#> [39] train-rmse:1.171560 test-rmse:3.402795 \n#> [40] train-rmse:1.154470 test-rmse:3.399611 \n#> [41] train-rmse:1.125709 test-rmse:3.389426 \n#> [42] train-rmse:1.117133 test-rmse:3.384284 \n#> [43] train-rmse:1.100629 test-rmse:3.384461 \n#> [44] train-rmse:1.074666 test-rmse:3.384487 \n#> [45] train-rmse:1.048657 test-rmse:3.389365 \n#> [46] train-rmse:1.040674 test-rmse:3.390344 \n#> [47] train-rmse:1.030349 test-rmse:3.391706 \n#> [48] train-rmse:1.004669 test-rmse:3.384443 \n#> [49] train-rmse:0.996423 test-rmse:3.387312 \n#> [50] train-rmse:0.986165 test-rmse:3.379558 \n#> [51] train-rmse:0.970477 test-rmse:3.379549 \n#> [52] train-rmse:0.965665 test-rmse:3.380382 \n#> [53] train-rmse:0.948489 test-rmse:3.382174 \n#> [54] train-rmse:0.910145 test-rmse:3.388917 \n#> [55] train-rmse:0.899244 test-rmse:3.380107 \n#> [56] train-rmse:0.894504 test-rmse:3.377116 \n#> [57] train-rmse:0.888464 test-rmse:3.379834 \n#> [58] train-rmse:0.870660 test-rmse:3.377150 \n#> [59] train-rmse:0.866385 test-rmse:3.382585 \n#> [60] train-rmse:0.856161 test-rmse:3.374949 \n#> [61] train-rmse:0.850367 test-rmse:3.375091 \n#> [62] train-rmse:0.843837 test-rmse:3.378089 \n#> [63] train-rmse:0.835610 test-rmse:3.380332 \n#> [64] train-rmse:0.828715 test-rmse:3.380860 \n#> [65] train-rmse:0.821685 test-rmse:3.381212 \n#> [66] train-rmse:0.805571 test-rmse:3.384954 \n#> [67] train-rmse:0.796083 test-rmse:3.388942 \n#> [68] train-rmse:0.781292 test-rmse:3.395359 \n#> [69] train-rmse:0.768496 test-rmse:3.386272 \n#> [70] train-rmse:0.760739 test-rmse:3.386573 \n#> [1]  train-rmse:17.233425    test-rmse:17.250586 \n#> [2]  train-rmse:12.485570    test-rmse:12.623902 \n#> [3]  train-rmse:9.195193 test-rmse:9.623004 \n#> [4]  train-rmse:6.837540 test-rmse:7.513643 \n#> [5]  train-rmse:5.241085 test-rmse:6.061415 \n#> [6]  train-rmse:4.133397 test-rmse:5.169767 \n#> [7]  train-rmse:3.391679 test-rmse:4.621963 \n#> [8]  train-rmse:2.904424 test-rmse:4.311491 \n#> [9]  train-rmse:2.583741 test-rmse:4.097347 \n#> [10] train-rmse:2.386201 test-rmse:3.944636 \n#> [11] train-rmse:2.250730 test-rmse:3.868993 \n#> [12] train-rmse:2.108683 test-rmse:3.764592 \n#> [13] train-rmse:2.052999 test-rmse:3.729400 \n#> [14] train-rmse:1.976534 test-rmse:3.697717 \n#> [15] train-rmse:1.921351 test-rmse:3.658235 \n#> [16] train-rmse:1.823806 test-rmse:3.602679 \n#> [17] train-rmse:1.791829 test-rmse:3.590524 \n#> [18] train-rmse:1.732673 test-rmse:3.588665 \n#> [19] train-rmse:1.691548 test-rmse:3.541515 \n#> [20] train-rmse:1.663846 test-rmse:3.533724 \n#> [21] train-rmse:1.644180 test-rmse:3.518984 \n#> [22] train-rmse:1.574901 test-rmse:3.499891 \n#> [23] train-rmse:1.559760 test-rmse:3.497970 \n#> [24] train-rmse:1.511802 test-rmse:3.470968 \n#> [25] train-rmse:1.472486 test-rmse:3.463712 \n#> [26] train-rmse:1.453882 test-rmse:3.467987 \n#> [27] train-rmse:1.425199 test-rmse:3.454385 \n#> [28] train-rmse:1.371651 test-rmse:3.477113 \n#> [29] train-rmse:1.356004 test-rmse:3.478002 \n#> [30] train-rmse:1.341601 test-rmse:3.466878 \n#> [31] train-rmse:1.324580 test-rmse:3.460670 \n#> [32] train-rmse:1.295253 test-rmse:3.451498 \n#> [33] train-rmse:1.283414 test-rmse:3.444746 \n#> [34] train-rmse:1.253856 test-rmse:3.437694 \n#> [35] train-rmse:1.236857 test-rmse:3.436742 \n#> [36] train-rmse:1.215436 test-rmse:3.425606 \n#> [37] train-rmse:1.180066 test-rmse:3.407364 \n#> [38] train-rmse:1.157078 test-rmse:3.406668 \n#> [39] train-rmse:1.141348 test-rmse:3.401793 \n#> [40] train-rmse:1.134249 test-rmse:3.398003 \n#> [41] train-rmse:1.100634 test-rmse:3.382796 \n#> [42] train-rmse:1.091744 test-rmse:3.386709 \n#> [43] train-rmse:1.061454 test-rmse:3.377536 \n#> [44] train-rmse:1.039501 test-rmse:3.379761 \n#> [45] train-rmse:1.020237 test-rmse:3.375047 \n#> [46] train-rmse:1.000813 test-rmse:3.376329 \n#> [47] train-rmse:0.977879 test-rmse:3.370643 \n#> [48] train-rmse:0.974470 test-rmse:3.368579 \n#> [49] train-rmse:0.957972 test-rmse:3.370339 \n#> [50] train-rmse:0.951925 test-rmse:3.369920 \n#> [51] train-rmse:0.935729 test-rmse:3.366010 \n#> [52] train-rmse:0.923628 test-rmse:3.372610 \n#> [53] train-rmse:0.912039 test-rmse:3.357990 \n#> [54] train-rmse:0.895802 test-rmse:3.361504 \n#> [55] train-rmse:0.870712 test-rmse:3.362466 \n#> [56] train-rmse:0.852411 test-rmse:3.359917 \n#> [57] train-rmse:0.837318 test-rmse:3.352954 \n#> [58] train-rmse:0.822292 test-rmse:3.342966 \n#> [59] train-rmse:0.812269 test-rmse:3.340620 \n#> [60] train-rmse:0.798920 test-rmse:3.341524 \n#> [61] train-rmse:0.787027 test-rmse:3.336136 \n#> [62] train-rmse:0.772426 test-rmse:3.333496 \n#> [63] train-rmse:0.756167 test-rmse:3.329261 \n#> [64] train-rmse:0.746415 test-rmse:3.332921 \n#> [65] train-rmse:0.733260 test-rmse:3.331723 \n#> [66] train-rmse:0.725389 test-rmse:3.327671 \n#> [67] train-rmse:0.719412 test-rmse:3.326824 \n#> [68] train-rmse:0.706745 test-rmse:3.326814 \n#> [69] train-rmse:0.697934 test-rmse:3.319295 \n#> [70] train-rmse:0.681594 test-rmse:3.318715 \n#> [1]  train-rmse:17.128594    test-rmse:17.124916 \n#> [2]  train-rmse:12.473696    test-rmse:12.576642 \n#> [3]  train-rmse:9.155455 test-rmse:9.317343 \n#> [4]  train-rmse:6.869103 test-rmse:7.051151 \n#> [5]  train-rmse:5.318934 test-rmse:5.554716 \n#> [6]  train-rmse:4.305869 test-rmse:4.620594 \n#> [7]  train-rmse:3.600153 test-rmse:3.880415 \n#> [8]  train-rmse:3.165971 test-rmse:3.499136 \n#> [9]  train-rmse:2.890240 test-rmse:3.277098 \n#> [10] train-rmse:2.648240 test-rmse:3.197614 \n#> [11] train-rmse:2.466549 test-rmse:3.075079 \n#> [12] train-rmse:2.373009 test-rmse:3.025359 \n#> [13] train-rmse:2.295470 test-rmse:2.996508 \n#> [14] train-rmse:2.242929 test-rmse:2.987894 \n#> [15] train-rmse:2.189766 test-rmse:2.936767 \n#> [16] train-rmse:2.115994 test-rmse:2.928127 \n#> [17] train-rmse:2.047733 test-rmse:2.922381 \n#> [18] train-rmse:1.993770 test-rmse:2.893833 \n#> [19] train-rmse:1.925173 test-rmse:2.868403 \n#> [20] train-rmse:1.891927 test-rmse:2.858402 \n#> [21] train-rmse:1.855631 test-rmse:2.846958 \n#> [22] train-rmse:1.817977 test-rmse:2.817564 \n#> [23] train-rmse:1.793867 test-rmse:2.826849 \n#> [24] train-rmse:1.760676 test-rmse:2.799355 \n#> [25] train-rmse:1.733016 test-rmse:2.806548 \n#> [26] train-rmse:1.698300 test-rmse:2.790279 \n#> [27] train-rmse:1.667252 test-rmse:2.818433 \n#> [28] train-rmse:1.632844 test-rmse:2.801740 \n#> [29] train-rmse:1.584815 test-rmse:2.803550 \n#> [30] train-rmse:1.559019 test-rmse:2.791002 \n#> [31] train-rmse:1.523995 test-rmse:2.788743 \n#> [32] train-rmse:1.496033 test-rmse:2.808156 \n#> [33] train-rmse:1.480189 test-rmse:2.802236 \n#> [34] train-rmse:1.453507 test-rmse:2.801068 \n#> [35] train-rmse:1.441042 test-rmse:2.798688 \n#> [36] train-rmse:1.425898 test-rmse:2.795132 \n#> [37] train-rmse:1.380954 test-rmse:2.779995 \n#> [38] train-rmse:1.356294 test-rmse:2.787496 \n#> [39] train-rmse:1.333006 test-rmse:2.790582 \n#> [40] train-rmse:1.316990 test-rmse:2.789516 \n#> [41] train-rmse:1.291145 test-rmse:2.784462 \n#> [42] train-rmse:1.267506 test-rmse:2.772754 \n#> [43] train-rmse:1.236319 test-rmse:2.773344 \n#> [44] train-rmse:1.209988 test-rmse:2.779797 \n#> [45] train-rmse:1.181657 test-rmse:2.770077 \n#> [46] train-rmse:1.156871 test-rmse:2.777735 \n#> [47] train-rmse:1.121685 test-rmse:2.780174 \n#> [48] train-rmse:1.100434 test-rmse:2.755684 \n#> [49] train-rmse:1.082500 test-rmse:2.758530 \n#> [50] train-rmse:1.060479 test-rmse:2.755751 \n#> [51] train-rmse:1.042871 test-rmse:2.749594 \n#> [52] train-rmse:1.026156 test-rmse:2.750159 \n#> [53] train-rmse:1.014550 test-rmse:2.748956 \n#> [54] train-rmse:0.992050 test-rmse:2.740920 \n#> [55] train-rmse:0.979768 test-rmse:2.744747 \n#> [56] train-rmse:0.957779 test-rmse:2.744108 \n#> [57] train-rmse:0.936233 test-rmse:2.743346 \n#> [58] train-rmse:0.930738 test-rmse:2.744985 \n#> [59] train-rmse:0.914788 test-rmse:2.746578 \n#> [60] train-rmse:0.897444 test-rmse:2.742718 \n#> [61] train-rmse:0.881162 test-rmse:2.746288 \n#> [62] train-rmse:0.873362 test-rmse:2.739386 \n#> [63] train-rmse:0.855085 test-rmse:2.746520 \n#> [64] train-rmse:0.835583 test-rmse:2.751073 \n#> [65] train-rmse:0.825417 test-rmse:2.744734 \n#> [66] train-rmse:0.805799 test-rmse:2.757439 \n#> [67] train-rmse:0.793748 test-rmse:2.762758 \n#> [68] train-rmse:0.780451 test-rmse:2.767125 \n#> [69] train-rmse:0.774147 test-rmse:2.769935 \n#> [70] train-rmse:0.762300 test-rmse:2.769790 \n#> [1]  train-rmse:17.292655    test-rmse:16.876471 \n#> [2]  train-rmse:12.577736    test-rmse:12.195365 \n#> [3]  train-rmse:9.241283 test-rmse:8.922933 \n#> [4]  train-rmse:6.913518 test-rmse:6.724334 \n#> [5]  train-rmse:5.326545 test-rmse:5.265766 \n#> [6]  train-rmse:4.262574 test-rmse:4.416342 \n#> [7]  train-rmse:3.480536 test-rmse:3.935688 \n#> [8]  train-rmse:2.985960 test-rmse:3.632995 \n#> [9]  train-rmse:2.660206 test-rmse:3.538935 \n#> [10] train-rmse:2.435057 test-rmse:3.492154 \n#> [11] train-rmse:2.264139 test-rmse:3.480262 \n#> [12] train-rmse:2.115240 test-rmse:3.447267 \n#> [13] train-rmse:1.997048 test-rmse:3.432714 \n#> [14] train-rmse:1.913323 test-rmse:3.423979 \n#> [15] train-rmse:1.836868 test-rmse:3.391161 \n#> [16] train-rmse:1.781762 test-rmse:3.377973 \n#> [17] train-rmse:1.708092 test-rmse:3.359732 \n#> [18] train-rmse:1.681469 test-rmse:3.367809 \n#> [19] train-rmse:1.625341 test-rmse:3.353789 \n#> [20] train-rmse:1.564421 test-rmse:3.327785 \n#> [21] train-rmse:1.531215 test-rmse:3.320419 \n#> [22] train-rmse:1.488833 test-rmse:3.341321 \n#> [23] train-rmse:1.456582 test-rmse:3.319002 \n#> [24] train-rmse:1.425750 test-rmse:3.304326 \n#> [25] train-rmse:1.379122 test-rmse:3.285657 \n#> [26] train-rmse:1.335484 test-rmse:3.281933 \n#> [27] train-rmse:1.307765 test-rmse:3.261889 \n#> [28] train-rmse:1.264471 test-rmse:3.255168 \n#> [29] train-rmse:1.254344 test-rmse:3.249777 \n#> [30] train-rmse:1.217107 test-rmse:3.232775 \n#> [31] train-rmse:1.201509 test-rmse:3.229382 \n#> [32] train-rmse:1.184240 test-rmse:3.222288 \n#> [33] train-rmse:1.156229 test-rmse:3.213939 \n#> [34] train-rmse:1.136092 test-rmse:3.216067 \n#> [35] train-rmse:1.110480 test-rmse:3.216788 \n#> [36] train-rmse:1.094752 test-rmse:3.206662 \n#> [37] train-rmse:1.075361 test-rmse:3.211364 \n#> [38] train-rmse:1.061166 test-rmse:3.207463 \n#> [39] train-rmse:1.044627 test-rmse:3.212024 \n#> [40] train-rmse:1.017754 test-rmse:3.214319 \n#> [41] train-rmse:0.995682 test-rmse:3.211204 \n#> [42] train-rmse:0.975897 test-rmse:3.204311 \n#> [43] train-rmse:0.969302 test-rmse:3.209325 \n#> [44] train-rmse:0.962742 test-rmse:3.213350 \n#> [45] train-rmse:0.950907 test-rmse:3.205705 \n#> [46] train-rmse:0.926037 test-rmse:3.203800 \n#> [47] train-rmse:0.913792 test-rmse:3.188461 \n#> [48] train-rmse:0.895957 test-rmse:3.184823 \n#> [49] train-rmse:0.883827 test-rmse:3.181743 \n#> [50] train-rmse:0.861841 test-rmse:3.181888 \n#> [51] train-rmse:0.850236 test-rmse:3.177201 \n#> [52] train-rmse:0.842394 test-rmse:3.176773 \n#> [53] train-rmse:0.827710 test-rmse:3.171668 \n#> [54] train-rmse:0.817296 test-rmse:3.175845 \n#> [55] train-rmse:0.812589 test-rmse:3.174930 \n#> [56] train-rmse:0.807248 test-rmse:3.178411 \n#> [57] train-rmse:0.799478 test-rmse:3.172622 \n#> [58] train-rmse:0.785276 test-rmse:3.171110 \n#> [59] train-rmse:0.780453 test-rmse:3.172351 \n#> [60] train-rmse:0.767954 test-rmse:3.173666 \n#> [61] train-rmse:0.764007 test-rmse:3.172314 \n#> [62] train-rmse:0.758229 test-rmse:3.173465 \n#> [63] train-rmse:0.747242 test-rmse:3.172619 \n#> [64] train-rmse:0.735999 test-rmse:3.170532 \n#> [65] train-rmse:0.725830 test-rmse:3.174557 \n#> [66] train-rmse:0.708749 test-rmse:3.173571 \n#> [67] train-rmse:0.689192 test-rmse:3.178361 \n#> [68] train-rmse:0.675599 test-rmse:3.176243 \n#> [69] train-rmse:0.657948 test-rmse:3.178651 \n#> [70] train-rmse:0.649430 test-rmse:3.176308 \n#> [1]  train-rmse:17.544533    test-rmse:16.355680 \n#> [2]  train-rmse:12.719916    test-rmse:11.847680 \n#> [3]  train-rmse:9.325918 test-rmse:8.587711 \n#> [4]  train-rmse:6.978470 test-rmse:6.520126 \n#> [5]  train-rmse:5.330168 test-rmse:5.197999 \n#> [6]  train-rmse:4.227713 test-rmse:4.370258 \n#> [7]  train-rmse:3.452389 test-rmse:3.899105 \n#> [8]  train-rmse:2.953947 test-rmse:3.646518 \n#> [9]  train-rmse:2.621221 test-rmse:3.488626 \n#> [10] train-rmse:2.390508 test-rmse:3.434280 \n#> [11] train-rmse:2.248430 test-rmse:3.393405 \n#> [12] train-rmse:2.148335 test-rmse:3.397108 \n#> [13] train-rmse:2.062937 test-rmse:3.364962 \n#> [14] train-rmse:1.965071 test-rmse:3.319566 \n#> [15] train-rmse:1.898697 test-rmse:3.274744 \n#> [16] train-rmse:1.864320 test-rmse:3.256337 \n#> [17] train-rmse:1.837230 test-rmse:3.262007 \n#> [18] train-rmse:1.803225 test-rmse:3.235490 \n#> [19] train-rmse:1.749646 test-rmse:3.211386 \n#> [20] train-rmse:1.711134 test-rmse:3.192852 \n#> [21] train-rmse:1.666622 test-rmse:3.184116 \n#> [22] train-rmse:1.628228 test-rmse:3.183039 \n#> [23] train-rmse:1.593111 test-rmse:3.160884 \n#> [24] train-rmse:1.569067 test-rmse:3.152188 \n#> [25] train-rmse:1.538398 test-rmse:3.129954 \n#> [26] train-rmse:1.487646 test-rmse:3.142378 \n#> [27] train-rmse:1.463082 test-rmse:3.140700 \n#> [28] train-rmse:1.420317 test-rmse:3.114777 \n#> [29] train-rmse:1.403061 test-rmse:3.115688 \n#> [30] train-rmse:1.358206 test-rmse:3.101864 \n#> [31] train-rmse:1.340879 test-rmse:3.100730 \n#> [32] train-rmse:1.311146 test-rmse:3.082216 \n#> [33] train-rmse:1.290238 test-rmse:3.077993 \n#> [34] train-rmse:1.280471 test-rmse:3.072337 \n#> [35] train-rmse:1.237008 test-rmse:3.043744 \n#> [36] train-rmse:1.223663 test-rmse:3.029086 \n#> [37] train-rmse:1.214975 test-rmse:3.022611 \n#> [38] train-rmse:1.185739 test-rmse:3.017997 \n#> [39] train-rmse:1.164385 test-rmse:3.013672 \n#> [40] train-rmse:1.154593 test-rmse:3.013402 \n#> [41] train-rmse:1.139351 test-rmse:3.012511 \n#> [42] train-rmse:1.121388 test-rmse:3.008539 \n#> [43] train-rmse:1.104007 test-rmse:2.995401 \n#> [44] train-rmse:1.097245 test-rmse:2.987109 \n#> [45] train-rmse:1.077283 test-rmse:2.990181 \n#> [46] train-rmse:1.050955 test-rmse:2.983796 \n#> [47] train-rmse:1.036467 test-rmse:2.979257 \n#> [48] train-rmse:1.028401 test-rmse:2.984757 \n#> [49] train-rmse:0.994774 test-rmse:2.976397 \n#> [50] train-rmse:0.985299 test-rmse:2.964851 \n#> [51] train-rmse:0.977850 test-rmse:2.957399 \n#> [52] train-rmse:0.956651 test-rmse:2.944479 \n#> [53] train-rmse:0.949326 test-rmse:2.947867 \n#> [54] train-rmse:0.927821 test-rmse:2.952446 \n#> [55] train-rmse:0.908993 test-rmse:2.944909 \n#> [56] train-rmse:0.884882 test-rmse:2.935588 \n#> [57] train-rmse:0.873803 test-rmse:2.938716 \n#> [58] train-rmse:0.845808 test-rmse:2.936628 \n#> [59] train-rmse:0.840582 test-rmse:2.937588 \n#> [60] train-rmse:0.831460 test-rmse:2.938824 \n#> [61] train-rmse:0.813577 test-rmse:2.944781 \n#> [62] train-rmse:0.800639 test-rmse:2.936827 \n#> [63] train-rmse:0.790201 test-rmse:2.933134 \n#> [64] train-rmse:0.776988 test-rmse:2.928452 \n#> [65] train-rmse:0.757110 test-rmse:2.933903 \n#> [66] train-rmse:0.743001 test-rmse:2.931065 \n#> [67] train-rmse:0.735839 test-rmse:2.929767 \n#> [68] train-rmse:0.723563 test-rmse:2.927346 \n#> [69] train-rmse:0.713754 test-rmse:2.925463 \n#> [70] train-rmse:0.698813 test-rmse:2.927692 \n#> [1]  train-rmse:16.674860    test-rmse:18.015271 \n#> [2]  train-rmse:12.063769    test-rmse:13.222241 \n#> [3]  train-rmse:8.811503 test-rmse:9.994677 \n#> [4]  train-rmse:6.582876 test-rmse:7.760594 \n#> [5]  train-rmse:5.018353 test-rmse:6.298805 \n#> [6]  train-rmse:3.953114 test-rmse:5.304116 \n#> [7]  train-rmse:3.254362 test-rmse:4.729481 \n#> [8]  train-rmse:2.794439 test-rmse:4.387156 \n#> [9]  train-rmse:2.501569 test-rmse:4.179851 \n#> [10] train-rmse:2.274560 test-rmse:4.039873 \n#> [11] train-rmse:2.137563 test-rmse:3.954427 \n#> [12] train-rmse:2.045113 test-rmse:3.914518 \n#> [13] train-rmse:1.954443 test-rmse:3.868235 \n#> [14] train-rmse:1.882999 test-rmse:3.860332 \n#> [15] train-rmse:1.804658 test-rmse:3.836931 \n#> [16] train-rmse:1.754922 test-rmse:3.828111 \n#> [17] train-rmse:1.716032 test-rmse:3.805052 \n#> [18] train-rmse:1.650214 test-rmse:3.778186 \n#> [19] train-rmse:1.600299 test-rmse:3.757008 \n#> [20] train-rmse:1.553732 test-rmse:3.739166 \n#> [21] train-rmse:1.525169 test-rmse:3.731639 \n#> [22] train-rmse:1.489868 test-rmse:3.712193 \n#> [23] train-rmse:1.442279 test-rmse:3.697139 \n#> [24] train-rmse:1.414500 test-rmse:3.692106 \n#> [25] train-rmse:1.365578 test-rmse:3.680468 \n#> [26] train-rmse:1.347896 test-rmse:3.673943 \n#> [27] train-rmse:1.332444 test-rmse:3.672703 \n#> [28] train-rmse:1.305014 test-rmse:3.661026 \n#> [29] train-rmse:1.277232 test-rmse:3.662294 \n#> [30] train-rmse:1.248798 test-rmse:3.650389 \n#> [31] train-rmse:1.241240 test-rmse:3.651887 \n#> [32] train-rmse:1.217933 test-rmse:3.639778 \n#> [33] train-rmse:1.206731 test-rmse:3.638429 \n#> [34] train-rmse:1.187312 test-rmse:3.635264 \n#> [35] train-rmse:1.164374 test-rmse:3.632264 \n#> [36] train-rmse:1.131843 test-rmse:3.640556 \n#> [37] train-rmse:1.124082 test-rmse:3.633127 \n#> [38] train-rmse:1.110077 test-rmse:3.630717 \n#> [39] train-rmse:1.100095 test-rmse:3.631217 \n#> [40] train-rmse:1.081659 test-rmse:3.635696 \n#> [41] train-rmse:1.072853 test-rmse:3.632723 \n#> [42] train-rmse:1.055953 test-rmse:3.633422 \n#> [43] train-rmse:1.031922 test-rmse:3.637305 \n#> [44] train-rmse:1.023225 test-rmse:3.625068 \n#> [45] train-rmse:1.006294 test-rmse:3.620948 \n#> [46] train-rmse:0.994529 test-rmse:3.617117 \n#> [47] train-rmse:0.979627 test-rmse:3.611547 \n#> [48] train-rmse:0.971881 test-rmse:3.608559 \n#> [49] train-rmse:0.965366 test-rmse:3.604040 \n#> [50] train-rmse:0.954073 test-rmse:3.605317 \n#> [51] train-rmse:0.931940 test-rmse:3.602057 \n#> [52] train-rmse:0.920522 test-rmse:3.596360 \n#> [53] train-rmse:0.916464 test-rmse:3.593168 \n#> [54] train-rmse:0.898960 test-rmse:3.587739 \n#> [55] train-rmse:0.873178 test-rmse:3.578689 \n#> [56] train-rmse:0.863999 test-rmse:3.576982 \n#> [57] train-rmse:0.844966 test-rmse:3.583278 \n#> [58] train-rmse:0.833886 test-rmse:3.580366 \n#> [59] train-rmse:0.818516 test-rmse:3.577260 \n#> [60] train-rmse:0.805208 test-rmse:3.577898 \n#> [61] train-rmse:0.797068 test-rmse:3.577023 \n#> [62] train-rmse:0.785239 test-rmse:3.568169 \n#> [63] train-rmse:0.776081 test-rmse:3.572902 \n#> [64] train-rmse:0.756783 test-rmse:3.572786 \n#> [65] train-rmse:0.737046 test-rmse:3.571570 \n#> [66] train-rmse:0.729238 test-rmse:3.572501 \n#> [67] train-rmse:0.723132 test-rmse:3.578412 \n#> [68] train-rmse:0.710213 test-rmse:3.568142 \n#> [69] train-rmse:0.703534 test-rmse:3.568780 \n#> [70] train-rmse:0.688026 test-rmse:3.571313\n#> [1] 3.486078\nwarnings() # no warnings for individual XGBoost function"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"building-weighted-ensembles-to-model-numerical-data","chapter":"4 Building weighted ensembles to model numerical data","heading":"4 Building weighted ensembles to model numerical data","text":"last chapter learned make 23 individual models,\nincluding calculating error rate (root mean squared error), \npredictions holdout data (test validation).chapter show use results make weighted\nensembles can used model numerical data.Let’s start end. Let’s imagine finished product. list \nensembles individual models, error rates sorted decreasing\norder (best result top list)Therefore ’re going need weighted ensemble. weight \nuse?turns excellent answer available virtually \nwork part (great!). model mean error score.\nweight use reciprocal error.Let’s say two models. One error rate 5.0 \nerror rate 2.0. Clearly model error rate 2.0\nsuperior model error rate 5.0.building ensemble multiply values \nensemble 1/(error rate). give higher weights models \nhigher accuracy.Let’s see works extremely simple ensemble.section making weighted ensemble\nusing 17 models numerical data, using ensemble measure\naccuracy models holdout (test) data.","code":"\nlibrary(tree) # Allows us to use tree models\nlibrary(MASS) # For the Boston Housing data set library(Metrics)\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ✖ dplyr::select() masks MASS::select()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Set initial values to 0\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_RMSE <- 0\nlinear_test_predict_value <- 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_RMSE <- 0\ntree_holdout_RMSE <- 0\ntree_test_predict_value <- 0\n\nensemble_linear_RMSE <- 0\nensemble_linear_RMSE_mean <- 0\nensemble_tree_RMSE <- 0\nensemble_tree_RMSE_mean <- 0\n\nnumerical_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Move target column to far right\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Set up resampling\nfor (i in 1:numresamples) {\n  idx <- sample(seq(1, 2), size = nrow(data), replace = TRUE, prob = c(train_amount, test_amount))\n  train <- data[idx == 1, ]\n  test <- data[idx == 2, ]\n\n# Fit linear model on the training data, make predictions on the test data\nlinear_model <- lm(y ~ ., data = train)\nlinear_predictions <- predict(object = linear_model, newdata = test)\nlinear_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = linear_predictions)\nlinear_RMSE_mean <- mean(linear_RMSE)\n\n# Fit tree model on the training data, make predictions on the test data\ntree_model <- tree(y ~ ., data = train)\ntree_predictions <- predict(object = tree_model, newdata = test)\ntree_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = tree_predictions)\ntree_RMSE_mean <- mean(tree_RMSE)\n\n# Make the weighted ensemble\nensemble <- data.frame(\n  'linear' = linear_predictions / linear_RMSE_mean,\n  'tree' = tree_predictions / tree_RMSE_mean,\n  'y_ensemble' = test$y)\n\n# Split ensemble between train and test\nensemble_idx <- sample(seq(1, 2), size = nrow(ensemble), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble[ensemble_idx == 1, ]\nensemble_test <- ensemble[ensemble_idx == 2, ]\n\n# Fit the ensemble data on the ensemble training data, predict on ensemble test data\nensemble_linear_model <- lm(y_ensemble ~ ., data = ensemble_train)\n\nensemble_linear_predictions <- predict(object = ensemble_linear_model, newdata = ensemble_test)\n\nensemble_linear_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_linear_predictions)\n\nensemble_linear_RMSE_mean <- mean(ensemble_linear_RMSE)\n\n# Fit the tree model on the ensemble training data, predict on ensemble test data\nensemble_tree_model <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree_model, newdata = ensemble_test) \n\nensemble_tree_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_tree_predictions)\n\nensemble_tree_RMSE_mean <- mean(ensemble_tree_RMSE)\n\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble_Linear', 'Ensemble_tree'),\n  'Error_Rate' = c(linear_RMSE_mean, tree_RMSE_mean, ensemble_linear_RMSE_mean, ensemble_tree_RMSE_mean)\n)\n\nresults <- results %>% arrange(Error_Rate)\n\n} # Closing brace for numresamples\nreturn(list(results))\n\n} # Closing brace for the function\n\nnumerical_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [[1]]\n#>             Model Error_Rate\n#> 1 Ensemble_Linear   4.192134\n#> 2   Ensemble_tree   4.638131\n#> 3          Linear   4.924980\n#> 4            Tree   4.972398\n\nwarnings()"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"think-before-you-do-something.-this-will-help-when-we-start-at-the-end-and-work-backwards-toward-the-beginning.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.1 Think before you do something. This will help when we start at the end and work backwards toward the beginning.","text":"going make ensemble. ensemble going made \npredictions numerical models. ’ve already seen couple \nensembles. one extremely similar, involve \nmodels.Starting end, want error rate (root mean squared error)\nensemble prediction models.means ’ll need make ensemble. means ’ll need\nindividual model predictions, ’s ensemble made. \nensemble made individual model predictions, means ’ll\nneed individual models. already know , \nlast chapter.’re going build simple ensemble seven models, use\nensemble four different methods. Ensembles package\nactually works total 40 different models. process \nexactly , whether working seven individual models \n23 individual models, five ensemble models 17 ensemble models. \nstructre methods .let’s get started!seven individual models building :Linear (tuned)Linear (tuned)BayesglmBayesglmBayesrnnBayesrnnGradient BoostedGradient BoostedRandomForestRandomForestTreesTreesIt’s important understand many options possible. \nencouraged add least one modeling method ensemble,\nsee impacts results.","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"one-of-your-own-add-one-model-to-the-list-of-seven-individual-models-see-how-it-impacts-results.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.2 One of your own: Add one model to the list of seven individual models, see how it impacts results.","text":"","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"plan-ahead-as-much-as-you-can-that-makes-the-entire-model-building-process-much-easier.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.3 Plan ahead as much as you can, that makes the entire model building process much easier.","text":"code build ensembles. strongly recommend \n, checking every 5-10 lines make sure \nerrors.","code":"\n\n#Load packages we will need\n\nlibrary(arm) # Allows us to run bayesglm\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russellconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\nlibrary(brnn) # Allows us to run brnn\n#> Loading required package: Formula\n#> Loading required package: truncnorm\nlibrary(e1071) # Allows us to run several tuned model, such as linear and KNN\nlibrary(randomForest) # Allows us to run random forest models\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\nlibrary(tree) # Allows us to run tree models"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"a-few-other-packages-we-will-need-to-keep-everything-running-smoothly","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.3.1 A few other packages we will need to keep everything running smoothly","text":"","code":"\nlibrary(tidyverse) # Amazing set of tools for data science\nlibrary(MASS) # Gives us the Boston Housing data set\nlibrary(Metrics) # Allows us to calculate accuracy or error rates"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"build-the-function-that-will-build-the-individual-and-ensemble-models","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.3.2 Build the function that will build the individual and ensemble models","text":"’s cool part setting way. \ntotally different data set, need put information\nfunction, everything runs. Check :","code":"\n\nnumerical <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Make the target column the right most column, change the column name to y:\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ]\n\n# Set initial values to 0 for both individual and ensemble methods:\n\nbayesglm_train_RMSE <- 0\nbayesglm_test_RMSE <- 0\nbayesglm_validation_RMSE <- 0\nbayesglm_sd <- 0\nbayesglm_overfitting <- 0\nbayesglm_duration <- 0\nbayesglm_duration_mean <- 0\nbayesglm_holdout_mean <- 0\nbayesglm_holdout_RMSE <- 0\nbayesglm_holdout_RMSE_mean <- 0\n\nbayesrnn_train_RMSE <- 0\nbayesrnn_test_RMSE <- 0\nbayesrnn_validation_RMSE <- 0\nbayesrnn_sd <- 0\nbayesrnn_overfitting <- 0\nbayesrnn_duration <- 0\nbayesrnn_duration_mean <- 0\nbayesrnn_holdout_mean <- 0\nbayesrnn_holdout_RMSE <- 0\nbayesrnn_holdout_RMSE_mean <- 0\n\ngb_train_RMSE <- 0\ngb_test_RMSE <- 0\ngb_validation_RMSE <- 0\ngb_sd <- 0\ngb_overfitting <- 0\ngb_duration <- 0\ngb_duration_mean <- 0\ngb_holdout_mean <- 0\ngb_holdout_RMSE <- 0\ngb_holdout_RMSE_mean <- 0\n\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_validation_RMSE <- 0\nlinear_sd <- 0\nlinear_overfitting <- 0\nlinear_duration <- 0\nlinear_holdout_RMSE <- 0\nlinear_holdout_RMSE_mean <- 0\n\nrf_train_RMSE <- 0\nrf_test_RMSE <- 0\nrf_validation_RMSE <- 0\nrf_sd <- 0\nrf_overfitting <- 0\nrf_duration <- 0\nrf_duration_mean <- 0\nrf_holdout_mean <- 0\nrf_holdout_RMSE <- 0\nrf_holdout_RMSE_mean <- 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_validation_RMSE <- 0\ntree_sd <- 0\ntree_overfitting <- 0\ntree_duration <- 0\ntree_duration_mean <- 0\ntree_holdout_mean <- 0\ntree_holdout_RMSE <- 0\ntree_holdout_RMSE_mean <- 0\n\nensemble_bayesglm_train_RMSE <- 0\nensemble_bayesglm_test_RMSE <- 0\nensemble_bayesglm_validation_RMSE <- 0\nensemble_bayesglm_sd <- 0\nensemble_bayesglm_overfitting <- 0\nensemble_bayesglm_duration <- 0\nensemble_bayesglm_holdout_RMSE <- 0\nensemble_bayesglm_holdout_RMSE_mean <- 0\nensemble_bayesglm_predict_value_mean <- 0\n\nensemble_bayesrnn_train_RMSE <- 0\nensemble_bayesrnn_test_RMSE <- 0\nensemble_bayesrnn_validation_RMSE <- 0\nensemble_bayesrnn_sd <- 0\nensemble_bayesrnn_overfitting <- 0\nensemble_bayesrnn_duration <- 0\nensemble_bayesrnn_holdout_RMSE <- 0\nensemble_bayesrnn_holdout_RMSE_mean <- 0\nensemble_bayesrnn_predict_value_mean <- 0\n\nensemble_gb_train_RMSE <- 0\nensemble_gb_test_RMSE <- 0\nensemble_gb_validation_RMSE <- 0\nensemble_gb_sd <- 0\nensemble_gb_overfitting <- 0\nensemble_gb_duration <- 0\nensemble_gb_holdout_RMSE <- 0\nensemble_gb_holdout_RMSE_mean <- 0\nensemble_gb_predict_value_mean <- 0\n\nensemble_linear_train_RMSE <- 0\nensemble_linear_test_RMSE <- 0\nensemble_linear_validation_RMSE <- 0\nensemble_linear_sd <- 0\nensemble_linear_overfitting <- 0\nensemble_linear_duration <- 0\nensemble_linear_holdout_RMSE <- 0\nensemble_linear_holdout_RMSE_mean <- 0\n\nensemble_rf_train_RMSE <- 0\nensemble_rf_test_RMSE <- 0\nensemble_rf_test_RMSE_mean <- 0\nensemble_rf_validation_RMSE <- 0\nensemble_rf_sd <- 0\nensemble_rf_overfitting <- 0\nensemble_rf_duration <- 0\nensemble_rf_holdout_RMSE <- 0\nensemble_rf_holdout_RMSE_mean <- 0\n\nensemble_tree_train_RMSE <- 0\nensemble_tree_test_RMSE <- 0\nensemble_tree_validation_RMSE <- 0\nensemble_tree_sd <- 0\nensemble_tree_overfitting <- 0\nensemble_tree_duration <- 0\nensemble_tree_holdout_RMSE <- 0\nensemble_tree_holdout_RMSE_mean <- 0\n\n#Let's build the function that does all the resampling and puts everything together:\n\nfor (i in 1:numresamples) {\n\n# Randomly split the data between train and test\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Bayesglm\n\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = gaussian(link = \"identity\"))\nbayesglm_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesglm_train_fit, newdata = train))\nbayesglm_train_RMSE_mean <- mean(bayesglm_train_RMSE)\nbayesglm_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesglm_train_fit, newdata = test))\nbayesglm_test_RMSE_mean <- mean(bayesglm_test_RMSE)\nbayesglm_holdout_RMSE[i] <- mean(bayesglm_test_RMSE_mean)\nbayesglm_holdout_RMSE_mean <- mean(bayesglm_holdout_RMSE)\nbayesglm_test_predict_value <- as.numeric(predict(object = bayesglm_train_fit, newdata = test))\ny_hat_bayesglm <- c(bayesglm_test_predict_value)\n\n# Bayesrnn\n\nbayesrnn_train_fit <- brnn::brnn(x = as.matrix(train), y = train$y)\nbayesrnn_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesrnn_train_fit, newdata = train))\nbayesrnn_train_RMSE_mean <- mean(bayesrnn_train_RMSE)\nbayesrnn_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesrnn_train_fit, newdata = test))\nbayesrnn_test_RMSE_mean <- mean(bayesrnn_test_RMSE)\nbayesrnn_holdout_RMSE[i] <- mean(c(bayesrnn_test_RMSE_mean))\nbayesrnn_holdout_RMSE_mean <- mean(bayesrnn_holdout_RMSE)\nbayesrnn_train_predict_value <- as.numeric(predict(object = bayesrnn_train_fit, newdata = train))\nbayesrnn_test_predict_value <- as.numeric(predict(object = bayesrnn_train_fit, newdata = test))\nbayesrnn_predict_value_mean <- mean(c(bayesrnn_test_predict_value))\ny_hat_bayesrnn <- c(bayesrnn_test_predict_value)\n\n# Gradient boosted\n\ngb_train_fit <- gbm::gbm(train$y ~ ., data = train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\ngb_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = gb_train_fit, newdata = train))\ngb_train_RMSE_mean <- mean(gb_train_RMSE)\ngb_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = gb_train_fit, newdata = test))\ngb_test_RMSE_mean <- mean(gb_test_RMSE)\ngb_holdout_RMSE[i] <- mean(c(gb_test_RMSE_mean))\ngb_holdout_RMSE_mean <- mean(gb_holdout_RMSE)\ngb_train_predict_value <- as.numeric(predict(object = gb_train_fit, newdata = train))\ngb_test_predict_value <- as.numeric(predict(object = gb_train_fit, newdata = test)) \ngb_predict_value_mean <- mean(c(gb_test_predict_value))\ny_hat_gb <- c(gb_test_predict_value)\n\n# Tuned linear models\n\nlinear_train_fit <- e1071::tune.rpart(formula = y ~ ., data = train)\nlinear_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = linear_train_fit$best.model, newdata = train))\nlinear_train_RMSE_mean <- mean(linear_train_RMSE)\nlinear_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = linear_train_fit$best.model, newdata = test))\nlinear_test_RMSE_mean <- mean(linear_test_RMSE)\nlinear_holdout_RMSE[i] <- mean(c(linear_test_RMSE_mean))\nlinear_holdout_RMSE_mean <- mean(linear_holdout_RMSE)\nlinear_train_predict_value <- as.numeric(predict(object = linear_train_fit$best.model, newdata = train))\nlinear_test_predict_value <- as.numeric(predict(object = linear_train_fit$best.model, newdata = test))\ny_hat_linear <- c(linear_test_predict_value)\n\n# RandomForest\n\nrf_train_fit <- e1071::tune.randomForest(x = train, y = train$y, data = train)\nrf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = rf_train_fit$best.model, newdata = train))\nrf_train_RMSE_mean <- mean(rf_train_RMSE)\nrf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = rf_train_fit$best.model, newdata = test))\nrf_test_RMSE_mean <- mean(rf_test_RMSE)\nrf_holdout_RMSE[i] <- mean(c(rf_test_RMSE_mean))\nrf_holdout_RMSE_mean <- mean(rf_holdout_RMSE)\nrf_train_predict_value <- predict(object = rf_train_fit$best.model, newdata = train)\nrf_test_predict_value <- predict(object = rf_train_fit$best.model, newdata = test)\ny_hat_rf <- c(rf_test_predict_value)\n\n# Trees\n\ntree_train_fit <- tree::tree(train$y ~ ., data = train)\ntree_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = tree_train_fit, newdata = train))\ntree_train_RMSE_mean <- mean(tree_train_RMSE)\ntree_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = tree_train_fit, newdata = test))\ntree_test_RMSE_mean <- mean(tree_test_RMSE)\ntree_holdout_RMSE[i] <- mean(c(tree_test_RMSE_mean))\ntree_holdout_RMSE_mean <- mean(tree_holdout_RMSE)\ntree_train_predict_value <- as.numeric(predict(object = tree::tree(y ~ ., data = train), newdata = train))\ntree_test_predict_value <- as.numeric(predict(object = tree::tree(y ~ ., data = train), newdata = test))\ny_hat_tree <- c(tree_test_predict_value)\n\n# Make the weighted ensemble:\n\nensemble <- data.frame(\n  \"BayesGLM\" = y_hat_bayesglm * 1 / bayesglm_holdout_RMSE_mean,\n  \"BayesRNN\" = y_hat_bayesrnn * 1 / bayesrnn_holdout_RMSE_mean,\n  \"GBM\" = y_hat_gb * 1 / gb_holdout_RMSE_mean,\n  \"Linear\" = y_hat_linear * 1 / linear_holdout_RMSE_mean,\n  \"RandomForest\" = y_hat_rf * 1 / rf_holdout_RMSE_mean,\n  \"Tree\" = y_hat_tree * 1 / tree_holdout_RMSE_mean\n  )\n\nensemble$Row_mean <- rowMeans(ensemble)\n  ensemble$y_ensemble <- c(test$y)\n  y_ensemble <- c(test$y)\n\n# Split the ensemble into train and test, according to user choices:\n\nensemble_idx <- sample(seq(1, 2), size = nrow(ensemble), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble[ensemble_idx == 1, ]\nensemble_test <- ensemble[ensemble_idx == 2, ]\n\n# Ensemble BayesGLM\n\nensemble_bayesglm_train_fit <- arm::bayesglm(y_ensemble ~ ., data = ensemble_train, family = gaussian(link = \"identity\"))\nensemble_bayesglm_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_bayesglm_train_fit, newdata = ensemble_train))\nensemble_bayesglm_train_RMSE_mean <- mean(ensemble_bayesglm_train_RMSE)\nensemble_bayesglm_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_bayesglm_train_fit, newdata = ensemble_test))\nensemble_bayesglm_test_RMSE_mean <- mean(ensemble_bayesglm_test_RMSE)\nensemble_bayesglm_holdout_RMSE[i] <- mean(c(ensemble_bayesglm_test_RMSE_mean))\nensemble_bayesglm_holdout_RMSE_mean <- mean(ensemble_bayesglm_holdout_RMSE)\n\n# Ensemble BayesRNN\n\nensemble_bayesrnn_train_fit <- brnn::brnn(x = as.matrix(ensemble_train), y = ensemble_train$y_ensemble)\nensemble_bayesrnn_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_bayesrnn_train_fit, newdata = ensemble_train))\nensemble_bayesrnn_train_RMSE_mean <- mean(ensemble_bayesrnn_train_RMSE)\nensemble_bayesrnn_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_bayesrnn_train_fit, newdata = ensemble_test))\nensemble_bayesrnn_test_RMSE_mean <- mean(ensemble_bayesrnn_test_RMSE)\nensemble_bayesrnn_holdout_RMSE[i] <- mean(c(ensemble_bayesrnn_test_RMSE_mean))\nensemble_bayesrnn_holdout_RMSE_mean <- mean(ensemble_bayesrnn_holdout_RMSE)\n\n# Ensemble Graident Boosted\n\nensemble_gb_train_fit <- gbm::gbm(ensemble_train$y_ensemble ~ ., data = ensemble_train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\nensemble_gb_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_gb_train_fit, newdata = ensemble_train))\nensemble_gb_train_RMSE_mean <- mean(ensemble_gb_train_RMSE)\nensemble_gb_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_gb_train_fit, newdata = ensemble_test))\nensemble_gb_test_RMSE_mean <- mean(ensemble_gb_test_RMSE)\nensemble_gb_holdout_RMSE[i] <- mean(c(ensemble_gb_test_RMSE_mean))\nensemble_gb_holdout_RMSE_mean <- mean(ensemble_gb_holdout_RMSE)\n\n# Ensemble using Tuned Random Forest\n\nensemble_rf_train_fit <- e1071::tune.randomForest(x = ensemble_train, y = ensemble_train$y_ensemble, data = ensemble_train)\nensemble_rf_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_rf_train_fit$best.model, newdata = ensemble_train))\nensemble_rf_train_RMSE_mean <- mean(ensemble_rf_train_RMSE)\nensemble_rf_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_rf_train_fit$best.model, newdata = ensemble_test))\nensemble_rf_test_RMSE_mean <- mean(ensemble_rf_test_RMSE)\nensemble_rf_holdout_RMSE[i] <- mean(c(ensemble_rf_test_RMSE_mean))\nensemble_rf_holdout_RMSE_mean <- mean(ensemble_rf_holdout_RMSE)\n\n# Trees\n\nensemble_tree_train_fit <- tree::tree(ensemble_train$y_ensemble ~ ., data = ensemble_train)\nensemble_tree_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_tree_train_fit, newdata = ensemble_train))\nensemble_tree_train_RMSE_mean <- mean(ensemble_tree_train_RMSE)\nensemble_tree_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_tree_train_fit, newdata = ensemble_test))\nensemble_tree_test_RMSE_mean <- mean(ensemble_tree_test_RMSE)\nensemble_tree_holdout_RMSE[i] <- mean(c(ensemble_tree_test_RMSE_mean))\nensemble_tree_holdout_RMSE_mean <- mean(ensemble_tree_holdout_RMSE)\n\nsummary_results <- data.frame(\n  'Model' = c('BayesGLM', 'BayesRNN', 'Gradient_Boosted', 'Linear', 'Random_Forest', 'Trees', 'Ensemble_BayesGLM', 'Ensemble_BayesRNN', 'Ensemble_Gradient_Boosted', 'Ensemble_Random_Forest', 'Ensemble_Trees'), \n  'Error' = c(bayesglm_holdout_RMSE_mean, bayesrnn_holdout_RMSE_mean, gb_holdout_RMSE_mean, linear_holdout_RMSE_mean, rf_holdout_RMSE_mean, tree_holdout_RMSE_mean, ensemble_bayesglm_holdout_RMSE_mean, ensemble_bayesrnn_holdout_RMSE_mean, ensemble_gb_holdout_RMSE_mean, ensemble_rf_holdout_RMSE_mean, ensemble_tree_holdout_RMSE_mean) )\n\nsummary_results <- summary_results %>% arrange(Error)\n\n} # closing brace for numresamples\nreturn(summary_results)\n\n} # closing brace for numerical function\n\nnumerical(data = MASS::Boston, colnum = 14, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015132 \n#> gamma= 30.6984    alpha= 5.0235   beta= 17385.46\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7039239 \n#> gamma= 14.9316    alpha= 2.2491   beta= 6725.389\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015669 \n#> gamma= 30.9741    alpha= 5.0722   beta= 14041.05\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7040551 \n#> gamma= 12.0505    alpha= 2.0858   beta= 6248.904\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015669 \n#> gamma= 30.887     alpha= 4.8933   beta= 21306.08\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7042691 \n#> gamma= 13.69      alpha= 1.9643   beta= 7017.744\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016138 \n#> gamma= 31.5899    alpha= 4.9545   beta= 14862.09\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7042319 \n#> gamma= 15.1336    alpha= 2.0335   beta= 8709.426\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015371 \n#> gamma= 28.7443    alpha= 2.6834   beta= 18766.47\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7044249 \n#> gamma= 13.7672    alpha= 2.3035   beta= 5504.11\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#>                        Model     Error\n#> 1          Ensemble_BayesGLM 0.1290663\n#> 2                   BayesRNN 0.1395422\n#> 3          Ensemble_BayesRNN 0.2036452\n#> 4     Ensemble_Random_Forest 0.9760099\n#> 5  Ensemble_Gradient_Boosted 1.9456769\n#> 6              Random_Forest 2.2685383\n#> 7             Ensemble_Trees 2.7199243\n#> 8           Gradient_Boosted 3.8926026\n#> 9                   BayesGLM 5.1940651\n#> 10                    Linear 5.3414045\n#> 11                     Trees 5.3419496\n\nwarnings()\n\nnumerical(data = ISLR::Auto[, 1:ncol(ISLR::Auto)-1], colnum = 1, numresamples = 25, train_amount = 0.50, test_amount = 0.50)\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024181 \n#> gamma= 17.8754    alpha= 2.1148   beta= 28311.91\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7052367 \n#> gamma= 11.3463    alpha= 2.1466   beta= 4183.872\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023708 \n#> gamma= 17.8849    alpha= 2.8721   beta= 13923.37\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7050725 \n#> gamma= 13.5256    alpha= 2.5551   beta= 5748.537\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025055 \n#> gamma= 17.435     alpha= 2.9693   beta= 14578.96\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.705473 \n#> gamma= 13.7312    alpha= 2.9181   beta= 4183.115\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024799 \n#> gamma= 18.1091    alpha= 2.8086   beta= 27836.69\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7055993 \n#> gamma= 13.4459    alpha= 2.769    beta= 5106.886\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024181 \n#> gamma= 19.4595    alpha= 3.5149   beta= 10268.65\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7048689 \n#> gamma= 12.8728    alpha= 2.2178   beta= 7297.72\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025317 \n#> gamma= 18.427     alpha= 3.0993   beta= 19500.47\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7047266 \n#> gamma= 13.2094    alpha= 2.2656   beta= 5097.726\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024181 \n#> gamma= 18.5107    alpha= 2.5087   beta= 25206.34\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7052939 \n#> gamma= 11.5393    alpha= 2.6027   beta= 4324.023\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023479 \n#> gamma= 17.9654    alpha= 3.0229   beta= 10051.06\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.705412 \n#> gamma= 11.2896    alpha= 1.9528   beta= 4490.733\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023143 \n#> gamma= 17.542     alpha= 2.4203   beta= 17709.26\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7048689 \n#> gamma= 12.0062    alpha= 2.2044   beta= 4532.527\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025719 \n#> gamma= 19.0704    alpha= 3.2064   beta= 9272.78\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7051261 \n#> gamma= 12.3612    alpha= 2.4379   beta= 6027.653\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023593 \n#> gamma= 19.3023    alpha= 3.7549   beta= 9427.73\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7047266 \n#> gamma= 12.0255    alpha= 2.324    beta= 5254.673\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024799 \n#> gamma= 18.8087    alpha= 3.1382   beta= 17642.63\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7045924 \n#> gamma= 13.4718    alpha= 2.4994   beta= 4805.303\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023593 \n#> gamma= 18.9825    alpha= 2.808    beta= 11395.42\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7057316 \n#> gamma= 11.8978    alpha= 2.7329   beta= 3679.377\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025449 \n#> gamma= 17.7812    alpha= 2.7909   beta= 10098.7\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.705412 \n#> gamma= 10.1428    alpha= 1.2125   beta= 4973.896\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025449 \n#> gamma= 18.794     alpha= 3.3928   beta= 9205.054\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7049182 \n#> gamma= 14.2765    alpha= 2.1772   beta= 4674.874\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025584 \n#> gamma= 18.7658    alpha= 3.225    beta= 9332.669\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7048205 \n#> gamma= 13.8588    alpha= 2.1879   beta= 6368.032\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7026858 \n#> gamma= 19.1712    alpha= 3.6412   beta= 8742.606\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7045924 \n#> gamma= 13.6159    alpha= 1.8263   beta= 5741.086\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024061 \n#> gamma= 18.6625    alpha= 3.2693   beta= 15446.81\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7051261 \n#> gamma= 12.8168    alpha= 2.2118   beta= 4232.279\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025185 \n#> gamma= 18.7258    alpha= 3.3314   beta= 8802.803\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7058703 \n#> gamma= 13.0608    alpha= 2.5011   beta= 5631.919\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024799 \n#> gamma= 18.1967    alpha= 2.4819   beta= 10940.86\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7042319 \n#> gamma= 12.74      alpha= 2.16     beta= 8684.951\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023593 \n#> gamma= 18.1476    alpha= 3.3654   beta= 12255.74\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.70502 \n#> gamma= 13.0133    alpha= 2.3955   beta= 5689.636\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024181 \n#> gamma= 18.6778    alpha= 3.3321   beta= 21537.02\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7058703 \n#> gamma= 11.6379    alpha= 2.4931   beta= 4076.327\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024548 \n#> gamma= 18.5762    alpha= 2.6635   beta= 11997.62\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7051808 \n#> gamma= 12.5477    alpha= 2.418    beta= 4228.324\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.702671 \n#> gamma= 18.7659    alpha= 2.5637   beta= 9836.966\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7048689 \n#> gamma= 12.4908    alpha= 2.0804   beta= 5926.432\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025584 \n#> gamma= 18.9006    alpha= 2.7887   beta= 16108.82\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7050725 \n#> gamma= 14.0494    alpha= 2.931    beta= 4645.783\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#>                        Model     Error\n#> 1          Ensemble_BayesGLM 0.1050296\n#> 2                   BayesRNN 0.1188054\n#> 3          Ensemble_BayesRNN 0.2071283\n#> 4     Ensemble_Random_Forest 0.8589843\n#> 5              Random_Forest 1.5462773\n#> 6             Ensemble_Trees 1.5752658\n#> 7  Ensemble_Gradient_Boosted 1.5775952\n#> 8           Gradient_Boosted 2.8410555\n#> 9                   BayesGLM 3.3874456\n#> 10                     Trees 3.5410364\n#> 11                    Linear 3.5660828"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"one-of-your-own-add-a-model-to-the-individual-models-and-a-model-to-the-ensemble-of-models","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.4 One of your own: Add a model to the individual models, and a model to the ensemble of models","text":"One : Change data, run , comment results","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"post-your-results-on-social-media-in-a-way-that-a-non-technical-person-can-understand-them.-for-example","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.5 Post your results on social media in a way that a non-technical person can understand them. For example:","text":"“Just ran six individual six ensemble models, easy , \nerrors warnings. plan ensembles data sets soon.\n#AIEnsembles","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"exercises-to-help-you-improve-your-skills","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.6 Exercises to help you improve your skills:","text":"Build individual numerical model using following model\nmethods (’s perfectly OK check prior sections book, \nexample delayed repetition):Gradient Boosted (gmb library)Rpart (rpart library)Support Vector Machines (tuned e1071 library)One model method choosingBuild ensemble using four methods, test using Boston\nHousing data set. Compare results ensemble one made\ntext chapter.Apply function made different numerical data set. can\ndone one line code, ensemble set .","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"post-the-results-of-your-new-ensemble-on-social-media-in-a-way-that-helps-others-understand-the-results-or-methods.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.7 Post the results of your new ensemble on social media in a way that helps others understand the results or methods.","text":"","code":""},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"classification-data-how-to-make-14-individual-classification-models","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5 Classification data: How to make 14 individual classification models","text":"Ensembles numerical data give results often superior individual model. ’ve seen results ensembles numerical data beat nearly individual models, measures lowest error rate.Now going classification data. build 15 individual models classification data chapter.basic series steps numerical data. follow steps, complete process models classification data.Load librarySet initial values 0Create functionBreak data train test setsSet random resamplingFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setAll models structured way close identical possible. high level consistency makes easier spot errors.","code":""},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"what-is-classification-data","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.1 What is classification data?","text":"Classification models set models identify class specific observation. example, Carseats data set, Shelve Location example data type:look ShelveLoc column, set numbers, one three locations shelf: Bad, Medium Good. Classification models statistical models predict class data.Classification models similar numerical models. follow basic steps numerical models, use classification models instead.One big difference accuracy measured classification data. numerical models used root mean squared error. measure classification simply accuracy result. can measured directly matrix values. example:C50 test results:accuracy determined calculating number correct responses divided total number responses. correct responses along main diagonal.example, model correct predicted 83 bad responses, however also predicted response bad actually Good, also predicted Bad actually Medium. correct responses main diagonal, responses errors. use calculation accuracy model results.case, (83 + 75 + 272) / (83 + 1 + 105 + 4 + 75 + 56 + 106 + 90 + 272) = 0.5429293. create function calculate result automatically classification model. Clearly higher accuracy, better results. best possible accuracy result 1.00.","code":"\nlibrary(ISLR)\nhead(Carseats)\n#>   Sales CompPrice Income Advertising Population Price\n#> 1  9.50       138     73          11        276   120\n#> 2 11.22       111     48          16        260    83\n#> 3 10.06       113     35          10        269    80\n#> 4  7.40       117    100           4        466    97\n#> 5  4.15       141     64           3        340   128\n#> 6 10.81       124    113          13        501    72\n#>   ShelveLoc Age Education Urban  US\n#> 1       Bad  42        17   Yes Yes\n#> 2      Good  65        10   Yes Yes\n#> 3    Medium  59        12   Yes Yes\n#> 4    Medium  55        14   Yes Yes\n#> 5       Bad  38        13   Yes  No\n#> 6       Bad  78        16    No Yes"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"build-our-first-classification-model-from-the-structure","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.2 Build our first classification model from the structure:","text":"Now structure, let’s build model:Now see one individual classification model made, let’s make 11 (total 12).","code":"\n\n# Load libraries\n\n# Set initial values to 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\n\n# Set up resamples\n\n# Randomize the rows\n\n# Split data into train and test sets\n\n# Fit the model on the training data\n\n# Check accuracy and make predictions from the model, applied to the test data\n\n# Calculate overall model accuracy\n\n# Calculate table\n# Print table results\n\n# Return accuracy results\n\n# Closing braces for numresamples loop\n# Closing brace for classification1 function\n\n# Test the function\n\n# Check for errors\n\n# Load libraries\nlibrary(C50)\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Set initial values to 0:\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_validation_accuracy <- 0\nC50_overfitting <- 0\nC50_holdout <- 0\nC50_table_total <- 0\n\n# Build the function:\nC50_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Changes target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\nC50_train_pred <- predict(C50_train_fit, train)\nC50_train_table <- table(C50_train_pred, y_train)\nC50_train_accuracy[i] <- sum(diag(C50_train_table)) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\nC50_train_mean <- mean(diag(C50_train_table)) / mean(C50_train_table)\n\n# Check accuracy and make predictions from the model, applied to the test data\nC50_test_pred <- predict(C50_train_fit, test)\nC50_test_table <- table(C50_test_pred, y_test)\nC50_test_accuracy[i] <- sum(diag(C50_test_table)) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\nC50_test_mean <- mean(diag(C50_test_table)) / mean(C50_test_table)\n\n# Calculate accuracy\nC50_holdout[i] <- mean(c(C50_test_accuracy_mean))\nC50_holdout_mean <- mean(C50_holdout)\n\n# Calculate table\nC50_table <- C50_test_table\nC50_table_total <- C50_table_total + C50_table\n\nprint(C50_table_total)\n\n# Return accuracy result\nreturn(c(C50_holdout_mean))\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\nC50_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>              y_test\n#> C50_test_pred Bad Good Medium\n#>        Bad      8    0     10\n#>        Good     0   14      6\n#>        Medium  22    7     41\n#> [1] 0.5833333"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"adabag-for-classification-data","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.3 Adabag for classification data","text":"","code":"\n\n# Load libraries\nlibrary(ipred)\n\n# Set initial values to 0\nadabag_train_accuracy <- 0\nadabag_test_accuracy <- 0\nadabag_validation_accuracy <- 0\nadabag_holdout <- 0\nadabag_table_total <- 0\n\n# Build the function\nadabag_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nadabag_train_fit <- ipred::bagging(formula = y ~ ., data = train01)\nadabag_train_pred <- predict(object = adabag_train_fit, newdata = train)\nadabag_train_table <- table(adabag_train_pred, y_train)\nadabag_train_accuracy[i] <- sum(diag(adabag_train_table)) / sum(adabag_train_table)\nadabag_train_accuracy_mean <- mean(adabag_train_accuracy)\n  \n# Check accuracy and make predictions from the model, applied to the test data\nadabag_test_pred <- predict(object = adabag_train_fit, newdata = test01)\nadabag_test_table <- table(adabag_test_pred, y_test)\nadabag_test_accuracy[i] <- sum(diag(adabag_test_table)) / sum(adabag_test_table)\nadabag_test_accuracy_mean <- mean(adabag_test_accuracy)\nadabag_test_mean <- mean(diag(adabag_test_table)) / mean(adabag_test_table)\n\n# Calculate accuracy\nadabag_holdout[i] <- mean(c(adabag_test_accuracy_mean))\nadabag_holdout_mean <- mean(adabag_holdout)\n\n# Calculate table\nadabag_table <- adabag_test_table\nadabag_table_total <- adabag_table_total + adabag_table\n\nprint(adabag_table_total)\n\n# Return accuracy results\nreturn(adabag_holdout_mean)\n\n} # Closing braces for numresamples loop\n\n} # Closing brace for classification1 function\n\n# Test the function\nadabag_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> adabag_test_pred Bad Good Medium\n#>           Bad     11    0      7\n#>           Good     0   13      2\n#>           Medium  15    8     44\n#> [1] 0.68\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"bagged-random-forest-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.4 Bagged Random Forest","text":"","code":"\n\n# Load libraries\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\n\n# Set initial values to 0\nbag_rf_train_accuracy <- 0\nbag_rf_test_accuracy <- 0\nbag_rf_validation_accuracy <- 0\nbag_rf_overfitting <- 0\nbag_rf_holdout <- 0\nbag_rf_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nbag_rf_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nbag_rf_train_fit <- randomForest::randomForest(y ~ ., data = train01, mtry = ncol(train))\nbag_rf_train_pred <- predict(bag_rf_train_fit, train, type = \"class\")\nbag_rf_train_table <- table(bag_rf_train_pred, y_train)\nbag_rf_train_accuracy[i] <- sum(diag(bag_rf_train_table)) / sum(bag_rf_train_table)\nbag_rf_train_accuracy_mean <- mean(bag_rf_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nbag_rf_test_pred <- predict(bag_rf_train_fit, test, type = \"class\")\nbag_rf_test_table <- table(bag_rf_test_pred, y_test)\nbag_rf_test_accuracy[i] <- sum(diag(bag_rf_test_table)) / sum(bag_rf_test_table)\nbag_rf_test_accuracy_mean <- mean(bag_rf_test_accuracy)\n\n# Calculate model accuracy\nbag_rf_holdout[i] <- mean(c(bag_rf_test_accuracy_mean))\nbag_rf_holdout_mean <- mean(bag_rf_holdout)\n\n# Calculate table\nbag_rf_table <- bag_rf_test_table\nbag_rf_table_total <- bag_rf_table_total + bag_rf_table\n\n# Print table results\nprint(bag_rf_table_total)\n\n# Return accuracy results\nreturn(bag_rf_holdout_mean)\n\n}# Closing braces for numresamples loop\n}# Closing brace for classification1 function\n\n# Test the function\nbag_rf_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> bag_rf_test_pred Bad Good Medium\n#>           Bad      9    1      6\n#>           Good     0   10      6\n#>           Medium  12    6     36\n#> [1] 0.6395349\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"linear-model","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.5 Linear model","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n#> \n#> Attaching package: 'MachineShop'\n#> The following object is masked from 'package:purrr':\n#> \n#>     lift\n#> The following object is masked from 'package:stats':\n#> \n#>     ppr\n\n# Set initial values to 0\nlinear_train_accuracy <- 0\nlinear_validation_accuracy <- 0\nlinear_test_accuracy <- 0\nlinear_test_accuracy_mean <- 0\nlinear_holdout <- 0\nlinear_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nlinear1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nlinear_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"LMModel\")\nlinear_train_pred <- predict(object = linear_train_fit, newdata = train01)\nlinear_train_table <- table(linear_train_pred, y_train)\nlinear_train_accuracy[i] <- sum(diag(linear_train_table)) / sum(linear_train_table)\nlinear_train_accuracy_mean <- mean(linear_train_accuracy)\nlinear_train_mean <- mean(diag(linear_train_table)) / mean(linear_train_table)\n\n# Check accuracy and make predictions from the model, applied to the test data\nlinear_test_pred <- predict(object = linear_train_fit, newdata = test01)\nlinear_test_table <- table(linear_test_pred, y_test)\nlinear_test_accuracy[i] <- sum(diag(linear_test_table)) / sum(linear_test_table)\nlinear_test_accuracy_mean <- mean(linear_test_accuracy)\n\n# Calculate overall model accuracy\nlinear_holdout[i] <- mean(c(linear_test_accuracy_mean))\nlinear_holdout_mean <- mean(linear_holdout)\n\n# Calculate table\nlinear_table <- linear_test_table\nlinear_table_total <- linear_table_total + linear_table\n\n# Print table results\nprint(linear_table_total)\n\n# Return accuracy results\nreturn(linear_holdout_mean)\n\n}# Closing braces for numresamples loop\n}# Closing brace for classification1 function\n\n# Test the function\nlinear1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> linear_test_pred Bad Good Medium\n#>           Bad      9    0      0\n#>           Good     0    9      1\n#>           Medium  11    5     51\n#> [1] 0.8023256\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"naive-bayes-model","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.6 Naive Bayes model","text":"","code":"\n\n# Load libraries\nlibrary(e1071)\n\n# Set initial values to 0\nn_bayes_train_accuracy <- 0\nn_bayes_test_accuracy <- 0\nn_bayes_accuracy <- 0\nn_bayes_test_accuracy_mean <- 0\nn_bayes_holdout <- 0\nn_bayes_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nn_bayes_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nn_bayes_train_fit <- e1071::naiveBayes(y_train ~ ., data = train)\nn_bayes_train_pred <- predict(n_bayes_train_fit, train)\nn_bayes_train_table <- table(n_bayes_train_pred, y_train)\nn_bayes_train_accuracy[i] <- sum(diag(n_bayes_train_table)) / sum(n_bayes_train_table)\nn_bayes_train_accuracy_mean <- mean(n_bayes_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nn_bayes_test_pred <- predict(n_bayes_train_fit, test)\nn_bayes_test_table <- table(n_bayes_test_pred, y_test)\nn_bayes_test_accuracy[i] <- sum(diag(n_bayes_test_table)) / sum(n_bayes_test_table)\nn_bayes_test_accuracy_mean <- mean(n_bayes_test_accuracy)\n\n# Calculate overall model accuracy\nn_bayes_holdout[i] <- mean(c(n_bayes_test_accuracy_mean))\nn_bayes_holdout_mean <- mean(n_bayes_holdout)\n\n# Calculate table\nn_bayes_table <- n_bayes_test_table\nn_bayes_table_total <- n_bayes_table_total + n_bayes_table\n\n# Print table results\nprint(n_bayes_table_total)\n\n# Return accuracy results\nreturn(n_bayes_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nn_bayes_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                  y_test\n#> n_bayes_test_pred Bad Good Medium\n#>            Bad      8    1      6\n#>            Good     1    9      8\n#>            Medium  15   11     45\n#> [1] 0.5961538\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"partial-least-squares-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.7 Partial Least Squares","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\npls_train_accuracy <- 0\npls_test_accuracy <- 0\npls_accuracy <- 0\npls_test_accuracy_mean <- 0\npls_holdout <- 0\npls_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\npls_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\npls_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"PLSModel\")\npls_train_predict <- predict(object = pls_train_fit, newdata = train01)\npls_train_table <- table(pls_train_predict, y_train)\npls_train_accuracy[i] <- sum(diag(pls_train_table)) / sum(pls_train_table)\npls_train_accuracy_mean <- mean(pls_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\npls_test_predict <- predict(object = pls_train_fit, newdata = test01)\npls_test_table <- table(pls_test_predict, y_test)\npls_test_accuracy[i] <- sum(diag(pls_test_table)) / sum(pls_test_table)\npls_test_accuracy_mean <- mean(pls_test_accuracy)\npls_test_pred <- pls_test_predict\n\n# Calculate overall model accuracy\npls_holdout[i] <- mean(c(pls_test_accuracy_mean))\npls_holdout_mean <- mean(pls_holdout)\n\n# Calculate table\npls_table <- pls_test_table\npls_table_total <- pls_table_total + pls_table\n\n# Print table results\nprint(pls_table_total)\n\n# Return accuracy results\nreturn(pls_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\npls_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> pls_test_predict Bad Good Medium\n#>           Bad      0    0      0\n#>           Good     0    0      0\n#>           Medium  17   19     46\n#> [1] 0.5609756\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"penalized-discriminant-analysis-model","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.8 Penalized Discriminant Analysis Model","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\npda_train_accuracy <- 0\npda_test_accuracy <- 0\npda_accuracy <- 0\npda_test_accuracy_mean <- 0\npda_holdout <- 0\npda_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\npda_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\npda_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"PDAModel\")\npda_train_predict <- predict(object = pda_train_fit, newdata = train01)\npda_train_table <- table(pda_train_predict, y_train)\npda_train_accuracy[i] <- sum(diag(pda_train_table)) / sum(pda_train_table)\npda_train_accuracy_mean <- mean(pda_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\npda_test_predict <- predict(object = pda_train_fit, newdata = test01)\npda_test_table <- table(pda_test_predict, y_test)\npda_test_accuracy[i] <- sum(diag(pda_test_table)) / sum(pda_test_table)\npda_test_accuracy_mean <- mean(pda_test_accuracy)\npda_test_pred <- pda_test_predict\n\n# Calculate overall model accuracy\npda_holdout[i] <- mean(c(pda_test_accuracy_mean))\npda_holdout_mean <- mean(pda_holdout)\n\n# Calculate table\npda_table <- pda_test_table\npda_table_total <- pda_table_total + pda_table\n\n# Print table results\nprint(pda_table_total)\n\n# Return accuracy results\nreturn(pda_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\npda_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> pda_test_predict Bad Good Medium\n#>           Bad     15    0      4\n#>           Good     0   12      6\n#>           Medium   4    3     49\n#> [1] 0.8172043\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"random-forest-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.9 Random Forest","text":"","code":"\n\n# Load libraries\nlibrary(randomForest)\n\n# Set initial values to 0\nrf_train_accuracy <- 0\nrf_test_accuracy <- 0\nrf_accuracy <- 0\nrf_test_accuracy_mean <- 0\nrf_holdout <- 0\nrf_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nrf_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nrf_train_fit <- randomForest::randomForest(x = train, y = y_train, data = df)\nrf_train_pred <- predict(rf_train_fit, train, type = \"class\")\nrf_train_table <- table(rf_train_pred, y_train)\nrf_train_accuracy[i] <- sum(diag(rf_train_table)) / sum(rf_train_table)\nrf_train_accuracy_mean <- mean(rf_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nrf_test_pred <- predict(rf_train_fit, test, type = \"class\")\nrf_test_table <- table(rf_test_pred, y_test)\nrf_test_accuracy[i] <- sum(diag(rf_test_table)) / sum(rf_test_table)\nrf_test_accuracy_mean <- mean(rf_test_accuracy)\n\n# Calculate overall model accuracy\nrf_holdout[i] <- mean(c(rf_test_accuracy_mean))\nrf_holdout_mean <- mean(rf_holdout)\n\n# Calculate table\nrf_table <- rf_test_table\nrf_table_total <- rf_table_total + rf_table\n\n# Print table results\nprint(rf_table_total)\n\n# Return accuracy results\nreturn(rf_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nrf_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>             y_test\n#> rf_test_pred Bad Good Medium\n#>       Bad      6    0      8\n#>       Good     1   18      3\n#>       Medium  16   12     56\n#> [1] 0.6666667\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"ranger","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.10 Ranger","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\nranger_train_accuracy <- 0\nranger_test_accuracy <- 0\nranger_accuracy <- 0\nranger_test_accuracy_mean <- 0\nranger_holdout <- 0\nranger_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nranger_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n  \n# Fit the model on the training data\nranger_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"RangerModel\")\nranger_train_predict <- predict(object = ranger_train_fit, newdata = train01)\nranger_train_table <- table(ranger_train_predict, y_train)\nranger_train_accuracy[i] <- sum(diag(ranger_train_table)) / sum(ranger_train_table)\nranger_train_accuracy_mean <- mean(ranger_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nranger_test_predict <- predict(object = ranger_train_fit, newdata = test01)\nranger_test_table <- table(ranger_test_predict, y_test)\nranger_test_accuracy[i] <- sum(diag(ranger_test_table)) / sum(ranger_test_table)\nranger_test_accuracy_mean <- mean(ranger_test_accuracy)\nranger_test_pred <- ranger_test_predict\n\n# Calculate overall model accuracy\nranger_holdout[i] <- mean(c(ranger_test_accuracy_mean))\nranger_holdout_mean <- mean(ranger_holdout)\n\n# Calculate table\nranger_table <- ranger_test_table\nranger_table_total <- ranger_table_total + ranger_table\n\n# Print table results\nprint(ranger_table_total)\n\n# Return accuracy results\nreturn(ranger_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nranger_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                    y_test\n#> ranger_test_predict Bad Good Medium\n#>              Bad      2    1      3\n#>              Good     1    8      3\n#>              Medium  16   18     43\n#> [1] 0.5578947\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"regularized-discriminant-analysis","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.11 Regularized Discriminant Analysis","text":"","code":"\n\n# Load libraries\nlibrary(klaR)\n#> Loading required package: MASS\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\n\n# Set initial values to 0\nrda_train_accuracy <- 0\nrda_test_accuracy <- 0\nrda_accuracy <- 0\nrda_test_accuracy_mean <- 0\nrda_holdout <- 0\nrda_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nrda_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nrda_train_fit <- klaR::rda(y_train ~ ., data = train)\nrda_train_pred <- predict(object = rda_train_fit, newdata = train)\nrda_train_table <- table(rda_train_pred$class, y_train)\nrda_train_accuracy[i] <- sum(diag(rda_train_table)) / sum(rda_train_table)\nrda_train_accuracy_mean <- mean(rda_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nrda_test_pred <- predict(object = rda_train_fit, newdata = test)\nrda_test_table <- table(rda_test_pred$class, y_test)\nrda_test_accuracy[i] <- sum(diag(rda_test_table)) / sum(rda_test_table)\nrda_test_accuracy_mean <- mean(rda_test_accuracy)\n\n# Calculate overall model accuracy\nrda_holdout[i] <- mean(c(rda_test_accuracy_mean))\nrda_holdout_mean <- mean(rda_holdout)\n\n# Calculate table\nrda_table <- rda_test_table\nrda_table_total <- rda_table_total + rda_table\n\n# Print table results\nprint(rda_table_total)\n\n# Return accuracy results\nreturn(rda_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nrda_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>         y_test\n#>          Bad Good Medium\n#>   Bad      0    0      0\n#>   Good     0    0      0\n#>   Medium  16   22     55\n#> [1] 0.5913978\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"rpart-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.12 Rpart","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\nrpart_train_accuracy <- 0\nrpart_test_accuracy <- 0\nrpart_accuracy <- 0\nrpart_test_accuracy_mean <- 0\nrpart_holdout <- 0\nrpart_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nrpart_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nrpart_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"RPartModel\")\nrpart_train_predict <- predict(object = rpart_train_fit, newdata = train01)\nrpart_train_table <- table(rpart_train_predict, y_train)\nrpart_train_accuracy[i] <- sum(diag(rpart_train_table)) / sum(rpart_train_table)\nrpart_train_accuracy_mean <- mean(rpart_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nrpart_test_predict <- predict(object = rpart_train_fit, newdata = test01)\nrpart_test_table <- table(rpart_test_predict, y_test)\nrpart_test_accuracy[i] <- sum(diag(rpart_test_table)) / sum(rpart_test_table)\nrpart_test_accuracy_mean <- mean(rpart_test_accuracy)\nrpart_test_pred <- rpart_test_predict\n\n# Calculate overall model accuracy\nrpart_holdout[i] <- mean(c(rpart_test_accuracy_mean))\nrpart_holdout_mean <- mean(rpart_holdout)\n\n# Calculate table\nrpart_table <- rpart_test_table\nrpart_table_total <- rpart_table_total + rpart_table\n  \n# Print table results\nprint(rpart_table_total)\n  \n# Return accuracy results\nreturn(rpart_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nrpart_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                   y_test\n#> rpart_test_predict Bad Good Medium\n#>             Bad     11    1     11\n#>             Good     1   14      9\n#>             Medium  11    4     35\n#> [1] 0.6185567\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"support-vector-machines-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.13 Support Vector Machines","text":"","code":"\n\n# Load libraries\nlibrary(e1071)\n\n# Set initial values to 0\nsvm_train_accuracy <- 0\nsvm_test_accuracy <- 0\nsvm_accuracy <- 0\nsvm_test_accuracy_mean <- 0\nsvm_holdout <- 0\nsvm_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nsvm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nsvm_train_fit <- e1071::svm(y_train ~ ., data = train, kernel = \"radial\", gamma = 1, cost = 1)\nsvm_train_pred <- predict(svm_train_fit, train, type = \"class\")\nsvm_train_table <- table(svm_train_pred, y_train)\nsvm_train_accuracy[i] <- sum(diag(svm_train_table)) / sum(svm_train_table)\nsvm_train_accuracy_mean <- mean(svm_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nsvm_test_pred <- predict(svm_train_fit, test, type = \"class\")\nsvm_test_table <- table(svm_test_pred, y_test)\nsvm_test_accuracy[i] <- sum(diag(svm_test_table)) / sum(svm_test_table)\nsvm_test_accuracy_mean <- mean(svm_test_accuracy)\n\n# Calculate overall model accuracy\nsvm_holdout[i] <- mean(c(svm_test_accuracy_mean))\nsvm_holdout_mean <- mean(svm_holdout)\n\n# Calculate table\nsvm_table <- svm_test_table\nsvm_table_total <- svm_table_total + svm_table\n\n# Print table results\nprint(svm_table_total)\n\n# Return accuracy results\nreturn(svm_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nsvm_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>              y_test\n#> svm_test_pred Bad Good Medium\n#>        Bad      0    0      0\n#>        Good     0    0      0\n#>        Medium  29   21     51\n#> [1] 0.5049505\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"trees-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.14 Trees","text":"","code":"\n\n# Load libraries\nlibrary(tree)\n\n# Set initial values to 0\ntree_train_accuracy <- 0\ntree_test_accuracy <- 0\ntree_accuracy <- 0\ntree_test_accuracy_mean <- 0\ntree_holdout <- 0\ntree_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\ntree_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\ntree_train_fit <- tree::tree(y_train ~ ., data = train)\ntree_train_pred <- predict(tree_train_fit, train, type = \"class\")\ntree_train_table <- table(tree_train_pred, y_train)\ntree_train_accuracy[i] <- sum(diag(tree_train_table)) / sum(tree_train_table)\ntree_train_accuracy_mean <- mean(tree_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\ntree_test_pred <- predict(tree_train_fit, test, type = \"class\")\ntree_test_table <- table(tree_test_pred, y_test)\ntree_test_accuracy[i] <- sum(diag(tree_test_table)) / sum(tree_test_table)\ntree_test_accuracy_mean <- mean(tree_test_accuracy)\n\n# Calculate overall model accuracy\ntree_holdout[i] <- mean(c(tree_test_accuracy_mean))\ntree_holdout_mean <- mean(tree_holdout)\n\n# Calculate table\ntree_table <- tree_test_table\ntree_table_total <- tree_table_total + tree_table  \n\n# Print table results\nprint(tree_table_total)\n\n# Return accuracy results\nreturn(tree_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\ntree_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>               y_test\n#> tree_test_pred Bad Good Medium\n#>         Bad     10    0     11\n#>         Good     2   11      8\n#>         Medium   6    7     42\n#> [1] 0.6494845\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"xgboost-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.15 XGBoost","text":"","code":"\n\n# Load libraries\nlibrary(xgboost)\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\n\n# Set initial values to 0\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_accuracy <- 0\nxgb_test_accuracy_mean <- 0\nxgb_holdout <- 0\nxgb_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nxgb_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_train), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_train + 1]\nxgb_train_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_train_accuracy[i] <- sum(diag(xgb_train_table)) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_test + 1]\nxgb_test_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_test_accuracy[i] <- sum(diag(xgb_test_table)) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\n# Calculate overall model accuracy\nxgb_holdout[i] <- mean(c(xgb_test_accuracy_mean))\nxgb_holdout_mean <- mean(xgb_holdout)\n\n# Calculate table\nxgb_table <- xgb_test_table\nxgb_table_total <- xgb_table_total + xgb_table\n\n# Print table results\nprint(xgb_table_total)\n\n# Return accuracy results\nreturn(xgb_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nxgb_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>         \n#>          Bad Good Medium\n#>   Bad      2    0      2\n#>   Good     0   12      4\n#>   Medium  24   11     40\n#> [1] 0.5684211\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"post-your-results","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.16 Post your results","text":"","code":""},{"path":"building-ensembles-of-classification-models.html","id":"building-ensembles-of-classification-models","chapter":"6 Building ensembles of classification models","heading":"6 Building ensembles of classification models","text":"section building two ensembles classification models. use six classification models, six ensembles, total 12 results.","code":""},{"path":"building-ensembles-of-classification-models.html","id":"lets-start-at-the-end-and-work-backwards","chapter":"6 Building ensembles of classification models","heading":"6.0.1 Let’s start at the end and work backwards","text":"know want finish predictions ensemble classification models. Therefore need ensemble classification models. Therefore need classification models. Let’s choose five classification models individual models, use five ensemble. Note may use modeling method wish ensemble, since ’s data.final result look something like :Predictions holdout data classification model 1Predictions holdout data classification model 2Predictions holdout data classification model 3Predictions holdout data classification model 4Predictions holdout data classification model 5Use predictions make ensembleUse ensemble models make predictions ensemble holdout dataReport resultsLet’s come list five classification models use:Bagged Random ForestC50RangerSupport Vector MachinesXGBoostNote nothing special using five models. number models may used, set good start.solution also add mean duration model finished report.Since basic outline, know want end, ready begin.","code":"\n\n# Load libraries\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::combine()  masks randomForest::combine()\n#> ✖ dplyr::filter()   masks stats::filter()\n#> ✖ dplyr::lag()      masks stats::lag()\n#> ✖ ggplot2::margin() masks randomForest::margin()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(tree)\n\n# Set initial values to 0\nbag_rf_train_accuracy <- 0\nbag_rf_test_accuracy <- 0\nbag_rf_validation_accuracy <- 0\nbag_rf_overfitting <- 0\nbag_rf_holdout <- 0\nbag_rf_table_total <- 0\nbag_rf_duration <- 0\n\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_validation_accuracy <- 0\nC50_overfitting <- 0\nC50_holdout <- 0\nC50_table_total <- 0\nC50_duration <- 0\n\nranger_train_accuracy <- 0\nranger_test_accuracy <- 0\nranger_accuracy <- 0\nranger_test_accuracy_mean <- 0\nranger_holdout <- 0\nranger_table_total <- 0\nranger_duration <- 0\n\nsvm_train_accuracy <- 0\nsvm_test_accuracy <- 0\nsvm_accuracy <- 0\nsvm_test_accuracy_mean <- 0\nsvm_holdout <- 0\nsvm_table_total <- 0\nsvm_duration <- 0\n\nensemble_bag_rf_train_accuracy <- 0\nensemble_bag_rf_test_accuracy <- 0\nensemble_bag_rf_validation_accuracy <- 0\nensemble_bag_rf_overfitting <- 0\nensemble_bag_rf_holdout <- 0\nensemble_bag_rf_table_total <- 0\nensemble_bag_rf_duration <- 0\n\nensemble_C50_train_accuracy <- 0\nensemble_C50_test_accuracy <- 0\nensemble_C50_validation_accuracy <- 0\nensemble_C50_overfitting <- 0\nensemble_C50_holdout <- 0\nensemble_C50_table_total <- 0\nensemble_C50_duration <- 0\n\nensemble_ranger_train_accuracy <- 0\nensemble_ranger_test_accuracy <- 0\nensemble_ranger_validation_accuracy <- 0\nensemble_ranger_overfitting <- 0\nensemble_ranger_holdout <- 0\nensemble_ranger_table_total <- 0\nensemble_ranger_duration <- 0\n\nensemble_rf_train_accuracy <- 0\nensemble_rf_test_accuracy <- 0\nensemble_rf_validation_accuracy <- 0\nensemble_rf_overfitting <- 0\nensemble_rf_holdout <- 0\nensemble_rf_table_total <- 0\nensemble_rf_duration <- 0\n\nensemble_svm_train_accuracy <- 0\nensemble_svm_test_accuracy <- 0\nensemble_svm_validation_accuracy <- 0\nensemble_svm_overfitting <- 0\nensemble_svm_holdout <- 0\nensemble_svm_table_total <- 0\nensemble_svm_duration <- 0\n\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nclassification_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nbag_rf_start <- Sys.time()\nbag_rf_train_fit <- randomForest::randomForest(y ~ ., data = train01, mtry = ncol(train))\nbag_rf_train_pred <- predict(bag_rf_train_fit, train, type = \"class\")\nbag_rf_train_table <- table(bag_rf_train_pred, y_train)\nbag_rf_train_accuracy[i] <- sum(diag(bag_rf_train_table)) / sum(bag_rf_train_table)\nbag_rf_train_accuracy_mean <- mean(bag_rf_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nbag_rf_test_pred <- predict(bag_rf_train_fit, test, type = \"class\")\nbag_rf_test_table <- table(bag_rf_test_pred, y_test)\nbag_rf_test_accuracy[i] <- sum(diag(bag_rf_test_table)) / sum(bag_rf_test_table)\nbag_rf_test_accuracy_mean <- mean(bag_rf_test_accuracy)\n\n# Calculate model accuracy\nbag_rf_holdout[i] <- mean(c(bag_rf_test_accuracy_mean))\nbag_rf_holdout_mean <- mean(bag_rf_holdout)\n\n# Calculate table\nbag_rf_table <- bag_rf_test_table\nbag_rf_table_total <- bag_rf_table_total + bag_rf_table\n\nbag_rf_end <- Sys.time()\nbag_rf_duration[i] <- bag_rf_end - bag_rf_start\nbag_rf_duration_mean <- mean(bag_rf_duration)\n\n# C50 model\nC50_start <- Sys.time()\n# Fit the model on the training data\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\nC50_train_pred <- predict(C50_train_fit, train)\nC50_train_table <- table(C50_train_pred, y_train)\nC50_train_accuracy[i] <- sum(diag(C50_train_table)) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\nC50_train_mean <- mean(diag(C50_train_table)) / mean(C50_train_table)\n\n# Check accuracy and make predictions from the model, applied to the test data\nC50_test_pred <- predict(C50_train_fit, test)\nC50_test_table <- table(C50_test_pred, y_test)\nC50_test_accuracy[i] <- sum(diag(C50_test_table)) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\nC50_test_mean <- mean(diag(C50_test_table)) / mean(C50_test_table)\n\n# Calculate accuracy\nC50_holdout[i] <- mean(c(C50_test_accuracy_mean))\nC50_holdout_mean <- mean(C50_holdout)\n\nC50_end <- Sys.time()\nC50_duration[i] <- C50_end - C50_start\nC50_duration_mean <- mean(C50_duration)\n\n# Ranger model\n\nranger_start <- Sys.time()\n\n# Fit the model on the training data\nranger_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"RangerModel\")\nranger_train_predict <- predict(object = ranger_train_fit, newdata = train01)\nranger_train_table <- table(ranger_train_predict, y_train)\nranger_train_accuracy[i] <- sum(diag(ranger_train_table)) / sum(ranger_train_table)\nranger_train_accuracy_mean <- mean(ranger_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nranger_test_predict <- predict(object = ranger_train_fit, newdata = test01)\nranger_test_table <- table(ranger_test_predict, y_test)\nranger_test_accuracy[i] <- sum(diag(ranger_test_table)) / sum(ranger_test_table)\nranger_test_accuracy_mean <- mean(ranger_test_accuracy)\nranger_test_pred <- ranger_test_predict\n\n# Calculate overall model accuracy\nranger_holdout[i] <- mean(c(ranger_test_accuracy_mean))\nranger_holdout_mean <- mean(ranger_holdout)\n\nranger_end <- Sys.time()\nranger_duration[i] <- ranger_end - ranger_start\nranger_duration_mean <- mean(ranger_duration)\n\n# Support vector machines\nsvm_start <- Sys.time()\n\nsvm_train_fit <- e1071::svm(y_train ~ ., data = train, kernel = \"radial\", gamma = 1, cost = 1)\nsvm_train_pred <- predict(svm_train_fit, train, type = \"class\")\nsvm_train_table <- table(svm_train_pred, y_train)\nsvm_train_accuracy[i] <- sum(diag(svm_train_table)) / sum(svm_train_table)\nsvm_train_accuracy_mean <- mean(svm_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nsvm_test_pred <- predict(svm_train_fit, test, type = \"class\")\nsvm_test_table <- table(svm_test_pred, y_test)\nsvm_test_accuracy[i] <- sum(diag(svm_test_table)) / sum(svm_test_table)\nsvm_test_accuracy_mean <- mean(svm_test_accuracy)\n\n# Calculate overall model accuracy\nsvm_holdout[i] <- mean(c(svm_test_accuracy_mean))\nsvm_holdout_mean <- mean(svm_holdout)\n\nsvm_end <- Sys.time()\nsvm_duration[i] <- svm_end - svm_start\nsvm_duration_mean <- mean(svm_duration)\n\n# XGBoost\nxgb_start <- Sys.time()\n\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_accuracy <- 0\nxgb_test_accuracy_mean <- 0\nxgb_holdout <- 0\nxgb_table_total <- 0\nxgb_duration <- 0\n\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_train), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_train + 1]\nxgb_train_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_train_accuracy[i] <- sum(diag(xgb_train_table)) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_test + 1]\nxgb_test_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_test_accuracy[i] <- sum(diag(xgb_test_table)) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\n# Calculate overall model accuracy\nxgb_holdout[i] <- mean(c(xgb_test_accuracy_mean))\nxgb_holdout_mean <- mean(xgb_holdout)\n\nxgb_end <- Sys.time()\nxgb_duration[i] <- xgb_end - xgb_start\nxgb_duration_mean <- mean(xgb_duration)\n\n# Build the ensemble of predictions\n\nensemble1 <- data.frame(\n  'Bag_rf' = bag_rf_test_pred,\n  'C50' = C50_test_pred,\n  'Ranger' = ranger_test_predict,\n  'SVM' = svm_test_pred,\n  'XGBoost' = xgb_preds\n)\n\nensemble_row_numbers <- as.numeric(row.names(ensemble1))\nensemble1$y <- df[ensemble_row_numbers, \"y\"]\n\nensemble_index <- sample(c(1:2), nrow(ensemble1), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble1[ensemble_index == 1, ]\nensemble_test <- ensemble1[ensemble_index == 2, ]\nensemble_y_train <- ensemble_train$y\nensemble_y_test <- ensemble_test$y\n\n\n# Ensemble bagged random forest\nensemble_bag_rf_start <- Sys.time()\n\nensemble_bag_train_rf <- randomForest::randomForest(ensemble_y_train ~ ., data = ensemble_train, mtry = ncol(ensemble_train) - 1)\nensemble_bag_rf_train_pred <- predict(ensemble_bag_train_rf, ensemble_train, type = \"class\")\nensemble_bag_rf_train_table <- table(ensemble_bag_rf_train_pred, ensemble_train$y)\nensemble_bag_rf_train_accuracy[i] <- sum(diag(ensemble_bag_rf_train_table)) / sum(ensemble_bag_rf_train_table)\nensemble_bag_rf_train_accuracy_mean <- mean(ensemble_bag_rf_train_accuracy)\n\nensemble_bag_rf_test_pred <- predict(ensemble_bag_train_rf, ensemble_test, type = \"class\")\nensemble_bag_rf_test_table <- table(ensemble_bag_rf_test_pred, ensemble_test$y)\nensemble_bag_rf_test_accuracy[i] <- sum(diag(ensemble_bag_rf_test_table)) / sum(ensemble_bag_rf_test_table)\nensemble_bag_rf_test_accuracy_mean <- mean(ensemble_bag_rf_test_accuracy)\n\nensemble_bag_rf_holdout[i] <- mean(c(ensemble_bag_rf_test_accuracy_mean))\nensemble_bag_rf_holdout_mean <- mean(ensemble_bag_rf_holdout)\n\nensemble_bag_rf_end <- Sys.time()\nensemble_bag_rf_duration[i] <- ensemble_bag_rf_end - ensemble_bag_rf_start\nensemble_bag_rf_duration_mean <- mean(ensemble_bag_rf_duration)\n\n# Ensemble C50\n\nensemble_C50_start <- Sys.time()\n\nensemble_C50_train_fit <- C50::C5.0(ensemble_y_train ~ ., data = ensemble_train)\nensemble_C50_train_pred <- predict(ensemble_C50_train_fit, ensemble_train)\nensemble_C50_train_table <- table(ensemble_C50_train_pred, ensemble_y_train)\nensemble_C50_train_accuracy[i] <- sum(diag(ensemble_C50_train_table)) / sum(ensemble_C50_train_table)\nensemble_C50_train_accuracy_mean <- mean(ensemble_C50_train_accuracy)\n\nensemble_C50_test_pred <- predict(ensemble_C50_train_fit, ensemble_test)\nensemble_C50_test_table <- table(ensemble_C50_test_pred, ensemble_y_test)\nensemble_C50_test_accuracy[i] <- sum(diag(ensemble_C50_test_table)) / sum(ensemble_C50_test_table)\nensemble_C50_test_accuracy_mean <- mean(ensemble_C50_test_accuracy)\n\nensemble_C50_holdout[i] <- mean(c(ensemble_C50_test_accuracy_mean))\nensemble_C50_holdout_mean <- mean(ensemble_C50_holdout)\n\nensemble_C50_end <- Sys.time()\nensemble_C50_duration[i] <- ensemble_C50_end - ensemble_C50_start\nensemble_C50_duration_mean <- mean(ensemble_C50_duration)\n\n# Ensemble using Ranger\n\nensemble_ranger_start <- Sys.time()\n\nensemble_ranger_train_fit <- MachineShop::fit(y ~ ., data = ensemble_train, model = \"RangerModel\")\nensemble_ranger_train_pred <- predict(ensemble_ranger_train_fit, newdata = ensemble_train)\nensemble_ranger_train_table <- table(ensemble_ranger_train_pred, ensemble_y_train)\nensemble_ranger_train_accuracy[i] <- sum(diag(ensemble_ranger_train_table)) / sum(ensemble_ranger_train_table)\nensemble_ranger_train_accuracy_mean <- mean(ensemble_ranger_train_accuracy)\n\nensemble_ranger_test_fit <- MachineShop::fit(y ~ ., data = ensemble_train, model = \"RangerModel\")\nensemble_ranger_test_pred <- predict(ensemble_ranger_test_fit, newdata = ensemble_test)\nensemble_ranger_test_table <- table(ensemble_ranger_test_pred, ensemble_y_test)\nensemble_ranger_test_accuracy[i] <- sum(diag(ensemble_ranger_test_table)) / sum(ensemble_ranger_test_table)\nensemble_ranger_test_accuracy_mean <- mean(ensemble_ranger_test_accuracy)\n\nensemble_ranger_holdout[i] <- mean(c(ensemble_ranger_test_accuracy_mean))\nensemble_ranger_holdout_mean <- mean(ensemble_ranger_holdout)\n\nensemble_ranger_end <- Sys.time()\nensemble_ranger_duration[i] <- ensemble_ranger_end - ensemble_ranger_start\nensemble_ranger_duration_mean <- mean(ensemble_ranger_duration)\n\n# Ensemble Random Forest\n\nensemble_rf_start <- Sys.time()\n\nensemble_train_rf_fit <- randomForest::randomForest(x = ensemble_train, y = ensemble_y_train)\nensemble_rf_train_pred <- predict(ensemble_train_rf_fit, ensemble_train, type = \"class\")\nensemble_rf_train_table <- table(ensemble_rf_train_pred, ensemble_y_train)\nensemble_rf_train_accuracy[i] <- sum(diag(ensemble_rf_train_table)) / sum(ensemble_rf_train_table)\nensemble_rf_train_accuracy_mean <- mean(ensemble_rf_train_accuracy)\n\nensemble_rf_test_pred <- predict(ensemble_train_rf_fit, ensemble_test, type = \"class\")\nensemble_rf_test_table <- table(ensemble_rf_test_pred, ensemble_y_test)\nensemble_rf_test_accuracy[i] <- sum(diag(ensemble_rf_test_table)) / sum(ensemble_rf_test_table)\nensemble_rf_test_accuracy_mean <- mean(ensemble_rf_test_accuracy)\n\nensemble_rf_holdout[i] <- mean(c(ensemble_rf_test_accuracy_mean))\nensemble_rf_holdout_mean <- mean(ensemble_rf_holdout)\n\nensemble_rf_end <- Sys.time()\nensemble_rf_duration[i] <- ensemble_rf_end -ensemble_rf_start\nensemble_rf_duration_mean <- mean(ensemble_rf_duration)\n\n\n# Ensemble Support Vector Machines\n\nensemble_svm_start <- Sys.time()\n\nensemble_svm_train_fit <- e1071::svm(ensemble_y_train ~ ., data = ensemble_train, kernel = \"radial\", gamma = 1, cost = 1)\nensemble_svm_train_pred <- predict(ensemble_svm_train_fit, ensemble_train, type = \"class\")\nensemble_svm_train_table <- table(ensemble_svm_train_pred, ensemble_y_train)\nensemble_svm_train_accuracy[i] <- sum(diag(ensemble_svm_train_table)) / sum(ensemble_svm_train_table)\nensemble_svm_train_accuracy_mean <- mean(ensemble_svm_train_accuracy)\n\nensemble_svm_test_fit <- e1071::svm(ensemble_y_train ~ ., data = ensemble_train, kernel = \"radial\", gamma = 1, cost = 1)\nensemble_svm_test_pred <- predict(ensemble_svm_test_fit, ensemble_test, type = \"class\")\nensemble_svm_test_table <- table(ensemble_svm_test_pred, ensemble_y_test)\nensemble_svm_test_accuracy[i] <- sum(diag(ensemble_svm_test_table)) / sum(ensemble_svm_test_table)\nensemble_svm_test_accuracy_mean <- mean(ensemble_svm_test_accuracy)\n\nensemble_svm_holdout[i] <- mean(c(ensemble_svm_test_accuracy_mean))\nensemble_svm_holdout_mean <- mean(ensemble_svm_holdout)\n\nensemble_svm_end <- Sys.time()\nensemble_svm_duration[i] <-  ensemble_svm_end - ensemble_svm_start\nensemble_svm_duration_mean <- mean(ensemble_svm_duration)\n\n# Return accuracy results\n\nresults <- data.frame(\n  'Model' = c('Bagged_Random_Forest', 'C50', 'Ranger', 'Support_Vector_Machines', 'XGBoost', 'Ensemble_Bag_RF', 'Ensemble_C50', 'Ensemble_Ranger', 'Ensemble_RF', 'Ensemble_SVM'),\n  'Accuracy' = c(bag_rf_holdout_mean, C50_holdout_mean, ranger_holdout_mean, svm_holdout_mean, xgb_holdout_mean, ensemble_bag_rf_holdout_mean, ensemble_C50_holdout_mean, ensemble_ranger_holdout_mean, ensemble_rf_holdout_mean, ensemble_svm_holdout_mean),\n  'Duration' = c(bag_rf_duration_mean, C50_duration_mean, ranger_duration_mean, svm_duration_mean, xgb_duration_mean, ensemble_bag_rf_duration_mean, ensemble_C50_duration_mean, ensemble_ranger_duration_mean, ensemble_rf_duration_mean, ensemble_svm_duration_mean)\n)\n\nresults <- results %>% arrange(desc(Accuracy))\n\nreturn(results)\n\n}# Closing braces for numresamples loop\n}# Closing brace for classification1 function\n\nclassification_1(data = ISLR::Carseats,colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#>                      Model  Accuracy    Duration\n#> 1          Ensemble_Bag_RF 1.0000000 0.130465031\n#> 2             Ensemble_C50 1.0000000 0.007788897\n#> 3              Ensemble_RF 0.9846154 0.019177198\n#> 4             Ensemble_SVM 0.8769231 0.006022215\n#> 5     Bagged_Random_Forest 0.6729560 0.095813036\n#> 6                   Ranger 0.6540881 0.736639977\n#> 7                  XGBoost 0.6415094 6.993471861\n#> 8          Ensemble_Ranger 0.5692308 0.034466028\n#> 9                      C50 0.5345912 0.420025110\n#> 10 Support_Vector_Machines 0.5220126 0.013696194\n\nwarnings()\n\ndf1 <- Ensembles::dry_beans_small\n#> Registered S3 method overwritten by 'tsibble':\n#>   method          from\n#>   format.interval inum\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n\nclassification_1(data = df1, colnum = 17, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#>                      Model  Accuracy    Duration\n#> 1          Ensemble_Bag_RF 1.0000000  0.12236214\n#> 2             Ensemble_C50 1.0000000  0.01281404\n#> 3                   Ranger 0.9236641  0.08070183\n#> 4                  XGBoost 0.9236641 23.26443696\n#> 5     Bagged_Random_Forest 0.9122137  0.16021895\n#> 6                      C50 0.8511450  0.02710509\n#> 7  Support_Vector_Machines 0.8473282  0.02061820\n#> 8             Ensemble_SVM 0.7722772  0.01609302\n#> 9              Ensemble_RF 0.7128713  0.15743709\n#> 10         Ensemble_Ranger 0.1683168  0.04702497\nwarnings()"},{"path":"individual-logistic-models.html","id":"individual-logistic-models","chapter":"7 Individual logistic models","heading":"7 Individual logistic models","text":"Logistic data sets extremely powerful. chapter ’ll use evaluate risk type 2 diabetes Pima Indian women, logistic ensembles chapter ’ll use make recommendations improve performance Lebron James.raises good question: can two fields far apart scientific research (Pima Indians) sports analytics (Lebron) connected? ’s structure data , ’s structure data makes easy use.Logistic regression rooted idea logical variable (hence name). variable logical variable specific number options, usually two options. many possible names come result. Names might include true false, presence absence condition (diabetes), success failure making basket.logistic modeling values converted 1 0 (converted already). Let’s start getting Pima Indians data set Kaggle web site:https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-databaseDownload data set, open system. example,can clearly see eight features used predict ninth feature, Outcome. final logistic model form : Outcome ~ ., data = df.far common way using Generalized Linear Models, begin . follow well established method building model, used numerical classification data:Load librarySet initial values 0Create functionSet random resamplingBreak data train testFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setFor set examples also going add results ROC curve, ROC curve printed automatically.results consistent similar results using Generalized Linear Models data set.","code":"\n\ndiabetes <- read.csv('/Users/russellconte/diabetes.csv')\nhead(diabetes)\n#>   Pregnancies Glucose BloodPressure SkinThickness Insulin\n#> 1           6     148            72            35       0\n#> 2           1      85            66            29       0\n#> 3           8     183            64             0       0\n#> 4           1      89            66            23      94\n#> 5           0     137            40            35     168\n#> 6           5     116            74             0       0\n#>    BMI DiabetesPedigreeFunction Age Outcome\n#> 1 33.6                    0.627  50       1\n#> 2 26.6                    0.351  31       0\n#> 3 23.3                    0.672  32       1\n#> 4 28.1                    0.167  21       0\n#> 5 43.1                    2.288  33       1\n#> 6 25.6                    0.201  30       0\n\n# Load the library\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(pROC)\n#> Type 'citation(\"pROC\")' for a citation.\n#> \n#> Attaching package: 'pROC'\n#> \n#> The following objects are masked from 'package:stats':\n#> \n#>     cov, smooth, var\n\n# Set initial values to 0\n\nglm_train_accuracy <- 0\nglm_test_accuracy <- 0\nglm_holdout_accuracy <- 0\nglm_duration <- 0\nglm_table_total <- 0\n\n# Create the function\n\nglm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \ncolnames(data)[colnum] <- \"y\"\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n\ndf <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n# Set up random resampling\n\nfor (i in 1:numresamples) {\n  \nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ny_train <- train$y\ny_test <- test$y\n\n# Fit the model to the training data, make predictions on the holdout data\n\nglm_train_fit <- stats::glm(y ~ ., data = train, family = binomial(link = \"logit\"))\n\nglm_train_pred <- stats::predict(glm_train_fit, train, type = \"response\")\nglm_train_predictions <- ifelse(glm_train_pred > 0.5, 1, 0)\nglm_train_table <- table(glm_train_predictions, y_train)\nglm_train_accuracy[i] <- (glm_train_table[1, 1] + glm_train_table[2, 2]) / sum(glm_train_table)\nglm_train_accuracy_mean <- mean(glm_train_accuracy)\n\nglm_test_pred <- stats::predict(glm_train_fit, test, type = \"response\")\nglm_test_predictions <- ifelse(glm_test_pred > 0.5, 1, 0)\nglm_test_table <- table(glm_test_predictions, y_test)\nglm_test_accuracy[i] <- (glm_test_table[1, 1] + glm_test_table[2, 2]) / sum(glm_test_table)\nglm_test_accuracy_mean <- mean(glm_test_accuracy)\n\nglm_holdout_accuracy_mean <- mean(glm_test_accuracy)\n\nglm_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(glm_test_pred))\nglm_auc <- round((pROC::auc(c(test$y), as.numeric(c(glm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(glm_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Generalized Linear Models \", \"(AUC = \", glm_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n\nreturn(glm_holdout_accuracy_mean)\n\n} # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nglm_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7575758\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"how-to-use-non-glm-models-in-logistic-analysis","chapter":"7 Individual logistic models","heading":"7.0.1 How to use non-GLM models in logistic analysis","text":"authors excellent book, Introduction Statistical Learning, describe demonstrate non-GLM methods may used logistic analysis. investigated Linear Discriminant Analysis, Quadratic Discriminant Analysis K-Nearest Neighbors. look total ten methods, though many possible.","code":""},{"path":"individual-logistic-models.html","id":"eight-individual-models-for-logistic-data","chapter":"7 Individual logistic models","heading":"7.0.2 Eight individual models for logistic data","text":"","code":""},{"path":"individual-logistic-models.html","id":"adaboost","chapter":"7 Individual logistic models","heading":"7.0.3 Adaboost","text":"","code":"\n\n# Load the library\nlibrary(MachineShop)\n#> \n#> Attaching package: 'MachineShop'\n#> The following object is masked from 'package:pROC':\n#> \n#>     auc\n#> The following object is masked from 'package:purrr':\n#> \n#>     lift\n#> The following object is masked from 'package:stats':\n#> \n#>     ppr\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nadaboost_train_accuracy <- 0\nadaboost_test_accuracy <- 0\nadaboost_holdout_accuracy <- 0\nadaboost_duration <- 0\nadaboost_table_total <- 0\n\n# Create the function\n\nadaboost_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nadaboost_train_fit <- MachineShop::fit(formula = as.factor(y) ~ ., data = train, model = \"AdaBoostModel\")\n\nadaboost_train_pred <- stats::predict(adaboost_train_fit, train, type = \"prob\")\nadaboost_train_predictions <- ifelse(adaboost_train_pred > 0.5, 1, 0)\nadaboost_train_table <- table(adaboost_train_predictions, y_train)\nadaboost_train_accuracy[i] <- (adaboost_train_table[1, 1] + adaboost_train_table[2, 2]) / sum(adaboost_train_table)\nadaboost_train_accuracy_mean <- mean(adaboost_train_accuracy)\n    \nadaboost_test_pred <- stats::predict(adaboost_train_fit, test, type = \"prob\")\nadaboost_test_predictions <- ifelse(adaboost_test_pred > 0.5, 1, 0)\nadaboost_test_table <- table(adaboost_test_predictions, y_test)\nadaboost_test_accuracy[i] <- (adaboost_test_table[1, 1] + adaboost_test_table[2, 2]) / sum(adaboost_test_table)\nadaboost_test_accuracy_mean <- mean(adaboost_test_accuracy)\n\nadaboost_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(adaboost_test_pred))\nadaboost_auc <- round((pROC::auc(c(test$y), as.numeric(c(adaboost_test_pred)) - 1)), 4)\nprint(pROC::ggroc(adaboost_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"ADAboost Models \", \"(AUC = \", adaboost_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(adaboost_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nadaboost_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.734375\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"bayesglm-1","chapter":"7 Individual logistic models","heading":"7.0.4 BayesGLM","text":"","code":"\n\n# Load the library\nlibrary(arm)\n#> Loading required package: MASS\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russellconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nbayesglm_train_accuracy <- 0\nbayesglm_test_accuracy <- 0\nbayesglm_holdout_accuracy <- 0\nbayesglm_duration <- 0\nbayesglm_table_total <- 0\n\n# Create the function\n\nbayesglm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = binomial)\n    \nbayesglm_train_pred <- stats::predict(bayesglm_train_fit, train, type = \"response\")\nbayesglm_train_predictions <- ifelse(bayesglm_train_pred > 0.5, 1, 0)\nbayesglm_train_table <- table(bayesglm_train_predictions, y_train)\nbayesglm_train_accuracy[i] <- (bayesglm_train_table[1, 1] + bayesglm_train_table[2, 2]) / sum(bayesglm_train_table)\nbayesglm_train_accuracy_mean <- mean(bayesglm_train_accuracy)\n\nbayesglm_test_pred <- stats::predict(bayesglm_train_fit, test, type = \"response\")\nbayesglm_test_predictions <- ifelse(bayesglm_test_pred > 0.5, 1, 0)\nbayesglm_test_table <- table(bayesglm_test_predictions, y_test)\n\nbayesglm_test_accuracy[i] <- (bayesglm_test_table[1, 1] + bayesglm_test_table[2, 2]) / sum(bayesglm_test_table)\nbayesglm_test_accuracy_mean <- mean(bayesglm_test_accuracy)\n\nbayesglm_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(bayesglm_test_pred))\nbayesglm_auc <- round((pROC::auc(c(test$y), as.numeric(c(bayesglm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(bayesglm_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"Bayesglm Models \", \"(AUC = \", bayesglm_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(bayesglm_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nbayesglm_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7272727\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"c50","chapter":"7 Individual logistic models","heading":"7.0.5 C50","text":"","code":"\n\n# Load the library\nlibrary(C50)\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_holdout_accuracy <- 0\nC50_duration <- 0\nC50_table_total <- 0\n\n# Create the function\n\nC50_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\n\nC50_train_pred <- stats::predict(C50_train_fit, train, type = \"prob\")\nC50_train_predictions <- ifelse(C50_train_pred[, 2] > 0.5, 1, 0)\nC50_train_table <- table(C50_train_predictions, y_train)\nC50_train_accuracy[i] <- (C50_train_table[1, 1] + C50_train_table[2, 2]) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\n\nC50_test_pred <- stats::predict(C50_train_fit, test, type = \"prob\")\nC50_test_predictions <- ifelse(C50_test_pred[, 2] > 0.5, 1, 0)\nC50_test_table <- table(C50_test_predictions, y_test)\nC50_test_accuracy[i] <- (C50_test_table[1, 1] + C50_test_table[2, 2]) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\n\nC50_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(C50_test_predictions)))\nC50_auc <- round((pROC::auc(c(test$y), as.numeric(c(C50_test_predictions)) - 1)), 4)\nprint(pROC::ggroc(C50_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"C50 ROC curve \", \"(AUC = \", C50_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(C50_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nC50_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 1\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"cubist-1","chapter":"7 Individual logistic models","heading":"7.0.6 Cubist","text":"","code":"\n\n# Load the library\nlibrary(Cubist)\n#> Loading required package: lattice\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\ncubist_train_accuracy <- 0\ncubist_test_accuracy <- 0\ncubist_holdout_accuracy <- 0\ncubist_duration <- 0\ncubist_table_total <- 0\n\n# Create the function\n\ncubist_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\ncubist_train_fit <- Cubist::cubist(x = as.data.frame(train), y = train$y)\n    \ncubist_train_pred <- stats::predict(cubist_train_fit, train, type = \"prob\")\ncubist_train_table <- table(cubist_train_pred, y_train)\ncubist_train_accuracy[i] <- (cubist_train_table[1, 1] + cubist_train_table[2, 2]) / sum(cubist_train_table)\ncubist_train_accuracy_mean <- mean(cubist_train_accuracy)\n\ncubist_test_pred <- stats::predict(cubist_train_fit, test, type = \"prob\")\ncubist_test_table <- table(cubist_test_pred, y_test)\ncubist_test_accuracy[i] <- (cubist_test_table[1, 1] + cubist_test_table[2, 2]) / sum(cubist_test_table)\ncubist_test_accuracy_mean <- mean(cubist_test_accuracy)\n\n\ncubist_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(cubist_test_pred)))\ncubist_auc <- round((pROC::auc(c(test$y), as.numeric(c(cubist_test_pred)) - 1)), 4)\nprint(pROC::ggroc(cubist_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Cubist ROC curve \", \"(AUC = \", cubist_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(cubist_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\ncubist_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 1\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"gradient-boosted-1","chapter":"7 Individual logistic models","heading":"7.0.7 Gradient Boosted","text":"","code":"\n\n# Load the library\nlibrary(gbm)\n#> Loaded gbm 2.1.9\n#> This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\ngb_train_accuracy <- 0\ngb_test_accuracy <- 0\ngb_holdout_accuracy <- 0\ngb_duration <- 0\ngb_table_total <- 0\n\n# Create the function\n\ngb_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\ngb_train_fit <- gbm::gbm(train$y ~ ., data = train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\n    \ngb_train_pred <- stats::predict(gb_train_fit, train, type = \"response\")\ngb_train_predictions <- ifelse(gb_train_pred > 0.5, 1, 0)\ngb_train_table <- table(gb_train_predictions, y_train)\ngb_train_accuracy[i] <- (gb_train_table[1, 1] + gb_train_table[2, 2]) / sum(gb_train_table)\ngb_train_accuracy_mean <- mean(gb_train_accuracy)\n\ngb_test_pred <- stats::predict(gb_train_fit, test, type = \"response\")\ngb_test_predictions <- ifelse(gb_test_pred > 0.5, 1, 0)\ngb_test_table <- table(gb_test_predictions, y_test)\ngb_test_accuracy[i] <- (gb_test_table[1, 1] + gb_test_table[2, 2]) / sum(gb_test_table)\ngb_test_accuracy_mean <- mean(gb_test_accuracy)\n\ngb_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(gb_test_pred)))\ngb_auc <- round((pROC::auc(c(test$y), as.numeric(c(gb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(gb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Gradient Boosted ROC curve \", \"(AUC = \", gb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(gb_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\ngb_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Using 100 trees...\n#> Using 100 trees...\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.759375\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"random-forest-2","chapter":"7 Individual logistic models","heading":"7.0.8 Random Forest","text":"","code":"\n\n# Load the library\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nrf_train_accuracy <- 0\nrf_test_accuracy <- 0\nrf_holdout_accuracy <- 0\nrf_duration <- 0\nrf_table_total <- 0\n\n# Create the function\n\nrf_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nrf_train_fit <- randomForest(x = train, y = as.factor(y_train), data = df)\n    \nrf_train_pred <- stats::predict(rf_train_fit, train, type = \"prob\")\nrf_train_probabilities <- ifelse(rf_train_pred > 0.50, 1, 0)[, 2]\nrf_train_table <- table(rf_train_probabilities, y_train)\nrf_train_accuracy[i] <- (rf_train_table[1, 1] + rf_train_table[2, 2]) / sum(rf_train_table)\nrf_train_accuracy_mean <- mean(rf_train_accuracy)\n\nrf_test_pred <- stats::predict(rf_train_fit, test, type = \"prob\")\nrf_test_probabilities <- ifelse(rf_test_pred > 0.50, 1, 0)[, 2]\nrf_test_table <- table(rf_test_probabilities, y_test)\nrf_test_accuracy[i] <- (rf_test_table[1, 1] + rf_test_table[2, 2]) / sum(rf_test_table)\nrf_test_accuracy_mean <- mean(rf_test_accuracy)\n\nrf_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(rf_test_probabilities)))\nrf_auc <- round((pROC::auc(c(test$y), as.numeric(c(rf_test_probabilities)) - 1)), 4)\nprint(pROC::ggroc(rf_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Random Forest \", \"(AUC = \", rf_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(rf_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nrf_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 1\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"support-vector-machines-2","chapter":"7 Individual logistic models","heading":"7.0.9 Support Vector Machines","text":"","code":"\n\n# Load the library\nlibrary(e1071)\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nsvm_train_accuracy <- 0\nsvm_test_accuracy <- 0\nsvm_holdout_accuracy <- 0\nsvm_duration <- 0\nsvm_table_total <- 0\n\n# Create the function\n\nsvm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nsvm_train_fit <- e1071::svm(as.factor(y) ~ ., data = train)\n    \nsvm_train_pred <- stats::predict(svm_train_fit, train, type = \"prob\")\nsvm_train_table <- table(svm_train_pred, y_train)\nsvm_train_accuracy[i] <- (svm_train_table[1, 1] + svm_train_table[2, 2]) / sum(svm_train_table)\nsvm_train_accuracy_mean <- mean(svm_train_accuracy)\n\nsvm_test_pred <- stats::predict(svm_train_fit, test, type = \"prob\")\nsvm_test_table <- table(svm_test_pred, y_test)\nsvm_test_accuracy[i] <- (svm_test_table[1, 1] + svm_test_table[2, 2]) / sum(svm_test_table)\nsvm_test_accuracy_mean <- mean(svm_test_accuracy)\n\nsvm_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(svm_test_pred)))\nsvm_auc <- round((pROC::auc(c(test$y), as.numeric(c(svm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(svm_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Random Forest \", \"(AUC = \", svm_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(svm_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nsvm_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7378049\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"xgboost-2","chapter":"7 Individual logistic models","heading":"7.0.10 XGBoost","text":"","code":"\n\n# Load the library\nlibrary(xgboost)\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_holdout_accuracy <- 0\nxgb_duration <- 0\nxgb_table_total <- 0\n\n# Create the function\n\nxgb_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n# Fit the model to the training data, make predictions on the holdout data\ntrain_x <- data.matrix(train[, -ncol(train)])\ntrain_y <- train[, ncol(train)]\n    \n# define predictor and response variables in test set\ntest_x <- data.matrix(test[, -ncol(test)])\ntest_y <- test[, ncol(test)]\n    \n# define final train and test sets\nxgb_train <- xgboost::xgb.DMatrix(data = train_x, label = train_y)\nxgb_test <- xgboost::xgb.DMatrix(data = test_x, label = test_y)\n\n# define watchlist\nwatchlist <- list(train = xgb_train)\nwatchlist_test <- list(train = xgb_train, test = xgb_test)\n\nxgb_model <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_test, nrounds = 70)\n    \nxgb_min <- which.min(xgb_model$evaluation_log$validation_rmse)\n    \nxgb_train_pred <- stats::predict(object = xgb_model, newdata = train_x, type = \"prob\")\nxgb_train_predictions <- ifelse(xgb_train_pred > 0.5, 1, 0)\nxgb_train_table <- table(xgb_train_predictions, y_train)\nxgb_train_accuracy[i] <- (xgb_train_table[1, 1] + xgb_train_table[2, 2]) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\nxgb_test_pred <- stats::predict(object = xgb_model, newdata = test_x, type = \"prob\")\nxgb_test_predictions <- ifelse(xgb_test_pred > 0.5, 1, 0)\nxgb_test_table <- table(xgb_test_predictions, y_test)\nxgb_test_accuracy[i] <- (xgb_test_table[1, 1] + xgb_test_table[2, 2]) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\nxgb_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(xgb_test_pred)))\nxgb_auc <- round((pROC::auc(c(test$y), as.numeric(c(xgb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(xgb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"XGBoost \", \"(AUC = \", xgb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(xgb_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nxgb_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> [1]  train-rmse:0.440691 test-rmse:0.468826 \n#> [2]  train-rmse:0.401963 test-rmse:0.444282 \n#> [3]  train-rmse:0.377513 test-rmse:0.431343 \n#> [4]  train-rmse:0.360344 test-rmse:0.422402 \n#> [5]  train-rmse:0.347503 test-rmse:0.419174 \n#> [6]  train-rmse:0.337063 test-rmse:0.417823 \n#> [7]  train-rmse:0.331359 test-rmse:0.416086 \n#> [8]  train-rmse:0.324571 test-rmse:0.416389 \n#> [9]  train-rmse:0.319701 test-rmse:0.415202 \n#> [10] train-rmse:0.317618 test-rmse:0.416348 \n#> [11] train-rmse:0.314221 test-rmse:0.415195 \n#> [12] train-rmse:0.308243 test-rmse:0.415841 \n#> [13] train-rmse:0.305834 test-rmse:0.414648 \n#> [14] train-rmse:0.299743 test-rmse:0.412698 \n#> [15] train-rmse:0.297681 test-rmse:0.411570 \n#> [16] train-rmse:0.295436 test-rmse:0.412985 \n#> [17] train-rmse:0.293643 test-rmse:0.411905 \n#> [18] train-rmse:0.291810 test-rmse:0.410634 \n#> [19] train-rmse:0.289988 test-rmse:0.410275 \n#> [20] train-rmse:0.287036 test-rmse:0.409675 \n#> [21] train-rmse:0.282526 test-rmse:0.408400 \n#> [22] train-rmse:0.280909 test-rmse:0.407913 \n#> [23] train-rmse:0.279729 test-rmse:0.407869 \n#> [24] train-rmse:0.277645 test-rmse:0.408945 \n#> [25] train-rmse:0.276890 test-rmse:0.409450 \n#> [26] train-rmse:0.273924 test-rmse:0.410037 \n#> [27] train-rmse:0.272506 test-rmse:0.409242 \n#> [28] train-rmse:0.270840 test-rmse:0.408662 \n#> [29] train-rmse:0.265381 test-rmse:0.409974 \n#> [30] train-rmse:0.260491 test-rmse:0.410605 \n#> [31] train-rmse:0.259413 test-rmse:0.411127 \n#> [32] train-rmse:0.258426 test-rmse:0.410396 \n#> [33] train-rmse:0.255586 test-rmse:0.412012 \n#> [34] train-rmse:0.251211 test-rmse:0.413926 \n#> [35] train-rmse:0.248288 test-rmse:0.414990 \n#> [36] train-rmse:0.244570 test-rmse:0.414701 \n#> [37] train-rmse:0.241985 test-rmse:0.414301 \n#> [38] train-rmse:0.241430 test-rmse:0.414539 \n#> [39] train-rmse:0.240695 test-rmse:0.414005 \n#> [40] train-rmse:0.240025 test-rmse:0.414341 \n#> [41] train-rmse:0.237113 test-rmse:0.415980 \n#> [42] train-rmse:0.234688 test-rmse:0.416899 \n#> [43] train-rmse:0.233183 test-rmse:0.416014 \n#> [44] train-rmse:0.230134 test-rmse:0.415525 \n#> [45] train-rmse:0.228345 test-rmse:0.417350 \n#> [46] train-rmse:0.226003 test-rmse:0.418268 \n#> [47] train-rmse:0.225099 test-rmse:0.418754 \n#> [48] train-rmse:0.223356 test-rmse:0.420382 \n#> [49] train-rmse:0.220902 test-rmse:0.420596 \n#> [50] train-rmse:0.220133 test-rmse:0.420513 \n#> [51] train-rmse:0.219022 test-rmse:0.419994 \n#> [52] train-rmse:0.218599 test-rmse:0.420128 \n#> [53] train-rmse:0.216922 test-rmse:0.420728 \n#> [54] train-rmse:0.216015 test-rmse:0.421179 \n#> [55] train-rmse:0.213153 test-rmse:0.421983 \n#> [56] train-rmse:0.210912 test-rmse:0.421714 \n#> [57] train-rmse:0.208177 test-rmse:0.422777 \n#> [58] train-rmse:0.206390 test-rmse:0.421708 \n#> [59] train-rmse:0.206141 test-rmse:0.422034 \n#> [60] train-rmse:0.204628 test-rmse:0.422521 \n#> [61] train-rmse:0.201648 test-rmse:0.423330 \n#> [62] train-rmse:0.199137 test-rmse:0.423374 \n#> [63] train-rmse:0.196867 test-rmse:0.424406 \n#> [64] train-rmse:0.193889 test-rmse:0.424663 \n#> [65] train-rmse:0.191601 test-rmse:0.425438 \n#> [66] train-rmse:0.189102 test-rmse:0.426845 \n#> [67] train-rmse:0.186894 test-rmse:0.427738 \n#> [68] train-rmse:0.184002 test-rmse:0.428882 \n#> [69] train-rmse:0.182116 test-rmse:0.428273 \n#> [70] train-rmse:0.180305 test-rmse:0.428459\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7475728\n\n# Check for any errors\nwarnings()"},{"path":"advice-to-lebron-james-and-everyone-who-does-talent-analytics-logistic-ensembles.html","id":"advice-to-lebron-james-and-everyone-who-does-talent-analytics-logistic-ensembles","chapter":"8 Advice to Lebron James (and everyone who does talent analytics): Logistic ensembles","heading":"8 Advice to Lebron James (and everyone who does talent analytics): Logistic ensembles","text":"section ’re going take lessons previous chapter move making ensembles models. process extremely similar, follows steps:Load librarySet initial values 0Create functionSet random resamplingBreak data train testFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setLogistic ensembles can used extremely wide range fields. Previously modeled diabetes Pima Indian women. chapter’s example performance court Lebron James.’s image Lebron play.LOT data sports HR analytics (extremely similar ways). lot data set logistic data. example, data set performance Lebron James. main column interest “result”, either 1 0. Thus perfectly fits requirements logistic analysis.Let’s look structure data:see numbers. might easier “qtr” “opponent” changed factors, ’ll first.Now ’re ready create ensemble models make predictions Lebron’s future performance. new skill chapter saving trained models Environment. allow us look trained models, use make strongest evidence based recommendations.use following individual models ensemble models:Individual models:AdaBoostBayesGLMC50CubistGeneralized Linear Models (GLM)Random ForestXGBoostWe make ensemble predictions five models, use ensemble model predictions Lebron’s performance.also show ROC curves results, save trained models end.","code":"\n\nlibrary(Ensembles)\n#> Loading required package: arm\n#> Loading required package: MASS\n#> Loading required package: Matrix\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russellconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\n#> Loading required package: brnn\n#> Loading required package: Formula\n#> Loading required package: truncnorm\n#> Loading required package: broom\n#> Loading required package: C50\n#> Loading required package: caret\n#> Loading required package: ggplot2\n#> Loading required package: lattice\n#> Loading required package: class\n#> Loading required package: corrplot\n#> corrplot 0.92 loaded\n#> \n#> Attaching package: 'corrplot'\n#> The following object is masked from 'package:arm':\n#> \n#>     corrplot\n#> Loading required package: Cubist\n#> Loading required package: doParallel\n#> Loading required package: foreach\n#> Loading required package: iterators\n#> Loading required package: parallel\n#> Loading required package: dplyr\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:MASS':\n#> \n#>     select\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n#> Loading required package: e1071\n#> Loading required package: fable\n#> Loading required package: fabletools\n#> Registered S3 method overwritten by 'tsibble':\n#>   method          from\n#>   format.interval inum\n#> \n#> Attaching package: 'fabletools'\n#> The following object is masked from 'package:e1071':\n#> \n#>     interpolate\n#> The following objects are masked from 'package:caret':\n#> \n#>     MAE, RMSE\n#> The following object is masked from 'package:lme4':\n#> \n#>     refit\n#> Loading required package: fable.prophet\n#> Loading required package: Rcpp\n#> Loading required package: feasts\n#> Loading required package: gam\n#> Loading required package: splines\n#> Loaded gam 1.22-3\n#> Loading required package: gbm\n#> Loaded gbm 2.1.9\n#> This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n#> Loading required package: GGally\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n#> Loading required package: glmnet\n#> Loaded glmnet 4.1-8\n#> Loading required package: gridExtra\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> Loading required package: gt\n#> Loading required package: gtExtras\n#> \n#> Attaching package: 'gtExtras'\n#> The following object is masked from 'package:MASS':\n#> \n#>     select\n#> Loading required package: ipred\n#> Loading required package: kernlab\n#> \n#> Attaching package: 'kernlab'\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     alpha\n#> Loading required package: klaR\n#> Loading required package: leaps\n#> Loading required package: MachineShop\n#> \n#> Attaching package: 'MachineShop'\n#> The following objects are masked from 'package:fabletools':\n#> \n#>     accuracy, response\n#> The following objects are masked from 'package:caret':\n#> \n#>     calibration, lift, precision, recall, rfe,\n#>     sensitivity, specificity\n#> The following object is masked from 'package:stats':\n#> \n#>     ppr\n#> Loading required package: magrittr\n#> Loading required package: mda\n#> Loaded mda 0.5-4\n#> \n#> Attaching package: 'mda'\n#> The following object is masked from 'package:MachineShop':\n#> \n#>     confusion\n#> Loading required package: Metrics\n#> \n#> Attaching package: 'Metrics'\n#> The following objects are masked from 'package:MachineShop':\n#> \n#>     accuracy, auc, mae, mse, msle, precision, recall,\n#>     rmse, rmsle\n#> The following object is masked from 'package:fabletools':\n#> \n#>     accuracy\n#> The following objects are masked from 'package:caret':\n#> \n#>     precision, recall\n#> Loading required package: neuralnet\n#> \n#> Attaching package: 'neuralnet'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     compute\n#> Loading required package: pls\n#> \n#> Attaching package: 'pls'\n#> The following object is masked from 'package:corrplot':\n#> \n#>     corrplot\n#> The following object is masked from 'package:caret':\n#> \n#>     R2\n#> The following objects are masked from 'package:arm':\n#> \n#>     coefplot, corrplot\n#> The following object is masked from 'package:stats':\n#> \n#>     loadings\n#> Loading required package: pROC\n#> Type 'citation(\"pROC\")' for a citation.\n#> \n#> Attaching package: 'pROC'\n#> The following object is masked from 'package:Metrics':\n#> \n#>     auc\n#> The following object is masked from 'package:MachineShop':\n#> \n#>     auc\n#> The following objects are masked from 'package:stats':\n#> \n#>     cov, smooth, var\n#> Loading required package: purrr\n#> \n#> Attaching package: 'purrr'\n#> The following object is masked from 'package:magrittr':\n#> \n#>     set_names\n#> The following object is masked from 'package:MachineShop':\n#> \n#>     lift\n#> The following object is masked from 'package:kernlab':\n#> \n#>     cross\n#> The following objects are masked from 'package:foreach':\n#> \n#>     accumulate, when\n#> The following object is masked from 'package:caret':\n#> \n#>     lift\n#> Loading required package: randomForest\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:gridExtra':\n#> \n#>     combine\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\n#> Loading required package: reactable\n#> Loading required package: reactablefmtr\n#> \n#> Attaching package: 'reactablefmtr'\n#> The following object is masked from 'package:randomForest':\n#> \n#>     margin\n#> The following objects are masked from 'package:gt':\n#> \n#>     google_font, html\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\n#> Loading required package: readr\n#> Loading required package: rpart\n#> Loading required package: scales\n#> \n#> Attaching package: 'scales'\n#> The following object is masked from 'package:readr':\n#> \n#>     col_factor\n#> The following object is masked from 'package:purrr':\n#> \n#>     discard\n#> The following object is masked from 'package:kernlab':\n#> \n#>     alpha\n#> The following object is masked from 'package:arm':\n#> \n#>     rescale\n#> Loading required package: tibble\n#> Loading required package: tidyr\n#> \n#> Attaching package: 'tidyr'\n#> The following object is masked from 'package:magrittr':\n#> \n#>     extract\n#> The following objects are masked from 'package:Matrix':\n#> \n#>     expand, pack, unpack\n#> Loading required package: tree\n#> Loading required package: tsibble\n#> \n#> Attaching package: 'tsibble'\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, union\n#> Loading required package: xgboost\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\nhead(lebron, n = 20)\n#>    top left  date qtr time_remaining result shot_type\n#> 1  310  203 19283   2            566      0         3\n#> 2  213  259 19283   2            518      0         2\n#> 3  143  171 19283   2            490      0         2\n#> 4   68  215 19283   2            324      1         2\n#> 5   66  470 19283   2             62      0         3\n#> 6   63  239 19283   4            690      1         2\n#> 7  230   54 19283   4            630      0         3\n#> 8   53  224 19283   4            605      1         2\n#> 9  241   67 19283   4            570      0         3\n#> 10 273  113 19283   4            535      0         3\n#> 11  62  224 19283   4            426      0         2\n#> 12  63  249 19283   4            233      1         2\n#> 13 103  236 19283   4            154      0         2\n#> 14  54  249 19283   4            108      1         2\n#> 15  53  240 19283   4             58      0         2\n#> 16 230   71 19283   5            649      1         3\n#> 17 231  358 19283   5            540      0         2\n#> 18  61  240 19283   5            524      1         2\n#> 19  59  235 19283   5             71      1         2\n#> 20 299  188 19283   5              6      1         3\n#>    distance_ft lead lebron_team_score opponent_team_score\n#> 1           26    0                 2                   2\n#> 2           16    0                 4                   5\n#> 3           11    0                 4                   7\n#> 4            3    0                12                  19\n#> 5           23    0                22                  23\n#> 6            1    0                24                  25\n#> 7           26    0                24                  27\n#> 8            2    0                26                  27\n#> 9           26    0                26                  29\n#> 10          25    0                26                  32\n#> 11           2    0                31                  39\n#> 12           1    0                39                  49\n#> 13           5    0                39                  51\n#> 14           1    0                44                  53\n#> 15           0    0                46                  55\n#> 16          25    0                58                  63\n#> 17          21    0                60                  70\n#> 18           1    0                62                  70\n#> 19           1    0                68                  91\n#> 20          25    0                71                  91\n#>    opponent\n#> 1         9\n#> 2         9\n#> 3         9\n#> 4         9\n#> 5         9\n#> 6         9\n#> 7         9\n#> 8         9\n#> 9         9\n#> 10        9\n#> 11        9\n#> 12        9\n#> 13        9\n#> 14        9\n#> 15        9\n#> 16        9\n#> 17        9\n#> 18        9\n#> 19        9\n#> 20        9\nlebron <- Ensembles::lebron\nstr(Ensembles::lebron)\n#> 'data.frame':    1533 obs. of  12 variables:\n#>  $ top                : int  310 213 143 68 66 63 230 53 241 273 ...\n#>  $ left               : int  203 259 171 215 470 239 54 224 67 113 ...\n#>  $ date               : num  19283 19283 19283 19283 19283 ...\n#>  $ qtr                : num  2 2 2 2 2 4 4 4 4 4 ...\n#>  $ time_remaining     : num  566 518 490 324 62 690 630 605 570 535 ...\n#>  $ result             : num  0 0 0 1 0 1 0 1 0 0 ...\n#>  $ shot_type          : int  3 2 2 2 3 2 3 2 3 3 ...\n#>  $ distance_ft        : int  26 16 11 3 23 1 26 2 26 25 ...\n#>  $ lead               : num  0 0 0 0 0 0 0 0 0 0 ...\n#>  $ lebron_team_score  : int  2 4 4 12 22 24 24 26 26 26 ...\n#>  $ opponent_team_score: int  2 5 7 19 23 25 27 27 29 32 ...\n#>  $ opponent           : num  9 9 9 9 9 9 9 9 9 9 ...\n\nlebron$qtr <- as.factor(lebron$qtr)\nlebron$opponent <- as.factor(lebron$opponent)\n\n# Load libraries - note these will work with individual and ensemble models\nlibrary(arm) # to use with BayesGLM\nlibrary(C50) # To use with C50\nlibrary(Cubist) # To use with Cubist modeling\nlibrary(MachineShop)# To use with ADABoost\nlibrary(randomForest) # Random Forest models\nlibrary(tidyverse) # My favorite tool for data science!\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ lubridate 1.9.3     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ purrr::accumulate()     masks foreach::accumulate()\n#> ✖ scales::alpha()         masks kernlab::alpha(), ggplot2::alpha()\n#> ✖ scales::col_factor()    masks readr::col_factor()\n#> ✖ randomForest::combine() masks gridExtra::combine(), dplyr::combine()\n#> ✖ neuralnet::compute()    masks dplyr::compute()\n#> ✖ purrr::cross()          masks kernlab::cross()\n#> ✖ scales::discard()       masks purrr::discard()\n#> ✖ tidyr::expand()         masks Matrix::expand()\n#> ✖ tidyr::extract()        masks magrittr::extract()\n#> ✖ dplyr::filter()         masks stats::filter()\n#> ✖ lubridate::interval()   masks tsibble::interval()\n#> ✖ dplyr::lag()            masks stats::lag()\n#> ✖ purrr::lift()           masks MachineShop::lift(), caret::lift()\n#> ✖ reactablefmtr::margin() masks randomForest::margin(), ggplot2::margin()\n#> ✖ tidyr::pack()           masks Matrix::pack()\n#> ✖ gtExtras::select()      masks dplyr::select(), MASS::select()\n#> ✖ purrr::set_names()      masks magrittr::set_names()\n#> ✖ xgboost::slice()        masks dplyr::slice()\n#> ✖ tidyr::unpack()         masks Matrix::unpack()\n#> ✖ purrr::when()           masks foreach::when()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(pROC) # To print ROC curves\n\n# Set initial values to 0\nadaboost_train_accuracy <- 0\nadaboost_test_accuracy <- 0\nadaboost_holdout_accuracy <- 0\nadaboost_duration <- 0\nadaboost_table_total <- 0\n\nbayesglm_train_accuracy <- 0\nbayesglm_test_accuracy <- 0\nbayesglm_holdout_accuracy <- 0\nbayesglm_duration <- 0\nbayesglm_table_total <- 0\n\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_holdout_accuracy <- 0\nC50_duration <- 0\nC50_table_total <- 0\n\ncubist_train_accuracy <- 0\ncubist_test_accuracy <- 0\ncubist_holdout_accuracy <- 0\ncubist_duration <- 0\ncubist_table_total <- 0\n\nrf_train_accuracy <- 0\nrf_test_accuracy <- 0\nrf_holdout_accuracy <- 0\nrf_duration <- 0\nrf_table_total <- 0\n\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_holdout_accuracy <- 0\nxgb_duration <- 0\nxgb_table_total <- 0\n\n\nensemble_adaboost_train_accuracy <- 0\nensemble_adaboost_test_accuracy <- 0\nensemble_adaboost_holdout_accuracy <- 0\nensemble_adaboost_duration <- 0\nensemble_adaboost_table_total <- 0\nensemble_adaboost_train_pred <- 0\n\nensemble_bayesglm_train_accuracy <- 0\nensemble_bayesglm_test_accuracy <- 0\nensemble_bayesglm_holdout_accuracy <- 0\nensemble_bayesglm_duration <- 0\nensemble_bayesglm_table_total <- 0\n\nensemble_C50_train_accuracy <- 0\nensemble_C50_test_accuracy <- 0\nensemble_C50_holdout_accuracy <- 0\nensemble_C50_duration <- 0\nensemble_C50_table_total <- 0\n\nensemble_rf_train_accuracy <- 0\nensemble_rf_test_accuracy <- 0\nensemble_rf_holdout_accuracy <- 0\nensemble_rf_duration <- 0\nensemble_rf_table_total <- 0\n\nensemble_xgb_train_accuracy <- 0\nensemble_xgb_test_accuracy <- 0\nensemble_xgb_holdout_accuracy <- 0\nensemble_xgb_duration <- 0\nensemble_xgb_table_total <- 0\n\n# Create the function\n\nlogistic_1 <- function(data, colnum, numresamples, train_amount, test_amount){\ncolnames(data)[colnum] <- \"y\"\n  \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \ndf <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n# Set up random resampling\n  \nfor (i in 1:numresamples) {\n    \nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n    \ny_train <- train$y\ny_test <- test$y\n  \n  \n# ADABoost model\nadaboost_train_fit <- MachineShop::fit(formula = as.factor(y) ~ ., data = train, model = \"AdaBoostModel\")\n\nadaboost_train_pred <- stats::predict(adaboost_train_fit, train, type = \"prob\")\nadaboost_train_predictions <- ifelse(adaboost_train_pred > 0.5, 1, 0)\nadaboost_train_table <- table(adaboost_train_predictions, y_train)\nadaboost_train_accuracy[i] <- (adaboost_train_table[1, 1] + adaboost_train_table[2, 2]) / sum(adaboost_train_table)\nadaboost_train_accuracy_mean <- mean(adaboost_train_accuracy)\n\nadaboost_test_pred <- stats::predict(adaboost_train_fit, test, type = \"prob\")\nadaboost_test_predictions <- ifelse(adaboost_test_pred > 0.5, 1, 0)\nadaboost_test_table <- table(adaboost_test_predictions, y_test)\nadaboost_test_accuracy[i] <- (adaboost_test_table[1, 1] + adaboost_test_table[2, 2]) / sum(adaboost_test_table)\nadaboost_test_accuracy_mean <- mean(adaboost_test_accuracy)\n\nadaboost_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(adaboost_test_pred))\nadaboost_auc <- round((pROC::auc(c(test$y), as.numeric(c(adaboost_test_pred)) - 1)), 4)\nprint(pROC::ggroc(adaboost_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"ADAboost Models \", \"(AUC = \", adaboost_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n\n# BayesGLM\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = binomial)\n    \nbayesglm_train_pred <- stats::predict(bayesglm_train_fit, train, type = \"response\")\nbayesglm_train_predictions <- ifelse(bayesglm_train_pred > 0.5, 1, 0)\nbayesglm_train_table <- table(bayesglm_train_predictions, y_train)\nbayesglm_train_accuracy[i] <- (bayesglm_train_table[1, 1] + bayesglm_train_table[2, 2]) / sum(bayesglm_train_table)\nbayesglm_train_accuracy_mean <- mean(bayesglm_train_accuracy)\n\nbayesglm_test_pred <- stats::predict(bayesglm_train_fit, test, type = \"response\")\nbayesglm_test_predictions <- ifelse(bayesglm_test_pred > 0.5, 1, 0)\nbayesglm_test_table <- table(bayesglm_test_predictions, y_test)\n\nbayesglm_test_accuracy[i] <- (bayesglm_test_table[1, 1] + bayesglm_test_table[2, 2]) / sum(bayesglm_test_table)\nbayesglm_test_accuracy_mean <- mean(bayesglm_test_accuracy)\n\nbayesglm_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(bayesglm_test_pred))\nbayesglm_auc <- round((pROC::auc(c(test$y), as.numeric(c(bayesglm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(bayesglm_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"Bayesglm Models \", \"(AUC = \", bayesglm_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# C50 model\n\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\n\nC50_train_pred <- stats::predict(C50_train_fit, train, type = \"prob\")\nC50_train_predictions <- ifelse(C50_train_pred[, 2] > 0.5, 1, 0)\nC50_train_table <- table(C50_train_predictions, y_train)\nC50_train_accuracy[i] <- (C50_train_table[1, 1] + C50_train_table[2, 2]) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\n\nC50_test_pred <- stats::predict(C50_train_fit, test, type = \"prob\")\nC50_test_predictions <- ifelse(C50_test_pred[, 2] > 0.5, 1, 0)\nC50_test_table <- table(C50_test_predictions, y_test)\nC50_test_accuracy[i] <- (C50_test_table[1, 1] + C50_test_table[2, 2]) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\n\nC50_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(C50_test_predictions)))\nC50_auc <- round((pROC::auc(c(test$y), as.numeric(c(C50_test_predictions)) - 1)), 4)\nprint(pROC::ggroc(C50_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"C50 ROC curve \", \"(AUC = \", C50_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# Cubist\ncubist_train_fit <- Cubist::cubist(x = as.data.frame(train), y = train$y)\n    \ncubist_train_pred <- stats::predict(cubist_train_fit, train, type = \"prob\")\ncubist_train_table <- table(cubist_train_pred, y_train)\ncubist_train_accuracy[i] <- (cubist_train_table[1, 1] + cubist_train_table[2, 2]) / sum(cubist_train_table)\ncubist_train_accuracy_mean <- mean(cubist_train_accuracy)\n\ncubist_test_pred <- stats::predict(cubist_train_fit, test, type = \"prob\")\ncubist_test_table <- table(cubist_test_pred, y_test)\ncubist_test_accuracy[i] <- (cubist_test_table[1, 1] + cubist_test_table[2, 2]) / sum(cubist_test_table)\ncubist_test_accuracy_mean <- mean(cubist_test_accuracy)\n\n\ncubist_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(cubist_test_pred)))\ncubist_auc <- round((pROC::auc(c(test$y), as.numeric(c(cubist_test_pred)) - 1)), 4)\nprint(pROC::ggroc(cubist_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Cubist ROC curve \", \"(AUC = \", cubist_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# Random Forest\nrf_train_fit <- randomForest(x = train, y = as.factor(y_train), data = df)\n    \nrf_train_pred <- stats::predict(rf_train_fit, train, type = \"prob\")\nrf_train_probabilities <- ifelse(rf_train_pred > 0.50, 1, 0)[, 2]\nrf_train_table <- table(rf_train_probabilities, y_train)\nrf_train_accuracy[i] <- (rf_train_table[1, 1] + rf_train_table[2, 2]) / sum(rf_train_table)\nrf_train_accuracy_mean <- mean(rf_train_accuracy)\n\nrf_test_pred <- stats::predict(rf_train_fit, test, type = \"prob\")\nrf_test_probabilities <- ifelse(rf_test_pred > 0.50, 1, 0)[, 2]\nrf_test_table <- table(rf_test_probabilities, y_test)\nrf_test_accuracy[i] <- (rf_test_table[1, 1] + rf_test_table[2, 2]) / sum(rf_test_table)\nrf_test_accuracy_mean <- mean(rf_test_accuracy)\n\nrf_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(rf_test_probabilities)))\nrf_auc <- round((pROC::auc(c(test$y), as.numeric(c(rf_test_probabilities)) - 1)), 4)\nprint(pROC::ggroc(rf_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Random Forest \", \"(AUC = \", rf_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# XGBoost\ntrain_x <- data.matrix(train[, -ncol(train)])\ntrain_y <- train[, ncol(train)]\n    \n# define predictor and response variables in test set\ntest_x <- data.matrix(test[, -ncol(test)])\ntest_y <- test[, ncol(test)]\n    \n# define final train and test sets\nxgb_train <- xgboost::xgb.DMatrix(data = train_x, label = train_y)\nxgb_test <- xgboost::xgb.DMatrix(data = test_x, label = test_y)\n\n# define watchlist\nwatchlist <- list(train = xgb_train)\nwatchlist_test <- list(train = xgb_train, test = xgb_test)\n\nxgb_model <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_test, nrounds = 70)\n    \nxgb_min <- which.min(xgb_model$evaluation_log$validation_rmse)\n    \nxgb_train_pred <- stats::predict(object = xgb_model, newdata = train_x, type = \"prob\")\nxgb_train_predictions <- ifelse(xgb_train_pred > 0.5, 1, 0)\nxgb_train_table <- table(xgb_train_predictions, y_train)\nxgb_train_accuracy[i] <- (xgb_train_table[1, 1] + xgb_train_table[2, 2]) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\nxgb_test_pred <- stats::predict(object = xgb_model, newdata = test_x, type = \"prob\")\nxgb_test_predictions <- ifelse(xgb_test_pred > 0.5, 1, 0)\nxgb_test_table <- table(xgb_test_predictions, y_test)\nxgb_test_accuracy[i] <- (xgb_test_table[1, 1] + xgb_test_table[2, 2]) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\nxgb_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(xgb_test_pred)))\nxgb_auc <- round((pROC::auc(c(test$y), as.numeric(c(xgb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(xgb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"XGBoost \", \"(AUC = \", xgb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# Ensemble\n\nensemble1 <- data.frame(\n  'ADABoost' = adaboost_test_predictions,\n  'BayesGLM'= bayesglm_test_predictions,\n  'C50' = C50_test_predictions,\n  'Cubist' = cubist_test_pred,\n  'Random_Forest' = rf_test_pred,\n  'XGBoost' = xgb_test_predictions,\n  'y' = test$y\n)\n\nensemble_index <- sample(c(1:2), nrow(ensemble1), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble1[ensemble_index == 1, ]\nensemble_test <- ensemble1[ensemble_index == 2, ]\nensemble_y_train <- ensemble_train$y\nensemble_y_test <- ensemble_test$y\n\n# Ensemble ADABoost\nensemble_adaboost_train_fit <- MachineShop::fit(as.factor(y) ~ ., data = ensemble_train, model = \"AdaBoostModel\")\n    \nensemble_adaboost_train_pred <- stats::predict(ensemble_adaboost_train_fit, ensemble_train, type = \"prob\")\nensemble_adaboost_train_probabilities <- ifelse(ensemble_adaboost_train_pred > 0.5, 1, 0)\nensemble_adaboost_train_table <- table(ensemble_adaboost_train_probabilities, ensemble_y_train)\nensemble_adaboost_train_accuracy[i] <- (ensemble_adaboost_train_table[1, 1] + ensemble_adaboost_train_table[2, 2]) / sum(ensemble_adaboost_train_table)\nensemble_adaboost_train_accuracy_mean <- mean(ensemble_adaboost_train_accuracy)\n    \nensemble_adaboost_test_pred <- stats::predict(ensemble_adaboost_train_fit, ensemble_test, type = \"prob\")\nensemble_adaboost_test_probabilities <- ifelse(ensemble_adaboost_test_pred > 0.5, 1, 0)\nensemble_adaboost_test_table <- table(ensemble_adaboost_test_probabilities, ensemble_y_test)\nensemble_adaboost_test_accuracy[i] <- (ensemble_adaboost_test_table[1, 1] + ensemble_adaboost_test_table[2, 2]) / sum(ensemble_adaboost_test_table)\nensemble_adaboost_test_accuracy_mean <- mean(ensemble_adaboost_test_accuracy)\n    \nensemble_adaboost_holdout_accuracy_mean <- mean(ensemble_adaboost_test_accuracy)\n    \nensemble_adaboost_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_adaboost_test_pred)))\nensemble_adaboost_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_adaboost_test_pred)) - 1)), 4)\nprint(pROC::ggroc(ensemble_adaboost_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"Ensemble Adaboostoost \", \"(AUC = \", ensemble_adaboost_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n\n# Ensembles using C50\nensemble_C50_train_fit <- C50::C5.0(as.factor(ensemble_y_train) ~ ., data = ensemble_train)\n\nensemble_C50_train_pred <- stats::predict(ensemble_C50_train_fit, ensemble_train, type = \"prob\")\nensemble_C50_train_probabilities <- ifelse(ensemble_C50_train_pred[, 2] > 0.5, 1, 0)\nensemble_C50_train_table <- table(ensemble_C50_train_probabilities, ensemble_y_train)\nensemble_C50_train_accuracy[i] <- (ensemble_C50_train_table[1, 1] + ensemble_C50_train_table[2, 2]) / sum(ensemble_C50_train_table)\nensemble_C50_train_accuracy_mean <- mean(ensemble_C50_train_accuracy)\n\nensemble_C50_test_pred <- stats::predict(ensemble_C50_train_fit, ensemble_test, type = \"prob\")\nensemble_C50_test_probabilities <- ifelse(ensemble_C50_test_pred[, 2] > 0.5, 1, 0)\nensemble_C50_test_table <- table(ensemble_C50_test_probabilities, ensemble_y_test)\nensemble_C50_test_accuracy[i] <- (ensemble_C50_test_table[1, 1] + ensemble_C50_test_table[2, 2]) / sum(ensemble_C50_test_table)\nensemble_C50_test_accuracy_mean <- mean(ensemble_C50_test_accuracy)\n\nensemble_C50_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_C50_test_pred[, 2])))\nensemble_C50_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_C50_test_pred[, 2])) - 1)), 4)\nprint(pROC::ggroc(ensemble_C50_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Ensemble_C50 \", \"(AUC = \", ensemble_C50_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n\n# Ensemble Random Forest\n\nensemble_rf_train_fit <- randomForest(x = ensemble_train, y = as.factor(ensemble_y_train), data = ensemble1)\n\nensemble_rf_train_pred <- stats::predict(ensemble_rf_train_fit, ensemble_train, type = \"prob\")\nensemble_rf_train_predictions <- ifelse(ensemble_rf_train_pred > 0.50, 1, 0)[, 2]\nensemble_rf_train_table <- table(ensemble_rf_train_predictions, ensemble_y_train)\nensemble_rf_train_accuracy[i] <- (ensemble_rf_train_table[1, 1] + ensemble_rf_train_table[2, 2]) / sum(ensemble_rf_train_table)\nensemble_rf_train_accuracy_mean <- mean(ensemble_rf_train_accuracy)\n\nensemble_rf_test_pred <- stats::predict(ensemble_rf_train_fit, ensemble_test, type = \"prob\")\nensemble_rf_test_predictions <- ifelse(ensemble_rf_test_pred > 0.50, 1, 0)[, 2]\nensemble_rf_test_table <- table(ensemble_rf_test_predictions, ensemble_y_test)\nensemble_rf_test_accuracy[i] <- (ensemble_rf_test_table[1, 1] + ensemble_rf_test_table[2, 2]) / sum(ensemble_rf_test_table)\nensemble_rf_test_accuracy_mean <- mean(ensemble_rf_test_accuracy)\n\nensemble_rf_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_rf_test_predictions)))\nensemble_rf_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_rf_test_predictions)) - 1)), 4)\nprint(pROC::ggroc(ensemble_rf_roc_obj, color = \"steelblue\", size = 2) +\n        ggplot2::ggtitle(paste0(\"Ensemble_rf \", \"(AUC = \", ensemble_rf_auc, \")\")) +\n        ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n        ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n\n# Ensemble XGBoost\n\nensemble_train_x <- data.matrix(ensemble_train[, -ncol(ensemble_train)])\nensemble_train_y <- ensemble_train[, ncol(ensemble_train)]\n\n  # define predictor and response variables in test set\nensemble_test_x <- data.matrix(ensemble_test[, -ncol(ensemble_test)])\nensemble_test_y <- ensemble_test[, ncol(ensemble_test)]\n\n# define final train and test sets\nensemble_xgb_train <- xgboost::xgb.DMatrix(data = ensemble_train_x, label = ensemble_train_y)\nensemble_xgb_test <- xgboost::xgb.DMatrix(data = ensemble_test_x, label = ensemble_test_y)\n\n# define watchlist\nensemble_watchlist <- list(train = ensemble_xgb_train)\nensemble_watchlist_test <- list(train = ensemble_xgb_train, test = ensemble_xgb_test)\n\nensemble_xgb_model <- xgboost::xgb.train(data = ensemble_xgb_train, max.depth = 3, watchlist = ensemble_watchlist_test, nrounds = 70)\n\nensemble_xgboost_min <- which.min(ensemble_xgb_model$evaluation_log$validation_rmse)\n\nensemble_xgb_train_pred <- predict(object = ensemble_xgb_model, newdata = ensemble_train_x, type = \"response\")\nensemble_xgb_train_probabilities <- ifelse(ensemble_xgb_train_pred > 0.5, 1, 0)\nensemble_xgb_train_table <- table(ensemble_xgb_train_probabilities, ensemble_y_train)\nensemble_xgb_train_accuracy[i] <- (ensemble_xgb_train_table[1, 1] + ensemble_xgb_train_table[2, 2]) / sum(ensemble_xgb_train_table)\nensemble_xgb_train_accuracy_mean <- mean(ensemble_xgb_train_accuracy)\n\nensemble_xgb_test_pred <- predict(object = ensemble_xgb_model, newdata = ensemble_test_x, type = \"response\")\nensemble_xgb_test_probabilities <- ifelse(ensemble_xgb_test_pred > 0.5, 1, 0)\nensemble_xgb_test_table <- table(ensemble_xgb_test_probabilities, ensemble_y_test)\nensemble_xgb_test_accuracy[i] <- (ensemble_xgb_test_table[1, 1] + ensemble_xgb_test_table[2, 2]) / sum(ensemble_xgb_test_table)\nensemble_xgb_test_accuracy_mean <- mean(ensemble_xgb_test_accuracy)\n\nensemble_xgb_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_xgb_test_pred)))\nensemble_xgb_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_xgb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(ensemble_xgb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Ensemble XGBoost \", \"(AUC = \", ensemble_xgb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n  \n  \n# Save all trained models to the Environment\nadaboost_train_fit <<- adaboost_train_fit\nbayesglm_train_fit <<- bayesglm_train_fit\nC50_train_fit<<- C50_train_fit\ncubist_train_fit <<- cubist_train_fit\nrf_train_fit <<- rf_train_fit\nxgb_model <<- xgb_model\nensemble_adaboost_train_fit <<- ensemble_adaboost_train_fit\nensemble_C50_train_fit <<- ensemble_C50_train_fit\nensemble_rf_train_fit <<- ensemble_rf_train_fit\nensemble_xgb_model <<- ensemble_xgb_model\n\nresults <- data.frame(\n  'Model'= c('ADABoost', 'BayesGLM', 'C50', 'Cubist', 'Random_Forest', 'XGBoost', 'Ensemble_ADABoost', 'Ensemble_C50', 'Ensemble_Random_Forest', 'Ensemble_XGBoost'),\n  'Accuracy' = c(adaboost_test_accuracy_mean, bayesglm_test_accuracy_mean, C50_test_accuracy_mean, cubist_test_accuracy_mean, rf_test_accuracy_mean, xgb_test_accuracy_mean, ensemble_adaboost_holdout_accuracy_mean, ensemble_C50_test_accuracy_mean, ensemble_rf_test_accuracy_mean, ensemble_xgb_test_accuracy_mean)\n)\n\nresults <- results %>% arrange(desc(Accuracy), Model)\n\n} # Closing loop for numresamples\nreturn(results)\n\n} # Closing loop for the function\n\nlogistic_1(data = lebron, colnum = 6, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.476144 test-rmse:0.478562 \n#> [2]  train-rmse:0.463574 test-rmse:0.467503 \n#> [3]  train-rmse:0.454959 test-rmse:0.460474 \n#> [4]  train-rmse:0.448993 test-rmse:0.455963 \n#> [5]  train-rmse:0.444821 test-rmse:0.452101 \n#> [6]  train-rmse:0.440419 test-rmse:0.451403 \n#> [7]  train-rmse:0.436667 test-rmse:0.450788 \n#> [8]  train-rmse:0.433511 test-rmse:0.449871 \n#> [9]  train-rmse:0.431663 test-rmse:0.450169 \n#> [10] train-rmse:0.427720 test-rmse:0.450046 \n#> [11] train-rmse:0.424788 test-rmse:0.450895 \n#> [12] train-rmse:0.422744 test-rmse:0.449402 \n#> [13] train-rmse:0.419597 test-rmse:0.450385 \n#> [14] train-rmse:0.417344 test-rmse:0.450237 \n#> [15] train-rmse:0.413427 test-rmse:0.450686 \n#> [16] train-rmse:0.411022 test-rmse:0.450659 \n#> [17] train-rmse:0.409211 test-rmse:0.450818 \n#> [18] train-rmse:0.405904 test-rmse:0.452264 \n#> [19] train-rmse:0.403075 test-rmse:0.452364 \n#> [20] train-rmse:0.402247 test-rmse:0.453080 \n#> [21] train-rmse:0.398977 test-rmse:0.452725 \n#> [22] train-rmse:0.397407 test-rmse:0.453944 \n#> [23] train-rmse:0.393930 test-rmse:0.453229 \n#> [24] train-rmse:0.389994 test-rmse:0.452871 \n#> [25] train-rmse:0.388467 test-rmse:0.453027 \n#> [26] train-rmse:0.386738 test-rmse:0.452534 \n#> [27] train-rmse:0.384467 test-rmse:0.452899 \n#> [28] train-rmse:0.382570 test-rmse:0.453100 \n#> [29] train-rmse:0.379445 test-rmse:0.454196 \n#> [30] train-rmse:0.377415 test-rmse:0.454774 \n#> [31] train-rmse:0.376504 test-rmse:0.455492 \n#> [32] train-rmse:0.375392 test-rmse:0.455794 \n#> [33] train-rmse:0.373634 test-rmse:0.455489 \n#> [34] train-rmse:0.371433 test-rmse:0.455733 \n#> [35] train-rmse:0.368571 test-rmse:0.456959 \n#> [36] train-rmse:0.367051 test-rmse:0.457008 \n#> [37] train-rmse:0.363892 test-rmse:0.458813 \n#> [38] train-rmse:0.361112 test-rmse:0.458801 \n#> [39] train-rmse:0.359866 test-rmse:0.458274 \n#> [40] train-rmse:0.358560 test-rmse:0.457887 \n#> [41] train-rmse:0.358106 test-rmse:0.458127 \n#> [42] train-rmse:0.355921 test-rmse:0.458705 \n#> [43] train-rmse:0.353788 test-rmse:0.459461 \n#> [44] train-rmse:0.353182 test-rmse:0.459651 \n#> [45] train-rmse:0.352354 test-rmse:0.459765 \n#> [46] train-rmse:0.350558 test-rmse:0.460158 \n#> [47] train-rmse:0.348136 test-rmse:0.458902 \n#> [48] train-rmse:0.346172 test-rmse:0.459073 \n#> [49] train-rmse:0.345309 test-rmse:0.460300 \n#> [50] train-rmse:0.342630 test-rmse:0.460079 \n#> [51] train-rmse:0.341311 test-rmse:0.461742 \n#> [52] train-rmse:0.339357 test-rmse:0.462133 \n#> [53] train-rmse:0.337334 test-rmse:0.462307 \n#> [54] train-rmse:0.334896 test-rmse:0.463136 \n#> [55] train-rmse:0.333012 test-rmse:0.463257 \n#> [56] train-rmse:0.332788 test-rmse:0.463209 \n#> [57] train-rmse:0.330628 test-rmse:0.464458 \n#> [58] train-rmse:0.328841 test-rmse:0.464871 \n#> [59] train-rmse:0.327336 test-rmse:0.466450 \n#> [60] train-rmse:0.325394 test-rmse:0.467173 \n#> [61] train-rmse:0.323361 test-rmse:0.467378 \n#> [62] train-rmse:0.321633 test-rmse:0.467970 \n#> [63] train-rmse:0.319833 test-rmse:0.468894 \n#> [64] train-rmse:0.317659 test-rmse:0.470018 \n#> [65] train-rmse:0.316241 test-rmse:0.470120 \n#> [66] train-rmse:0.314436 test-rmse:0.471113 \n#> [67] train-rmse:0.312160 test-rmse:0.471866 \n#> [68] train-rmse:0.311475 test-rmse:0.472474 \n#> [69] train-rmse:0.309364 test-rmse:0.473537 \n#> [70] train-rmse:0.308724 test-rmse:0.473588\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350824 test-rmse:0.350826 \n#> [2]  train-rmse:0.246155 test-rmse:0.246158 \n#> [3]  train-rmse:0.172714 test-rmse:0.172718 \n#> [4]  train-rmse:0.121185 test-rmse:0.121188 \n#> [5]  train-rmse:0.085029 test-rmse:0.085032 \n#> [6]  train-rmse:0.059661 test-rmse:0.059663 \n#> [7]  train-rmse:0.041861 test-rmse:0.041863 \n#> [8]  train-rmse:0.029372 test-rmse:0.029373 \n#> [9]  train-rmse:0.020608 test-rmse:0.020610 \n#> [10] train-rmse:0.014460 test-rmse:0.014461 \n#> [11] train-rmse:0.010146 test-rmse:0.010146 \n#> [12] train-rmse:0.007119 test-rmse:0.007119 \n#> [13] train-rmse:0.004995 test-rmse:0.004995 \n#> [14] train-rmse:0.003505 test-rmse:0.003505 \n#> [15] train-rmse:0.002459 test-rmse:0.002459 \n#> [16] train-rmse:0.001725 test-rmse:0.001726 \n#> [17] train-rmse:0.001211 test-rmse:0.001211 \n#> [18] train-rmse:0.000849 test-rmse:0.000849 \n#> [19] train-rmse:0.000596 test-rmse:0.000596 \n#> [20] train-rmse:0.000418 test-rmse:0.000418 \n#> [21] train-rmse:0.000293 test-rmse:0.000293 \n#> [22] train-rmse:0.000206 test-rmse:0.000206 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.470376 test-rmse:0.479959 \n#> [2]  train-rmse:0.453387 test-rmse:0.471480 \n#> [3]  train-rmse:0.442872 test-rmse:0.467979 \n#> [4]  train-rmse:0.435073 test-rmse:0.467341 \n#> [5]  train-rmse:0.431218 test-rmse:0.466991 \n#> [6]  train-rmse:0.425888 test-rmse:0.467568 \n#> [7]  train-rmse:0.423895 test-rmse:0.466266 \n#> [8]  train-rmse:0.419954 test-rmse:0.466441 \n#> [9]  train-rmse:0.417434 test-rmse:0.465230 \n#> [10] train-rmse:0.413718 test-rmse:0.466888 \n#> [11] train-rmse:0.410281 test-rmse:0.466853 \n#> [12] train-rmse:0.407585 test-rmse:0.468813 \n#> [13] train-rmse:0.406435 test-rmse:0.468586 \n#> [14] train-rmse:0.405209 test-rmse:0.469302 \n#> [15] train-rmse:0.401530 test-rmse:0.468692 \n#> [16] train-rmse:0.398126 test-rmse:0.468722 \n#> [17] train-rmse:0.395889 test-rmse:0.469836 \n#> [18] train-rmse:0.392163 test-rmse:0.470516 \n#> [19] train-rmse:0.387945 test-rmse:0.471069 \n#> [20] train-rmse:0.386932 test-rmse:0.471384 \n#> [21] train-rmse:0.384878 test-rmse:0.472052 \n#> [22] train-rmse:0.382496 test-rmse:0.472522 \n#> [23] train-rmse:0.379117 test-rmse:0.473605 \n#> [24] train-rmse:0.375649 test-rmse:0.473835 \n#> [25] train-rmse:0.373457 test-rmse:0.474532 \n#> [26] train-rmse:0.370817 test-rmse:0.474590 \n#> [27] train-rmse:0.368762 test-rmse:0.476130 \n#> [28] train-rmse:0.366161 test-rmse:0.476836 \n#> [29] train-rmse:0.363188 test-rmse:0.477363 \n#> [30] train-rmse:0.361532 test-rmse:0.476964 \n#> [31] train-rmse:0.360264 test-rmse:0.477921 \n#> [32] train-rmse:0.357790 test-rmse:0.477276 \n#> [33] train-rmse:0.355311 test-rmse:0.478001 \n#> [34] train-rmse:0.352196 test-rmse:0.477349 \n#> [35] train-rmse:0.350114 test-rmse:0.477405 \n#> [36] train-rmse:0.349005 test-rmse:0.477194 \n#> [37] train-rmse:0.347719 test-rmse:0.477163 \n#> [38] train-rmse:0.346784 test-rmse:0.477026 \n#> [39] train-rmse:0.345636 test-rmse:0.477004 \n#> [40] train-rmse:0.343623 test-rmse:0.477453 \n#> [41] train-rmse:0.340926 test-rmse:0.477200 \n#> [42] train-rmse:0.339610 test-rmse:0.477063 \n#> [43] train-rmse:0.338114 test-rmse:0.476603 \n#> [44] train-rmse:0.337161 test-rmse:0.477081 \n#> [45] train-rmse:0.335310 test-rmse:0.477718 \n#> [46] train-rmse:0.333155 test-rmse:0.478709 \n#> [47] train-rmse:0.331283 test-rmse:0.479318 \n#> [48] train-rmse:0.328594 test-rmse:0.479546 \n#> [49] train-rmse:0.327513 test-rmse:0.480445 \n#> [50] train-rmse:0.327141 test-rmse:0.481062 \n#> [51] train-rmse:0.325709 test-rmse:0.481752 \n#> [52] train-rmse:0.323339 test-rmse:0.482711 \n#> [53] train-rmse:0.320991 test-rmse:0.483422 \n#> [54] train-rmse:0.320335 test-rmse:0.483969 \n#> [55] train-rmse:0.318912 test-rmse:0.484782 \n#> [56] train-rmse:0.317346 test-rmse:0.485648 \n#> [57] train-rmse:0.316029 test-rmse:0.485568 \n#> [58] train-rmse:0.314046 test-rmse:0.486434 \n#> [59] train-rmse:0.311916 test-rmse:0.487268 \n#> [60] train-rmse:0.309432 test-rmse:0.487138 \n#> [61] train-rmse:0.308339 test-rmse:0.487302 \n#> [62] train-rmse:0.307715 test-rmse:0.487245 \n#> [63] train-rmse:0.306562 test-rmse:0.487641 \n#> [64] train-rmse:0.305473 test-rmse:0.487510 \n#> [65] train-rmse:0.303432 test-rmse:0.487882 \n#> [66] train-rmse:0.301222 test-rmse:0.488623 \n#> [67] train-rmse:0.298861 test-rmse:0.489104 \n#> [68] train-rmse:0.297184 test-rmse:0.490484 \n#> [69] train-rmse:0.295459 test-rmse:0.490403 \n#> [70] train-rmse:0.294919 test-rmse:0.490425\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350822 test-rmse:0.350821 \n#> [2]  train-rmse:0.246152 test-rmse:0.246151 \n#> [3]  train-rmse:0.172711 test-rmse:0.172710 \n#> [4]  train-rmse:0.121182 test-rmse:0.121180 \n#> [5]  train-rmse:0.085026 test-rmse:0.085025 \n#> [6]  train-rmse:0.059658 test-rmse:0.059657 \n#> [7]  train-rmse:0.041859 test-rmse:0.041858 \n#> [8]  train-rmse:0.029370 test-rmse:0.029369 \n#> [9]  train-rmse:0.020607 test-rmse:0.020607 \n#> [10] train-rmse:0.014459 test-rmse:0.014459 \n#> [11] train-rmse:0.010145 test-rmse:0.010145 \n#> [12] train-rmse:0.007118 test-rmse:0.007118 \n#> [13] train-rmse:0.004994 test-rmse:0.004994 \n#> [14] train-rmse:0.003504 test-rmse:0.003504 \n#> [15] train-rmse:0.002459 test-rmse:0.002459 \n#> [16] train-rmse:0.001725 test-rmse:0.001725 \n#> [17] train-rmse:0.001210 test-rmse:0.001210 \n#> [18] train-rmse:0.000849 test-rmse:0.000849 \n#> [19] train-rmse:0.000596 test-rmse:0.000596 \n#> [20] train-rmse:0.000418 test-rmse:0.000418 \n#> [21] train-rmse:0.000293 test-rmse:0.000293 \n#> [22] train-rmse:0.000206 test-rmse:0.000206 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.471391 test-rmse:0.481189 \n#> [2]  train-rmse:0.455547 test-rmse:0.469744 \n#> [3]  train-rmse:0.445057 test-rmse:0.466894 \n#> [4]  train-rmse:0.438664 test-rmse:0.466114 \n#> [5]  train-rmse:0.432583 test-rmse:0.464660 \n#> [6]  train-rmse:0.427073 test-rmse:0.466880 \n#> [7]  train-rmse:0.421625 test-rmse:0.464989 \n#> [8]  train-rmse:0.417663 test-rmse:0.464809 \n#> [9]  train-rmse:0.415027 test-rmse:0.465373 \n#> [10] train-rmse:0.409836 test-rmse:0.466984 \n#> [11] train-rmse:0.405816 test-rmse:0.469168 \n#> [12] train-rmse:0.403440 test-rmse:0.470523 \n#> [13] train-rmse:0.401912 test-rmse:0.471266 \n#> [14] train-rmse:0.398924 test-rmse:0.472400 \n#> [15] train-rmse:0.395019 test-rmse:0.472489 \n#> [16] train-rmse:0.393648 test-rmse:0.471867 \n#> [17] train-rmse:0.391375 test-rmse:0.473183 \n#> [18] train-rmse:0.388984 test-rmse:0.472980 \n#> [19] train-rmse:0.388284 test-rmse:0.473698 \n#> [20] train-rmse:0.386324 test-rmse:0.474739 \n#> [21] train-rmse:0.385213 test-rmse:0.474960 \n#> [22] train-rmse:0.382278 test-rmse:0.474498 \n#> [23] train-rmse:0.378827 test-rmse:0.476668 \n#> [24] train-rmse:0.377884 test-rmse:0.477570 \n#> [25] train-rmse:0.374871 test-rmse:0.477621 \n#> [26] train-rmse:0.372707 test-rmse:0.477877 \n#> [27] train-rmse:0.370127 test-rmse:0.477583 \n#> [28] train-rmse:0.367759 test-rmse:0.476749 \n#> [29] train-rmse:0.366066 test-rmse:0.476368 \n#> [30] train-rmse:0.363867 test-rmse:0.476622 \n#> [31] train-rmse:0.362045 test-rmse:0.477304 \n#> [32] train-rmse:0.358841 test-rmse:0.476316 \n#> [33] train-rmse:0.356543 test-rmse:0.476612 \n#> [34] train-rmse:0.353660 test-rmse:0.476929 \n#> [35] train-rmse:0.352077 test-rmse:0.477073 \n#> [36] train-rmse:0.351153 test-rmse:0.477482 \n#> [37] train-rmse:0.348208 test-rmse:0.477965 \n#> [38] train-rmse:0.346105 test-rmse:0.477946 \n#> [39] train-rmse:0.344032 test-rmse:0.477134 \n#> [40] train-rmse:0.341681 test-rmse:0.477221 \n#> [41] train-rmse:0.338970 test-rmse:0.478715 \n#> [42] train-rmse:0.336918 test-rmse:0.479090 \n#> [43] train-rmse:0.335588 test-rmse:0.479170 \n#> [44] train-rmse:0.333278 test-rmse:0.480013 \n#> [45] train-rmse:0.332120 test-rmse:0.480946 \n#> [46] train-rmse:0.329920 test-rmse:0.481960 \n#> [47] train-rmse:0.328341 test-rmse:0.482355 \n#> [48] train-rmse:0.326399 test-rmse:0.483102 \n#> [49] train-rmse:0.324738 test-rmse:0.484285 \n#> [50] train-rmse:0.324118 test-rmse:0.484037 \n#> [51] train-rmse:0.321977 test-rmse:0.485103 \n#> [52] train-rmse:0.320736 test-rmse:0.485361 \n#> [53] train-rmse:0.318751 test-rmse:0.487033 \n#> [54] train-rmse:0.316191 test-rmse:0.488110 \n#> [55] train-rmse:0.314172 test-rmse:0.487651 \n#> [56] train-rmse:0.313694 test-rmse:0.488266 \n#> [57] train-rmse:0.311423 test-rmse:0.488294 \n#> [58] train-rmse:0.310172 test-rmse:0.488361 \n#> [59] train-rmse:0.308348 test-rmse:0.488932 \n#> [60] train-rmse:0.306982 test-rmse:0.489724 \n#> [61] train-rmse:0.305282 test-rmse:0.490384 \n#> [62] train-rmse:0.303546 test-rmse:0.490633 \n#> [63] train-rmse:0.302081 test-rmse:0.490621 \n#> [64] train-rmse:0.301427 test-rmse:0.490028 \n#> [65] train-rmse:0.300181 test-rmse:0.490678 \n#> [66] train-rmse:0.298538 test-rmse:0.492055 \n#> [67] train-rmse:0.297295 test-rmse:0.492493 \n#> [68] train-rmse:0.296920 test-rmse:0.492876 \n#> [69] train-rmse:0.295868 test-rmse:0.493036 \n#> [70] train-rmse:0.295132 test-rmse:0.493147\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350732 test-rmse:0.350737 \n#> [2]  train-rmse:0.246025 test-rmse:0.246032 \n#> [3]  train-rmse:0.172578 test-rmse:0.172585 \n#> [4]  train-rmse:0.121057 test-rmse:0.121064 \n#> [5]  train-rmse:0.084917 test-rmse:0.084923 \n#> [6]  train-rmse:0.059566 test-rmse:0.059571 \n#> [7]  train-rmse:0.041784 test-rmse:0.041788 \n#> [8]  train-rmse:0.029310 test-rmse:0.029313 \n#> [9]  train-rmse:0.020560 test-rmse:0.020562 \n#> [10] train-rmse:0.014422 test-rmse:0.014424 \n#> [11] train-rmse:0.010116 test-rmse:0.010118 \n#> [12] train-rmse:0.007096 test-rmse:0.007097 \n#> [13] train-rmse:0.004978 test-rmse:0.004979 \n#> [14] train-rmse:0.003492 test-rmse:0.003492 \n#> [15] train-rmse:0.002449 test-rmse:0.002450 \n#> [16] train-rmse:0.001718 test-rmse:0.001718 \n#> [17] train-rmse:0.001205 test-rmse:0.001205 \n#> [18] train-rmse:0.000845 test-rmse:0.000846 \n#> [19] train-rmse:0.000593 test-rmse:0.000593 \n#> [20] train-rmse:0.000416 test-rmse:0.000416 \n#> [21] train-rmse:0.000292 test-rmse:0.000292 \n#> [22] train-rmse:0.000205 test-rmse:0.000205 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000049 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.474806 test-rmse:0.477327 \n#> [2]  train-rmse:0.460194 test-rmse:0.468567 \n#> [3]  train-rmse:0.451103 test-rmse:0.464446 \n#> [4]  train-rmse:0.444766 test-rmse:0.461353 \n#> [5]  train-rmse:0.439933 test-rmse:0.459735 \n#> [6]  train-rmse:0.434587 test-rmse:0.458148 \n#> [7]  train-rmse:0.432074 test-rmse:0.456628 \n#> [8]  train-rmse:0.428070 test-rmse:0.456919 \n#> [9]  train-rmse:0.425809 test-rmse:0.457126 \n#> [10] train-rmse:0.423424 test-rmse:0.457745 \n#> [11] train-rmse:0.421371 test-rmse:0.457869 \n#> [12] train-rmse:0.418730 test-rmse:0.457889 \n#> [13] train-rmse:0.415050 test-rmse:0.457260 \n#> [14] train-rmse:0.410685 test-rmse:0.458697 \n#> [15] train-rmse:0.406654 test-rmse:0.458959 \n#> [16] train-rmse:0.402437 test-rmse:0.459619 \n#> [17] train-rmse:0.398835 test-rmse:0.460915 \n#> [18] train-rmse:0.396471 test-rmse:0.461584 \n#> [19] train-rmse:0.395539 test-rmse:0.461310 \n#> [20] train-rmse:0.392449 test-rmse:0.462386 \n#> [21] train-rmse:0.388385 test-rmse:0.463716 \n#> [22] train-rmse:0.386142 test-rmse:0.464954 \n#> [23] train-rmse:0.383937 test-rmse:0.466150 \n#> [24] train-rmse:0.383189 test-rmse:0.467036 \n#> [25] train-rmse:0.381766 test-rmse:0.467289 \n#> [26] train-rmse:0.379216 test-rmse:0.467775 \n#> [27] train-rmse:0.376576 test-rmse:0.468056 \n#> [28] train-rmse:0.374455 test-rmse:0.468093 \n#> [29] train-rmse:0.372813 test-rmse:0.468241 \n#> [30] train-rmse:0.371925 test-rmse:0.468451 \n#> [31] train-rmse:0.370339 test-rmse:0.468939 \n#> [32] train-rmse:0.368483 test-rmse:0.469352 \n#> [33] train-rmse:0.365769 test-rmse:0.470102 \n#> [34] train-rmse:0.363748 test-rmse:0.470834 \n#> [35] train-rmse:0.362648 test-rmse:0.470980 \n#> [36] train-rmse:0.359854 test-rmse:0.471540 \n#> [37] train-rmse:0.358168 test-rmse:0.472813 \n#> [38] train-rmse:0.355880 test-rmse:0.472769 \n#> [39] train-rmse:0.353999 test-rmse:0.472446 \n#> [40] train-rmse:0.351588 test-rmse:0.473530 \n#> [41] train-rmse:0.349368 test-rmse:0.472972 \n#> [42] train-rmse:0.348816 test-rmse:0.473473 \n#> [43] train-rmse:0.346445 test-rmse:0.474530 \n#> [44] train-rmse:0.344027 test-rmse:0.474449 \n#> [45] train-rmse:0.341265 test-rmse:0.476748 \n#> [46] train-rmse:0.338787 test-rmse:0.477893 \n#> [47] train-rmse:0.336858 test-rmse:0.478158 \n#> [48] train-rmse:0.335004 test-rmse:0.479104 \n#> [49] train-rmse:0.334594 test-rmse:0.478971 \n#> [50] train-rmse:0.333463 test-rmse:0.478899 \n#> [51] train-rmse:0.332069 test-rmse:0.479791 \n#> [52] train-rmse:0.331328 test-rmse:0.481001 \n#> [53] train-rmse:0.329724 test-rmse:0.481301 \n#> [54] train-rmse:0.327866 test-rmse:0.481444 \n#> [55] train-rmse:0.325509 test-rmse:0.481943 \n#> [56] train-rmse:0.323790 test-rmse:0.482850 \n#> [57] train-rmse:0.322412 test-rmse:0.483244 \n#> [58] train-rmse:0.320115 test-rmse:0.483289 \n#> [59] train-rmse:0.319143 test-rmse:0.483689 \n#> [60] train-rmse:0.318132 test-rmse:0.483346 \n#> [61] train-rmse:0.317056 test-rmse:0.483574 \n#> [62] train-rmse:0.314811 test-rmse:0.484470 \n#> [63] train-rmse:0.313338 test-rmse:0.485339 \n#> [64] train-rmse:0.310976 test-rmse:0.485383 \n#> [65] train-rmse:0.309806 test-rmse:0.486243 \n#> [66] train-rmse:0.307857 test-rmse:0.486291 \n#> [67] train-rmse:0.306005 test-rmse:0.486813 \n#> [68] train-rmse:0.303882 test-rmse:0.486972 \n#> [69] train-rmse:0.303206 test-rmse:0.487281 \n#> [70] train-rmse:0.301917 test-rmse:0.487293\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350811 test-rmse:0.350811 \n#> [2]  train-rmse:0.246136 test-rmse:0.246136 \n#> [3]  train-rmse:0.172695 test-rmse:0.172695 \n#> [4]  train-rmse:0.121166 test-rmse:0.121166 \n#> [5]  train-rmse:0.085013 test-rmse:0.085013 \n#> [6]  train-rmse:0.059647 test-rmse:0.059647 \n#> [7]  train-rmse:0.041850 test-rmse:0.041850 \n#> [8]  train-rmse:0.029363 test-rmse:0.029363 \n#> [9]  train-rmse:0.020601 test-rmse:0.020601 \n#> [10] train-rmse:0.014454 test-rmse:0.014454 \n#> [11] train-rmse:0.010141 test-rmse:0.010141 \n#> [12] train-rmse:0.007115 test-rmse:0.007115 \n#> [13] train-rmse:0.004992 test-rmse:0.004992 \n#> [14] train-rmse:0.003503 test-rmse:0.003503 \n#> [15] train-rmse:0.002458 test-rmse:0.002458 \n#> [16] train-rmse:0.001724 test-rmse:0.001724 \n#> [17] train-rmse:0.001210 test-rmse:0.001210 \n#> [18] train-rmse:0.000849 test-rmse:0.000849 \n#> [19] train-rmse:0.000596 test-rmse:0.000596 \n#> [20] train-rmse:0.000418 test-rmse:0.000418 \n#> [21] train-rmse:0.000293 test-rmse:0.000293 \n#> [22] train-rmse:0.000206 test-rmse:0.000206 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.471577 test-rmse:0.480744 \n#> [2]  train-rmse:0.454265 test-rmse:0.471583 \n#> [3]  train-rmse:0.444762 test-rmse:0.468178 \n#> [4]  train-rmse:0.438154 test-rmse:0.465017 \n#> [5]  train-rmse:0.434042 test-rmse:0.464051 \n#> [6]  train-rmse:0.429515 test-rmse:0.463162 \n#> [7]  train-rmse:0.424879 test-rmse:0.462757 \n#> [8]  train-rmse:0.422605 test-rmse:0.463300 \n#> [9]  train-rmse:0.418741 test-rmse:0.463332 \n#> [10] train-rmse:0.415888 test-rmse:0.465554 \n#> [11] train-rmse:0.412331 test-rmse:0.465948 \n#> [12] train-rmse:0.409270 test-rmse:0.466338 \n#> [13] train-rmse:0.407598 test-rmse:0.464630 \n#> [14] train-rmse:0.405619 test-rmse:0.465577 \n#> [15] train-rmse:0.403221 test-rmse:0.465341 \n#> [16] train-rmse:0.401821 test-rmse:0.465707 \n#> [17] train-rmse:0.400564 test-rmse:0.466053 \n#> [18] train-rmse:0.399474 test-rmse:0.465748 \n#> [19] train-rmse:0.397441 test-rmse:0.465750 \n#> [20] train-rmse:0.394472 test-rmse:0.466224 \n#> [21] train-rmse:0.393116 test-rmse:0.466412 \n#> [22] train-rmse:0.390620 test-rmse:0.465684 \n#> [23] train-rmse:0.387416 test-rmse:0.466095 \n#> [24] train-rmse:0.386378 test-rmse:0.466550 \n#> [25] train-rmse:0.384605 test-rmse:0.466656 \n#> [26] train-rmse:0.381410 test-rmse:0.466764 \n#> [27] train-rmse:0.378212 test-rmse:0.465584 \n#> [28] train-rmse:0.377111 test-rmse:0.465691 \n#> [29] train-rmse:0.374755 test-rmse:0.466208 \n#> [30] train-rmse:0.371775 test-rmse:0.466713 \n#> [31] train-rmse:0.370571 test-rmse:0.466772 \n#> [32] train-rmse:0.369029 test-rmse:0.468559 \n#> [33] train-rmse:0.368610 test-rmse:0.468287 \n#> [34] train-rmse:0.367808 test-rmse:0.468524 \n#> [35] train-rmse:0.367062 test-rmse:0.468352 \n#> [36] train-rmse:0.366186 test-rmse:0.468407 \n#> [37] train-rmse:0.363883 test-rmse:0.467376 \n#> [38] train-rmse:0.361354 test-rmse:0.468753 \n#> [39] train-rmse:0.359755 test-rmse:0.469330 \n#> [40] train-rmse:0.357923 test-rmse:0.469506 \n#> [41] train-rmse:0.354185 test-rmse:0.469816 \n#> [42] train-rmse:0.352034 test-rmse:0.471337 \n#> [43] train-rmse:0.348321 test-rmse:0.471297 \n#> [44] train-rmse:0.344673 test-rmse:0.471338 \n#> [45] train-rmse:0.341964 test-rmse:0.471260 \n#> [46] train-rmse:0.339466 test-rmse:0.471782 \n#> [47] train-rmse:0.338032 test-rmse:0.471911 \n#> [48] train-rmse:0.335915 test-rmse:0.471929 \n#> [49] train-rmse:0.333468 test-rmse:0.472566 \n#> [50] train-rmse:0.331806 test-rmse:0.472157 \n#> [51] train-rmse:0.328948 test-rmse:0.472021 \n#> [52] train-rmse:0.326076 test-rmse:0.472872 \n#> [53] train-rmse:0.323846 test-rmse:0.473045 \n#> [54] train-rmse:0.321768 test-rmse:0.473011 \n#> [55] train-rmse:0.320399 test-rmse:0.472836 \n#> [56] train-rmse:0.318842 test-rmse:0.472671 \n#> [57] train-rmse:0.316777 test-rmse:0.473612 \n#> [58] train-rmse:0.316078 test-rmse:0.473772 \n#> [59] train-rmse:0.315315 test-rmse:0.473332 \n#> [60] train-rmse:0.313517 test-rmse:0.473501 \n#> [61] train-rmse:0.312084 test-rmse:0.474112 \n#> [62] train-rmse:0.310038 test-rmse:0.473522 \n#> [63] train-rmse:0.309017 test-rmse:0.473653 \n#> [64] train-rmse:0.307556 test-rmse:0.473114 \n#> [65] train-rmse:0.305566 test-rmse:0.472764 \n#> [66] train-rmse:0.303763 test-rmse:0.472985 \n#> [67] train-rmse:0.302499 test-rmse:0.473220 \n#> [68] train-rmse:0.300772 test-rmse:0.474625 \n#> [69] train-rmse:0.298926 test-rmse:0.475909 \n#> [70] train-rmse:0.297109 test-rmse:0.475763\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350845 test-rmse:0.350845 \n#> [2]  train-rmse:0.246185 test-rmse:0.246185 \n#> [3]  train-rmse:0.172745 test-rmse:0.172745 \n#> [4]  train-rmse:0.121214 test-rmse:0.121214 \n#> [5]  train-rmse:0.085054 test-rmse:0.085055 \n#> [6]  train-rmse:0.059682 test-rmse:0.059682 \n#> [7]  train-rmse:0.041878 test-rmse:0.041878 \n#> [8]  train-rmse:0.029385 test-rmse:0.029386 \n#> [9]  train-rmse:0.020620 test-rmse:0.020620 \n#> [10] train-rmse:0.014469 test-rmse:0.014469 \n#> [11] train-rmse:0.010152 test-rmse:0.010152 \n#> [12] train-rmse:0.007124 test-rmse:0.007124 \n#> [13] train-rmse:0.004999 test-rmse:0.004999 \n#> [14] train-rmse:0.003508 test-rmse:0.003508 \n#> [15] train-rmse:0.002461 test-rmse:0.002461 \n#> [16] train-rmse:0.001727 test-rmse:0.001727 \n#> [17] train-rmse:0.001212 test-rmse:0.001212 \n#> [18] train-rmse:0.000850 test-rmse:0.000850 \n#> [19] train-rmse:0.000597 test-rmse:0.000597 \n#> [20] train-rmse:0.000419 test-rmse:0.000419 \n#> [21] train-rmse:0.000294 test-rmse:0.000294 \n#> [22] train-rmse:0.000206 test-rmse:0.000206 \n#> [23] train-rmse:0.000145 test-rmse:0.000145 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#>                     Model  Accuracy\n#> 1                     C50 1.0000000\n#> 2                  Cubist 1.0000000\n#> 3       Ensemble_ADABoost 1.0000000\n#> 4            Ensemble_C50 1.0000000\n#> 5  Ensemble_Random_Forest 1.0000000\n#> 6        Ensemble_XGBoost 1.0000000\n#> 7           Random_Forest 1.0000000\n#> 8                BayesGLM 0.6455523\n#> 9                 XGBoost 0.6441496\n#> 10               ADABoost 0.6086781\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"how-to-make-27-individual-forecasting-models","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9 How to Make 27 Individual Forecasting Models","text":"chapter builds time series models. also known forecasting, professional organization named International Institute Forecasters, website https://forecasters.org. strongly recommend checking IIF, ’ve found good source skills knowledge comes forecasting.chapter going build 16 forecasting models. large groups models, variations within groups. example, use (use) seasonality model making process.’ll follow pattern/process ’ve following previous sections:Load librarySet initial values 0Create functionBreak data train test setsSet random resamplingFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setThe first step load library case time series forecasting, library excellent FPP3 library. excellent book available guides learner time series process. book Forecasting Principles Practice. currently third edition, recommend highly. website book :https://otexts.com/fpp3/time series data use important data published regular bases United States federal government: monthly labor report. large set time series data sets Bureau Labor Statistics website:https://www.bls.govThe top picks time series data :https://data.bls.gov/cgi-bin/surveymost?ceFor work looking one data set, ’s far watched result: Total nonfarm employment. data can found :https://data.bls.gov/timeseries/CES0000000001I data stored Github repository, w accessing data, ways data may retrieved. plan use lot, consider registering Application Program Interface (API) time series data. information API directions register available :https://www.bls.gov/developers/","code":""},{"path":"how-to-make-27-individual-forecasting-models.html","id":"individual-time-series-models","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1 12 Individual Time Series Models","text":"","code":""},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima-1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.1 Arima 1","text":"","code":"\n\nlibrary(fpp3)\n#> ── Attaching packages ────────────────────────── fpp3 0.5 ──\n#> ✔ tibble      3.2.1     ✔ tsibble     1.1.4\n#> ✔ dplyr       1.1.4     ✔ tsibbledata 0.4.1\n#> ✔ tidyr       1.3.1     ✔ feasts      0.3.2\n#> ✔ lubridate   1.9.3     ✔ fable       0.3.4\n#> ✔ ggplot2     3.5.1     ✔ fabletools  0.4.2\n#> ── Conflicts ───────────────────────────── fpp3_conflicts ──\n#> ✖ lubridate::date()    masks base::date()\n#> ✖ dplyr::filter()      masks stats::filter()\n#> ✖ tsibble::intersect() masks base::intersect()\n#> ✖ tsibble::interval()  masks lubridate::interval()\n#> ✖ dplyr::lag()         masks stats::lag()\n#> ✖ tsibble::setdiff()   masks base::setdiff()\n#> ✖ tsibble::union()     masks base::union()\n\n# Set initial values to 0\n\n# Set up function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\n# Build the model\nArima1_model = fable::ARIMA(Difference ~ season() + trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n\n# Calculate error rate\nArima1_test_error <- time_series_train %>%\n    fabletools::model(Arima1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n\n# Make predictions on the holdout/test data\nArima1_predictions <- time_series_test %>%\n    fabletools::model(\n      Arima1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n\n# Report the predictions\nArima1_prediction_model <- Arima1_predictions[1]\nArima1_prediction_date<- Arima1_predictions[2]\nArima1_prediction_range <- Arima1_predictions[3]\nArima1_prediction_mean <-Arima1_predictions[4]\n\nresults <- data.frame(\n  'Model' = Arima1_predictions[1],\n  'Error' = Arima1_test_error$RMSE,\n  'Date' = Arima1_predictions[2],\n  'Forecast' = Arima1_predictions[4]\n)\n\nreturn(results)\n\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date     .mean\n#> 1 Arima1_model 58.39161 2024 May  740.0623\n#> 2 Arima1_model 58.39161 2024 Jun 1029.3480\n#> 3 Arima1_model 58.39161 2024 Jul  586.4908\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima-2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.2 Arima 2","text":"","code":"\n\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\nArima2_model <- fable::ARIMA(Difference ~ season(), stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n\n# Calculate error rate\nArima2_test_error <- time_series_train %>%\n  fabletools::model(Arima2_model) %>%\n  fabletools::forecast(h = number) %>%\n  fabletools::accuracy(time_series_test)\n\n# Make predictions on the holdout/test data\nArima2_predictions <- time_series_test %>%\n  fabletools::model(\n    Arima2_model,\n  ) %>%\n  fabletools::forecast(h = number)\n\n# Report the predictions\nArima2_prediction_model <- Arima2_predictions[1]\nArima2_prediction_date<- Arima2_predictions[2]\nArima2_prediction_range <- Arima2_predictions[3]\nArima2_prediction_mean <-Arima2_predictions[4]\n\nresults <- data.frame(\n  'Model' = Arima2_predictions[1],\n  'Error' = Arima2_test_error$RMSE,\n  'Date' = Arima2_predictions[2],\n  'Forecast' = Arima2_predictions[4]\n)\n\nreturn(results)\n\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date    .mean\n#> 1 Arima2_model 54.98322 2024 May 623.5714\n#> 2 Arima2_model 54.98322 2024 Jun 912.8571\n#> 3 Arima2_model 54.98322 2024 Jul 470.0000\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.3 Arima3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\n# Create the model:\nArima3_model <- fable::ARIMA(Difference ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n\n# Calculate the error:\nArima3_test_error <- time_series_train %>%\n    fabletools::model(Arima3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n\n# Calculate the forecast:\nArima3_predictions <- time_series_test %>%\n    fabletools::model(\n      Arima3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n\n# Report the predictions:\nresults <- data.frame(\n  'Model' = Arima3_predictions[1],\n  'Error' = Arima3_test_error$RMSE,\n  'Date' = Arima3_predictions[2],\n  'Forecast' = Arima3_predictions[4]\n)\n\nreturn(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date    .mean\n#> 1 Arima3_model 46.96308 2024 May 196.6640\n#> 2 Arima3_model 46.96308 2024 Jun 197.5579\n#> 3 Arima3_model 46.96308 2024 Jul 198.4518\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.4 Arima4","text":"","code":"\n\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nArima4_model <- fable::ARIMA(Difference ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n  \n  # Calculate the error:\n  Arima4_test_error <- time_series_train %>%\n    fabletools::model(Arima4_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Arima4_predictions <- time_series_test %>%\n    fabletools::model(\n      Arima4_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Arima4_predictions[1],\n    'Error' = Arima4_test_error$RMSE,\n    'Date' = Arima4_predictions[2],\n    'Forecast' = Arima4_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date    .mean\n#> 1 Arima4_model 46.96308 2024 May 196.6640\n#> 2 Arima4_model 46.96308 2024 Jun 197.5579\n#> 3 Arima4_model 46.96308 2024 Jul 198.4518\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"deterministic","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.5 Deterministic","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\nDeterministic_model <- fable::ARIMA(Difference ~  1 + pdq(d = 0))\n  \n# Calculate the error:\nDeterministic_test_error <- time_series_train %>%\n    fabletools::model(Deterministic_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nDeterministic_predictions <- time_series_test %>%\n    fabletools::model(\n      Deterministic_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Deterministic_predictions[1],\n    'Error' = Deterministic_test_error$RMSE,\n    'Date' = Deterministic_predictions[2],\n    'Forecast' = Deterministic_predictions[4]\n  )\n  \nreturn(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                .model    Error     Date    .mean\n#> 1 Deterministic_model 42.86143 2024 May 146.3409\n#> 2 Deterministic_model 42.86143 2024 Jun 146.3409\n#> 3 Deterministic_model 42.86143 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"drift","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.6 Drift","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nDrift_model <- fable::SNAIVE(Difference ~ drift())\n\n# Calculate the error:\nDrift_test_error <- time_series_train %>%\n    fabletools::model(Drift_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nDrift_predictions <- time_series_test %>%\n    fabletools::model(\n      Drift_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Drift_predictions[1],\n    'Error' = Drift_test_error$RMSE,\n    'Date' = Drift_predictions[2],\n    'Range' = Drift_predictions[3],\n    'Value' = Drift_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>        .model    Error     Date            Difference\n#> 1 Drift_model 99.09185 2024 May N(287.3684, 12520161)\n#> 2 Drift_model 99.09185 2024 Jun N(191.3684, 12520161)\n#> 3 Drift_model 99.09185 2024 Jul N(193.3684, 12520161)\n#>      .mean\n#> 1 287.3684\n#> 2 191.3684\n#> 3 193.3684\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.7 ETS1","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS1_model <-   fable::ETS(Difference ~ season() + trend())\n\n# Calculate the error:\nETS1_test_error <- time_series_train %>%\n    fabletools::model(ETS1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS1_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS1_predictions[1],\n    'Error' = ETS1_test_error$RMSE,\n    'Date' = ETS1_predictions[2],\n    'Value' = ETS1_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS1_model 42.98491 2024 May 189.8662\n#> 2 ETS1_model 42.98491 2024 Jun 189.8662\n#> 3 ETS1_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.8 ETS2","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS2_model <- fable::ETS(Difference ~ trend())\n\n# Calculate the error:\nETS2_test_error <- time_series_train %>%\n    fabletools::model(ETS2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS2_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS2_predictions[1],\n    'Error' = ETS2_test_error$RMSE,\n    'Date' = ETS2_predictions[2],\n    'Value' = ETS2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS2_model 42.98491 2024 May 189.8662\n#> 2 ETS2_model 42.98491 2024 Jun 189.8662\n#> 3 ETS2_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.9 ETS3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS3_model <- fable::ETS(Difference ~ season())\n\n# Calculate the error:\nETS3_test_error <- time_series_train %>%\n    fabletools::model(ETS3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS3_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS3_predictions[1],\n    'Error' = ETS3_test_error$RMSE,\n    'Date' = ETS3_predictions[2],\n    'Value' = ETS3_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS3_model 42.98491 2024 May 189.8662\n#> 2 ETS3_model 42.98491 2024 Jun 189.8662\n#> 3 ETS3_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.10 ETS4","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS4_model <- fable::ETS(Difference)\n\n# Calculate the error:\nETS4_test_error <- time_series_train %>%\n    fabletools::model(ETS4_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS4_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS4_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS4_predictions[1],\n    'Error' = ETS4_test_error$RMSE,\n    'Date' = ETS4_predictions[2],\n    'Value' = ETS4_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS4_model 42.98491 2024 May 189.8662\n#> 2 ETS4_model 42.98491 2024 Jun 189.8662\n#> 3 ETS4_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"holt-winters-additive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.11 Holt-Winters Additive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nHolt_Winters_Additive_model <- fable::ETS(Difference ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n\n# Calculate the error:\nHolt_Winters_Additive_test_error <- time_series_train %>%\n    fabletools::model(Holt_Winters_Additive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nHolt_Winters_Additive_predictions <- time_series_test %>%\n    fabletools::model(\n      Holt_Winters_Additive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Holt_Winters_Additive_predictions[1],\n    'Error' = Holt_Winters_Additive_test_error$RMSE,\n    'Date' = Holt_Winters_Additive_predictions[2],\n    'Value' = Holt_Winters_Additive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                        .model    Error     Date    .mean\n#> 1 Holt_Winters_Additive_model 52.74689 2024 May 467.2980\n#> 2 Holt_Winters_Additive_model 52.74689 2024 Jun 802.6268\n#> 3 Holt_Winters_Additive_model 52.74689 2024 Jul 229.5501\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"holt-winters-damped","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.12 Holt-Winters Damped","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nHolt_Winters_Damped_model <- fable::ETS(Difference ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n\n# Calculate the error:\nHolt_Winters_Damped_test_error <- time_series_train %>%\n    fabletools::model(Holt_Winters_Damped_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nHolt_Winters_Damped_predictions <- time_series_test %>%\n    fabletools::model(\n      Holt_Winters_Damped_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Holt_Winters_Damped_predictions[1],\n    'Error' = Holt_Winters_Damped_test_error$RMSE,\n    'Date' = Holt_Winters_Damped_predictions[2],\n    'Value' = Holt_Winters_Damped_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                      .model    Error     Date     .mean\n#> 1 Holt_Winters_Damped_model 481.2281 2024 May 137.36755\n#> 2 Holt_Winters_Damped_model 481.2281 2024 Jun 116.40494\n#> 3 Holt_Winters_Damped_model 481.2281 2024 Jul  97.76673\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"holt-winters-multiplicative","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.13 Holt-Winters Multiplicative","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nHolt_Winters_Multiplicative_model <- fable::ETS(Difference ~ error(\"M\") + trend(\"A\") + season(\"M\"))\n\n# Calculate the error:\nHolt_Winters_Multiplicative_test_error <- time_series_train %>%\n    fabletools::model(Holt_Winters_Multiplicative_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nHolt_Winters_Multiplicative_predictions <- time_series_test %>%\n    fabletools::model(\n      Holt_Winters_Multiplicative_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Holt_Winters_Multiplicative_predictions[1],\n    'Error' = Holt_Winters_Multiplicative_test_error$RMSE,\n    'Date' = Holt_Winters_Multiplicative_predictions[2],\n    'Value' = Holt_Winters_Multiplicative_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                              .model    Error     Date\n#> 1 Holt_Winters_Multiplicative_model 470.7401 2024 May\n#> 2 Holt_Winters_Multiplicative_model 470.7401 2024 Jun\n#> 3 Holt_Winters_Multiplicative_model 470.7401 2024 Jul\n#>      .mean\n#> 1 122.6052\n#> 2 128.6342\n#> 3 108.2321\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.14 Linear 1","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nLinear1_model <- fable::ETS(Difference ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n\n# Calculate the error:\nLinear1_test_error <- time_series_train %>%\n    fabletools::model(Linear1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nLinear1_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Linear1_predictions[1],\n    'Error' = Linear1_test_error$RMSE,\n    'Date' = Linear1_predictions[2],\n    'Value' = Linear1_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date     .mean\n#> 1 Linear1_model 481.2281 2024 May 137.36755\n#> 2 Linear1_model 481.2281 2024 Jun 116.40494\n#> 3 Linear1_model 481.2281 2024 Jul  97.76673\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.15 Linear 2","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Linear2_model <- fable::TSLM(Difference)\n  \n  # Calculate the error:\n  Linear2_test_error <- time_series_train %>%\n    fabletools::model(Linear2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Linear2_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Linear2_predictions[1],\n    'Error' = Linear2_test_error$RMSE,\n    'Date' = Linear2_predictions[2],\n    'Value' = Linear2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date    .mean\n#> 1 Linear2_model 120.6944 2024 May 146.3409\n#> 2 Linear2_model 120.6944 2024 Jun 146.3409\n#> 3 Linear2_model 120.6944 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.16 Linear 3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Linear2_model <- fable::TSLM(Difference)\n  \n  # Calculate the error:\n  Linear2_test_error <- time_series_train %>%\n    fabletools::model(Linear2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Linear2_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Linear2_predictions[1],\n    'Error' = Linear2_test_error$RMSE,\n    'Date' = Linear2_predictions[2],\n    'Value' = Linear2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date    .mean\n#> 1 Linear2_model 120.6944 2024 May 146.3409\n#> 2 Linear2_model 120.6944 2024 Jun 146.3409\n#> 3 Linear2_model 120.6944 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.17 Linear 4","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Linear4_model <- fable::TSLM(Difference ~ trend())\n  \n  # Calculate the error:\n  Linear4_test_error <- time_series_train %>%\n    fabletools::model(Linear4_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Linear4_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear4_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Linear4_predictions[1],\n    'Error' = Linear4_test_error$RMSE,\n    'Date' = Linear4_predictions[2],\n    'Value' = Linear4_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date    .mean\n#> 1 Linear4_model 87.83978 2024 May 313.7312\n#> 2 Linear4_model 87.83978 2024 Jun 317.4928\n#> 3 Linear4_model 87.83978 2024 Jul 321.2543\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"mean","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.18 Mean","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Mean_model <- fable::MEAN(Difference)\n  \n  # Calculate the error:\n  Mean_test_error <- time_series_train %>%\n    fabletools::model(Mean_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Mean_predictions <- time_series_test %>%\n    fabletools::model(\n      Mean_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Mean_predictions[1],\n    'Error' = Mean_test_error$RMSE,\n    'Date' = Mean_predictions[2],\n    'Value' = Mean_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 Mean_model 120.6944 2024 May 146.3409\n#> 2 Mean_model 120.6944 2024 Jun 146.3409\n#> 3 Mean_model 120.6944 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"naive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.19 Naive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Naive_model <- fable::NAIVE(Difference)\n  \n  # Calculate the error:\n  Naive_test_error <- time_series_train %>%\n    fabletools::model(Naive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Naive_predictions <- time_series_test %>%\n    fabletools::model(\n      Naive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Naive_predictions[1],\n    'Error' = Naive_test_error$RMSE,\n    'Date' = Naive_predictions[2],\n    'Value' = Naive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>        .model    Error     Date .mean\n#> 1 Naive_model 52.38957 2024 May   175\n#> 2 Naive_model 52.38957 2024 Jun   175\n#> 3 Naive_model 52.38957 2024 Jul   175\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.20 Neuralnet 1","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet1_model <- fable::NNETAR(Difference ~ season() + trend())\n  \n  # Calculate the error:\n  Neuralnet1_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet1_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet1_predictions[1],\n    'Error' = Neuralnet1_test_error$RMSE,\n    'Date' = Neuralnet1_predictions[2],\n    'Value' = Neuralnet1_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date      .mean\n#> 1 Neuralnet1_model 80.01379 2024 May  254.58940\n#> 2 Neuralnet1_model 80.01379 2024 Jun   31.16869\n#> 3 Neuralnet1_model 80.01379 2024 Jul -149.61350\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.21 Neuralnet 2","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet2_model <- fable::NNETAR(Difference ~ trend())\n  \n  # Calculate the error:\n  Neuralnet2_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet2_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet2_predictions[1],\n    'Error' = Neuralnet2_test_error$RMSE,\n    'Date' = Neuralnet2_predictions[2],\n    'Value' = Neuralnet2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date      .mean\n#> 1 Neuralnet2_model 53.91726 2024 May   234.1566\n#> 2 Neuralnet2_model 53.91726 2024 Jun -1894.7670\n#> 3 Neuralnet2_model 53.91726 2024 Jul -4940.7453\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.22 Neuralnet 3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet3_model <- fable::NNETAR(Difference ~ season())\n  \n  # Calculate the error:\n  Neuralnet3_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet3_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet3_predictions[1],\n    'Error' = Neuralnet3_test_error$RMSE,\n    'Date' = Neuralnet3_predictions[2],\n    'Value' = Neuralnet3_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date    .mean\n#> 1 Neuralnet3_model 32.00607 2024 May 277.4085\n#> 2 Neuralnet3_model 32.00607 2024 Jun 425.5440\n#> 3 Neuralnet3_model 32.00607 2024 Jul 249.4818\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.23 Neuralnet 4","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet3_model <- fable::NNETAR(Difference ~ season())\n  \n  # Calculate the error:\n  Neuralnet3_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet3_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet3_predictions[1],\n    'Error' = Neuralnet3_test_error$RMSE,\n    'Date' = Neuralnet3_predictions[2],\n    'Value' = Neuralnet3_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date     .mean\n#> 1 Neuralnet3_model 47.50821 2024 May 263.52302\n#> 2 Neuralnet3_model 47.50821 2024 Jun 240.83646\n#> 3 Neuralnet3_model 47.50821 2024 Jul -38.30823\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"prophet-additive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.24 Prophet Additive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Prophet_Additive_model <- fable.prophet::prophet(Difference ~ season(period = 12, type = \"additive\"))\n  \n  # Calculate the error:\n  Prophet_Additive_test_error <- time_series_train %>%\n    fabletools::model(Prophet_Additive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Prophet_Additive_predictions <- time_series_test %>%\n    fabletools::model(\n      Prophet_Additive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Prophet_Additive_predictions[1],\n    'Error' = Prophet_Additive_test_error$RMSE,\n    'Date' = Prophet_Additive_predictions[2],\n    'Value' = Prophet_Additive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                   .model   Error     Date    .mean\n#> 1 Prophet_Additive_model 98.6034 2024 May 2686.376\n#> 2 Prophet_Additive_model 98.6034 2024 Jun 3176.369\n#> 3 Prophet_Additive_model 98.6034 2024 Jul 1016.474\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"prophet-multiplicative","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.25 Prophet Multiplicative","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Prophet_Multiplicative_model <- fable.prophet::prophet(Difference ~ season(period = 12, type = \"multiplicative\"))\n  \n  # Calculate the error:\n  Prophet_Multiplicative_test_error <- time_series_train %>%\n    fabletools::model(Prophet_Multiplicative_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Prophet_Multiplicative_predictions <- time_series_test %>%\n    fabletools::model(\n      Prophet_Multiplicative_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Prophet_Multiplicative_predictions[1],\n    'Error' = Prophet_Multiplicative_test_error$RMSE,\n    'Date' = Prophet_Multiplicative_predictions[2],\n    'Value' = Prophet_Multiplicative_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                         .model    Error     Date     .mean\n#> 1 Prophet_Multiplicative_model 75.97023 2024 May -37.12521\n#> 2 Prophet_Multiplicative_model 75.97023 2024 Jun -65.45028\n#> 3 Prophet_Multiplicative_model 75.97023 2024 Jul -30.47203\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"seasonal-naive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.26 Seasonal Naive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  SNaive_model <- fable::SNAIVE(Difference)\n  \n  # Calculate the error:\n  SNaive_test_error <- time_series_train %>%\n    fabletools::model(SNaive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  SNaive_predictions <- time_series_test %>%\n    fabletools::model(\n      SNaive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = SNaive_predictions[1],\n    'Error' = SNaive_test_error$RMSE,\n    'Date' = SNaive_predictions[2],\n    'Value' = SNaive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date .mean\n#> 1 SNaive_model 98.94106 2024 May   281\n#> 2 SNaive_model 98.94106 2024 Jun   185\n#> 3 SNaive_model 98.94106 2024 Jul   187\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"stochastic","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.27 Stochastic","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Stochastic_model <- fable::ARIMA(Difference ~ pdq(d = 1), stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n  \n  # Calculate the error:\n  Stochastic_test_error <- time_series_train %>%\n    fabletools::model(Stochastic_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Stochastic_predictions <- time_series_test %>%\n    fabletools::model(\n      Stochastic_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Stochastic_predictions[1],\n    'Error' = Stochastic_test_error$RMSE,\n    'Date' = Stochastic_predictions[2],\n    'Value' = Stochastic_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date    .mean\n#> 1 Stochastic_model 42.98025 2024 May 239.7312\n#> 2 Stochastic_model 42.98025 2024 Jun 256.0506\n#> 3 Stochastic_model 42.98025 2024 Jul 241.0237\nwarnings()\nsummary_table <- data.frame()"},{"path":"ensembles-of-26-forecasting-models.html","id":"ensembles-of-26-forecasting-models","chapter":"10 Ensembles of 26 Forecasting Models","heading":"10 Ensembles of 26 Forecasting Models","text":"know make 27 individual time series forecasting models, ensemble simply puts 27 models together.","code":"\n\nlibrary(fpp3)\n#> ── Attaching packages ────────────────────────── fpp3 0.5 ──\n#> ✔ tibble      3.2.1     ✔ tsibble     1.1.4\n#> ✔ dplyr       1.1.4     ✔ tsibbledata 0.4.1\n#> ✔ tidyr       1.3.1     ✔ feasts      0.3.2\n#> ✔ lubridate   1.9.3     ✔ fable       0.3.4\n#> ✔ ggplot2     3.5.1     ✔ fabletools  0.4.2\n#> ── Conflicts ───────────────────────────── fpp3_conflicts ──\n#> ✖ lubridate::date()    masks base::date()\n#> ✖ dplyr::filter()      masks stats::filter()\n#> ✖ tsibble::intersect() masks base::intersect()\n#> ✖ tsibble::interval()  masks lubridate::interval()\n#> ✖ dplyr::lag()         masks stats::lag()\n#> ✖ tsibble::setdiff()   masks base::setdiff()\n#> ✖ tsibble::union()     masks base::union()\n\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\n# Fit the ensemble model on the training data\nEnsembles_model <- time_series_train %>%\n    fabletools::model(\n      Ensemble = (\n      fable::TSLM(Value ~ season() + trend()) +\n      fable::TSLM(Value) + fable::TSLM(Value ~ season()) +\n      fable::TSLM(Value ~ trend()) +\n      fable::ARIMA(Value ~ season() + trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Value ~ season(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Value ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +         fable::ARIMA(Value) + fable::ARIMA(Value ~ pdq(d = 1), stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ETS(Value ~ season() + trend()) + fable::ETS(Value ~ trend()) + fable::ETS(Value ~ season()) +\n      fable::ETS(Value) +\n      fable::ETS(Value ~ error(\"A\") + trend(\"A\") + season(\"A\")) + fable::ETS(Value ~ error(\"M\") + trend(\"A\") + season(\"M\")) +\n      fable::ETS(Value ~ error(\"M\") + trend(\"Ad\") + season(\"M\")) +\n      fable::MEAN(Value) +\n      fable::NAIVE(Value) +\n      fable::SNAIVE(Value) +\n      fable::SNAIVE(Value ~ drift()) +\n      fable.prophet::prophet(Value ~ season(period = 12, type = \"multiplicative\")) +\n      fable.prophet::prophet(Value ~ season(period = 12, type = \"additive\")) +\n      fable::NNETAR(Value ~ season() + trend()) +\n      fable::NNETAR(Value ~ trend()) +\n      fable::NNETAR(Value ~ season()) +\n      fable::NNETAR(Value))/26\n    )\n\n# # Make predicitons:\n# Ensemble_predictions <- time_series_test %>% \n#   model(Ensemble_model) %>%\n#     fabletools::forecast(h = number)\n\nEnsemble_predictions <- time_series_test %>%\n  fabletools::model(\n    Ensemble = (\n      fable::TSLM(Difference ~ season() + trend()) +\n      fable::TSLM(Difference) +\n      fable::TSLM(Difference ~ season()) +\n      fable::TSLM(Difference ~ trend()) +\n      fable::ARIMA(Difference ~ season() + trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Difference ~ season(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Difference ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Difference) +\n      fable::ARIMA(Difference ~ pdq(d = 1), stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ETS(Difference ~ season() + trend()) +\n      fable::ETS(Difference ~ trend()) +\n      fable::ETS(Difference ~ season()) +\n      fable::ETS(Difference) +\n      fable::ETS(Difference ~ error(\"A\") + trend(\"A\") + season(\"A\")) +\n      fable::ETS(Difference ~ error(\"M\") + trend(\"A\") + season(\"M\")) +\n      fable::ETS(Difference ~ error(\"M\") + trend(\"Ad\") + season(\"M\")) +\n      fable::MEAN(Difference) +\n      fable::NAIVE(Difference) +\n      fable::SNAIVE(Difference) +\n      fable::SNAIVE(Difference ~ drift()) +\n      fable.prophet::prophet(Difference ~ season(period = 12, type = \"multiplicative\")) +\n      fable.prophet::prophet(Difference ~ season(period = 12, type = \"additive\")) +\n      fable::NNETAR(Difference ~ season() + trend()) +\n      fable::NNETAR(Difference ~ trend()) +\n      fable::NNETAR(Difference ~ season()) +\n      fable::NNETAR(Difference)/26\n    )\n  ) %>%\n  fabletools::forecast(h = number)\n\nresults <- data.frame(\n  'Model' = Ensemble_predictions[1],\n  'Date' = Ensemble_predictions[2],\n  'Forecast' = Ensemble_predictions[4]\n)\n\nreturn(results)\n\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#> Warning in sqrt(diag(best$var.coef)): NaNs produced\n#>     .model     Date     .mean\n#> 1 Ensemble 2024 May  9937.501\n#> 2 Ensemble 2024 Jun 11192.887\n#> 3 Ensemble 2024 Jul  2437.582\nwarnings()"},{"path":"predicting-on-totally-new-data-with-individual-models-and-ensembles.html","id":"predicting-on-totally-new-data-with-individual-models-and-ensembles","chapter":"11 Predicting on totally new data with individual models and ensembles","heading":"11 Predicting on totally new data with individual models and ensembles","text":"Let’s start simple ensemble cubist, gam linear models:","code":"\nlibrary(tree) # Allows us to use tree models\nlibrary(MASS) # For the Boston Housing data set library(Metrics)\nlibrary(reactable) # For the final report - looks amazing!\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ✖ dplyr::select() masks MASS::select()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Set initial values to 0\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_RMSE <- 0\nlinear_test_predict_value <- 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_RMSE <- 0\ntree_holdout_RMSE <- 0\ntree_test_predict_value <- 0\n\nensemble_linear_RMSE <- 0\nensemble_linear_RMSE_mean <- 0\nensemble_tree_RMSE <- 0\nensemble_tree_RMSE_mean <- 0\n\nnumerical_1 <- function(data, colnum, train_amount, test_amount, numresamples, do_you_have_new_data = c(\"Y\", \"N\")){\n\n# Move target column to far right\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Set up resampling\nfor (i in 1:numresamples) {\n  idx <- sample(seq(1, 2), size = nrow(data), replace = TRUE, prob = c(train_amount, test_amount))\n  train <- data[idx == 1, ]\n  test <- data[idx == 2, ]\n\n# Fit linear model on the training data, make predictions on the test data\nlinear_train_fit <- lm(y ~ ., data = train)\nlinear_predictions <- predict(object = linear_train_fit, newdata = test)\nlinear_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = linear_predictions)\nlinear_RMSE_mean <- mean(linear_RMSE)\n\n# Fit tree model on the training data, make predictions on the test data\ntree_train_fit <- tree(y ~ ., data = train)\ntree_predictions <- predict(object = tree_train_fit, newdata = test)\ntree_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = tree_predictions)\ntree_RMSE_mean <- mean(tree_RMSE)\n\n# Make the weighted ensemble\nensemble <- data.frame(\n  'linear' = linear_predictions / linear_RMSE_mean,\n  'tree' = tree_predictions / tree_RMSE_mean,\n  'y_ensemble' = test$y)\n\n# Split ensemble between train and test\nensemble_idx <- sample(seq(1, 2), size = nrow(ensemble), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble[ensemble_idx == 1, ]\nensemble_test <- ensemble[ensemble_idx == 2, ]\n\n# Fit the ensemble data on the ensemble training data, predict on ensemble test data\nensemble_linear_train_fit <- lm(y_ensemble ~ ., data = ensemble_train)\n\nensemble_linear_predictions <- predict(object = ensemble_linear_train_fit, newdata = ensemble_test)\n\nensemble_linear_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_linear_predictions)\n\nensemble_linear_RMSE_mean <- mean(ensemble_linear_RMSE)\n\n# Fit the tree model on the ensemble training data, predict on ensemble test data\nensemble_tree_train_fit <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree_train_fit, newdata = ensemble_test) \n\nensemble_tree_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_tree_predictions)\n\nensemble_tree_RMSE_mean <- mean(ensemble_tree_RMSE)\n\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble_Linear', 'Ensemble_tree'),\n  'Error_Rate' = c(linear_RMSE_mean, tree_RMSE_mean, ensemble_linear_RMSE_mean, ensemble_tree_RMSE_mean)\n)\n\nresults <- results %>% arrange(Error_Rate)\n\n} # Closing brace for numresamples\n\nif (do_you_have_new_data == \"Y\") {\n  new_data <- read.csv('/Users/russellconte/NewBoston.csv', header = TRUE, sep = ',')\n\n  y <- 0\n  colnames(new_data)[colnum] <- \"y\"\n\n  new_data <- new_data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n}\n  \n  new_linear <- predict(object = linear_train_fit, newdata = new_data)\n  new_tree <- predict(object = tree_train_fit, newdata = new_data)\n\n  new_ensemble <- data.frame(\n    \"linear\" = new_linear / linear_RMSE_mean,\n    \"tree\" = new_tree / tree_RMSE_mean\n    )\n\n  new_ensemble$Row_mean <- rowMeans(new_ensemble)\n  new_ensemble$y_ensemble <- new_data$y\n\n  new_ensemble_linear <- predict(object = ensemble_linear_train_fit, newdata = new_ensemble)\n  new_ensemble_tree <- predict(object = ensemble_tree_train_fit, newdata = new_ensemble)\n\n  new_data_results <-\n    data.frame(\n      \"True_Value\" = new_ensemble$y_ensemble,\n      \"Linear\" = round(new_linear, 4),\n      \"Tree\" = round(new_tree, 4),\n      \"Ensemble_Linear\" = round(new_ensemble_linear, 4),\n      \"Ensemble_Tree\" = round(new_ensemble_tree, 4)\n    )\n\n  df1 <- t(new_data_results)\n\n  predictions_of_new_data <- reactable::reactable(\n    data = df1, searchable = TRUE, pagination = FALSE, wrap = TRUE, rownames = TRUE, fullWidth = TRUE, filterable = TRUE, bordered = TRUE,\n    striped = TRUE, highlight = TRUE, resizable = TRUE\n  ) %>%\n    \n    reactablefmtr::add_title(\"Predictions of new data\")\n  \n  results <- reactable::reactable(\n    data = results, searchable = TRUE, pagination = FALSE, wrap = TRUE, rownames = TRUE, fullWidth = TRUE, filterable = TRUE, bordered = TRUE, striped = TRUE, highlight = TRUE, resizable = TRUE\n  ) %>% \n    reactablefmtr::add_title(\"Model and error rates\")\n\nreturn(list(results, predictions_of_new_data))\n\n} # Closing brace for the function\n\nnumerical_1(data = Ensembles::Boston_Housing, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25, do_you_have_new_data = \"Y\")\n#> Registered S3 method overwritten by 'tsibble':\n#>   method          from\n#>   format.interval inum\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n#> [[1]]\n#> \n#> [[2]]\n\n# Note these results show up in the Viewer."},{"path":"how-to-communicate-your-results.html","id":"how-to-communicate-your-results","chapter":"12 How to communicate your results","heading":"12 How to communicate your results","text":"chapter, going discuss, presenting results people various levels organization. include people ranging management, analysts, people C-Suite. elements common presenting results, elements going specific, depending person’s responsibilities include.’ve opportunity manage multi-million dollar accounts fortune 1000 company, ’ve also run two volunteer nonprofits, chapter Amnesty International, Chicago Apple User Group.work also includes several nonprofit social service organizations. ’ve also run vacation rental business, ’ve done lot volunteer work. Therefore, can speak wide range experience business needs.","code":""},{"path":"how-to-communicate-your-results.html","id":"a-very-basic-introduction-to-financial-reporting","chapter":"12 How to communicate your results","heading":"12.1 A very basic introduction to financial reporting","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"always-give-the-best-service-you-can-possibly-do-the-ritz-carlton-method","chapter":"12 How to communicate your results","heading":"12.1.1 Always give the best service you can possibly do: The Ritz Carlton method","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"help-your-manager-make-the-best-possible-decisions","chapter":"12 How to communicate your results","heading":"12.2 Help your manager make the best possible decisions","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"have-empathy-for-your-managersand-customers-situations","chapter":"12 How to communicate your results","heading":"12.3 Have empathy for your manager’s—and customer’s— situations","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"dont-need-them-in-any-way","chapter":"12 How to communicate your results","heading":"12.3.1 don’t need them in any way","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"have-a-great-story-to-tellhow-to-create-a-great-story","chapter":"12 How to communicate your results","heading":"12.3.2 Have a great story to tell—How to create a great story","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"br-prepared-with-strong-counterexamples","chapter":"12 How to communicate your results","heading":"12.3.3 Br prepared with strong counterexamples","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"include-uncertainty","chapter":"12 How to communicate your results","heading":"12.3.4 include uncertainty","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"tell-the-hard-truth-prepare-them-for-criticism","chapter":"12 How to communicate your results","heading":"12.3.5 tell the (hard) truth, prepare them for criticism","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"communicating-results-with-staff-who-have-profit-and-loss-responsibility","chapter":"12 How to communicate your results","heading":"12.4 Communicating results with staff who have Profit and Loss responsibility","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"vision-mission-values---what-is-the-central-question","chapter":"12 How to communicate your results","heading":"12.4.1 Vision, mission, values - what is the central question?","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"financials","chapter":"12 How to communicate your results","heading":"12.4.2 Financials","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"the-higher-up-the-org-chart-the-more-you-will-be-working-with-vision-stragety-financials","chapter":"12 How to communicate your results","heading":"12.4.3 The higher up the org chart, the more you will be working with vision, stragety, financials","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"communicating-with-customers-and-vendors","chapter":"12 How to communicate your results","heading":"12.5 Communicating with customers and vendors","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"communicating-roi-and-other-results-of-data-science","chapter":"12 How to communicate your results","heading":"12.6 Communicating ROI and other results of data science","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"social-media-and-more","chapter":"12 How to communicate your results","heading":"12.7 Social media and more","text":"","code":""},{"path":"final-comprhensive-project.html","id":"final-comprhensive-project","chapter":"13 Final Comprhensive Project","heading":"13 Final Comprhensive Project","text":"text.","code":""},{"path":"summary-and-charge-to-the-graduates-of-the-class.html","id":"summary-and-charge-to-the-graduates-of-the-class","chapter":"14 Summary and Charge to the Graduates of the Class!","heading":"14 Summary and Charge to the Graduates of the Class!","text":"text","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":" Introduction\nPhoto City Chicago snow plow stuck snow: Victorgrigas English Wikipedia - (t3xt (talk)) created work entirely ., CC0, Link","code":""}]

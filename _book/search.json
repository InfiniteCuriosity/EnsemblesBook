[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"Welcome Ensembles! book guide entire process building ensemble models beginning end. also give full access Ensembles package automates entire process.’ve done best make book interesting, fun, practical. lots examples using real world data steps included.able wonderful things complete skills book. book show, ensembles much accurate method help us understand model nature. done level accuracy achieved previously. can .phrase “wonderful things” intentional. Howard Carter archaeology, one point November, 1922, quite sure found something important. Carter made small hole see . Lord Carnarvon (paying !) asked Howard Carter, “Can see anything?. Howard Carter’s famous reply,”Yes, wonderful things!“. opened everything , found intact tomb Tutankhamun. contained 5,000 items, enriched knowledge ancient Africa beyond find.tiny taste one 5,000 “wonderful things” found Howard Carter, Lord Carnarvon, team archaeologists.best share many “wonderful things” entire book explore world ensembles.Ensembles package ’ve made entire analysis process automatically. put power ensembles hands, give strongest foundation work, highest degree accuracy.examples book come real data. example (many examples book):• HR Analytics• Predicting winning time London Marathon• World’s accurate score difficult classification problem• Beat best score student Kaggle competitionsWe many practical examples wide range fields enjoy.book show ensembles improve understanding nature, can use ensembles work. results using ensembles much accurate ever possible , demonstrated book. able use ensembles understand world, build models data, level accuracy achieved .","code":""},{"path":"index.html","id":"ensembles-the-new-ai-from-beginner-to-expert","chapter":"1 Welcome!","heading":"1.1 Ensembles: The New AI, from beginner to expert","text":"see, Ensembles new AI. Science gone calculus Newton Leibnetz, differential equations, modern world creating models, many points -. Ensembles powerful way put models together achieve best possible results. book guide process, show can build ensembles pass testing.new AI. Welcome path, ’s extremely fun, look forward sharing !","code":""},{"path":"index.html","id":"what-you-will-be-able-to-do-by-the-end-of-the-book","chapter":"1 Welcome!","heading":"1.2 What you will be able to do by the end of the book","text":"• Make customized ensembles models numerical, classification, logistic time series data.• Use Ensembles package entire process automatically (little customization possible).• Make ensemble solutions packages can shared users.• Make ensemble solutions totally self-contained solutions can shared anyone.• Learn ensembles models can help make wisest possible decision based data.• Learn present results different levels, regular user CEO board directors.• present results social media friendly.• Find data create ensemble solution beginning end (called One chapter exercises)• Solve real world examples book ensembles achieve results :• Beat top score student data science competition 90% (numerical ensembles).• Correctly predict winning time 2024 Men’s London Marathon (time series ensembles).• Produce 100% accurate solution dry beans classification problem (first world data set, done using classification ensembles).• Make recommendations Lebron James can improve performance basketball court (logistic ensembles).• Complete comprehensive Final Project put new skills ensembles together. result can shared employers, advisors, social media, job interviews, anywhere else like share work.","code":""},{"path":"index.html","id":"how-this-book-is-organized-so-you-learn-the-material-as-easily-as-possible","chapter":"1 Welcome!","heading":"1.3 How this book is organized so you learn the material as easily as possible","text":"book begins foundations making ensembles models. look :• Individual numerical models• Ensembles numerical models• Individual classification models• Ensembles classification models• Individual logistic models• Ensembles logistic models• Individual forecasting models• Ensembles forecasting models• Advanced data visualizations• Multiple ways communicate results. range people field, customers, C-Suite (CEO, CTO, board directors, etc.)• look treat data science business. particular pay close attention showing return investment (ROI) data science, using ensembles models.• book conclude showing four examples final comprehensive project. one example numerical data, classification data, logistic data forecasting data. example professionally formatted. source files eight files available github repository.","code":""},{"path":"index.html","id":"how-you-can-learn-the-skills-as-fast-as-possible-how-the-exercises-are-organized","chapter":"1 Welcome!","heading":"1.4 How you can learn the skills as fast as possible: How the exercises are organized","text":"young child, learned much better retention system always called delayed repetition. means learn best fastest see worked example, several practice examples, repeat delay time. delay can range hour days.example, exercises Individual Classification Models chapter ask build models using techniques classification models prior chapters. exercises logistic ensembles ask build models content logistic models chapter, previous chapters. experience repeating fastest way learn new content, retain longest period time.time get Final Comprehensive Project, skills sharp modeling techniques.","code":""},{"path":"index.html","id":"going-from-student-to-teacher-you-are-required-to-post-on-social-media-and-help-others-understand-the-results","chapter":"1 Welcome!","heading":"1.5 Going from student to teacher: You are required to post on social media and help others understand the results","text":"One important parts role data science communicating findings. present many examples summaries reports adapt use projects. also required post results social media. may use appropriate choice social media, needs publicly available. number important benefits :• build body work shows skill level• results demonstrate ability communicate way works wide variety people• work demonstrate good skills video /audio production• Use hashtag #AIEnsembles post social media","code":""},{"path":"index.html","id":"helping-you-use-the-power-of-pre-trained-ensembles-and-individual-models","chapter":"1 Welcome!","heading":"1.6 Helping you use the power of pre-trained ensembles and individual models","text":"Another important part skills learn includes building pre-trained ensembles models. book walk process building pre-trained models ensembles four types data (numerical, classification, logicial, time series).","code":""},{"path":"index.html","id":"helping-you-master-the-material-one-of-your-own-exercises","chapter":"1 Welcome!","heading":"1.7 Helping you master the material: One of your own exercises","text":"One differences exercises Ensembles inclusion One exercises. set exercises include one asks find data (many hints given help find data), define problem, make ensemble, report results.","code":""},{"path":"index.html","id":"keeping-it-real-actual-business-data-and-problems-as-the-source-of-all-the-data-sets","chapter":"1 Welcome!","heading":"1.8 Keeping it real: Actual business data and problems as the source of all the data sets","text":"data sets book use real data. exceptions, synthetic data. sources data cited, real world implications can found simple search. data absolutely real.","code":""},{"path":"index.html","id":"check-your-biases-test-your-model-on-a-neutral-data-set","chapter":"1 Welcome!","heading":"1.9 Check your biases: Test your model on a neutral data set","text":"set exercises ask check one trained models neutral data set. model biases, reveal . ’ll knowledge go back address biases models.","code":""},{"path":"index.html","id":"helping-you-check-your-workand-verifying-that-your-results-beat-previously-published-results","chapter":"1 Welcome!","heading":"1.10 Helping you check your work—and verifying that your results beat previously published results","text":"Many data sets solved previous investigators (competitions), results can easily compared published results.example, look Boston Housing data set look numerical data sets. data set used many times Kaggle competitions, published papers, Github repositories, among many sources.Ensembles package automatically solve data set, return RMSE less 0.20 (slight variation depending parameters set, explained chapters). comparison, Boston Housing data set used Kaggle student competition: https://www.kaggle.com/competitions/uou-g03784-2022-spring/leaderboard?tab=public, best score 2.09684. Ensembles package beat best result Kaggle student competition 90%. Ensembles package requires one line code.","code":""},{"path":"index.html","id":"helping-you-work-as-a-team-with-fully-reproducible-ensembles-and-individual-models","chapter":"1 Welcome!","heading":"1.11 Helping you work as a team with fully reproducible ensembles and individual models","text":"large part skills learn include make results reproducible. include:• Multiple random resamplings data• Learning test totally unseen data individual ensemble models• repeat results (example, 25 times), report accuracy resamplingFor example, make ensembles models, use trained models make predictions totally unseen data.","code":""},{"path":"index.html","id":"the-final-comprehensive-project-will-put-everything-together-for-you","chapter":"1 Welcome!","heading":"1.12 The Final Comprehensive Project will put everything together for you","text":"studying data science, one professors said papers turned “good enough show CEO Board Directors” Fortune 1000 company worked . chapter Final Comprehensive Project share highest level skills following:• Truly understanding business problem• able convey high value data science brings table• able back 100% claims rock solid evidence, facts, clear reasoning• make truly professional quality presentation worthy C-SuiteI’ve incredible pleasure learning many different skills. include able play 20 musical instruments, communicate three languages professional level, manage multi-million dollar division Fortune 1000 company, run two non-profit volunteer groups, snowboard three mile run Colorado, work professional counselor, much . book reading recent project. None skills acquired overnight. huge part success able make slow (usually) steady progress. next chapter reveal big secret getting results, now best plan regular time work contents book.Always remember test everything, save ton problems road.","code":""},{"path":"index.html","id":"exercises-to-help-improve-your-skills","chapter":"1 Welcome!","heading":"1.13 Exercises to help improve your skills","text":"Exercise 1: Schedule regular time work bookYou gain much progress work steady pace. Take everything small pieces. ’s OK go slow, long keep going. Schedule regular time work book, get largest possible reward efforts.Exercise 2: Read chapter least twice begin working material.Reading chapter twice begin working actually speed progress results. actually take less time complete chapter. might believe right now, ’s totally true.Exercise 2a: Read chapter ahead able .Exercise 3: Read chapter ","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"introduction-and-your-first-ensembles","chapter":"2 Introduction and your first ensembles","heading":"2 Introduction and your first ensembles","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"how-a-chicago-blizzard-led-to-the-very-unlikely-story-of-the-best-solutions-to-supervised-data","chapter":"2 Introduction and your first ensembles","heading":"2.1 How a Chicago blizzard led to the very unlikely story of the best solutions to supervised data","text":"journey advanced AI world started actual\nblizzard Chicago. might seem like Chicago never get \nblizzard, 2011, incredibly intense, \nvideo shows:https://www.youtube.com/watch?v=cPiFn52ztd8What Chicago 2011\nSnomageddon\ncreation advanced AI? Everything. ’s\nstory.time 2011 Blizzard worked Recruiter Kelly\nServices, \nworked since 1996. agreed work Kelly Services office \nFrankfort, Illinois time, though worked nearly every\nKelly Services office one time another. trip Frankfort\ninvolved daily commute office, able make best\nuse time road.manager time let know several days advance \nlarge amount snow forecast, might want \nprepared. recent forecasts large amounts snow \nChicago area amounted nothing. perfectly normal days \nChicago area, predicted storm also nothing, based\nrecent results. great example prior\nprediction transferring well current situation.morning went work normal, even look \nweather forecast. Around 2:45 pm manager came office \nsaid “Russ, need come look weather radar!”. \nwalked office, saw map winter storm \nincredibly huge. image zoomed , possible see\nseveral states. tell, massive snow storm \nbarreling Chicago, 15 minutes away \nlocation.told candidate interviewing leaving immediately,\nallowed stay. get home fast \npossible safety.storm started dropping snow trip north back home. commute\ntook around 50% longer normal due rapidly falling snow.later learned, storm forecast start Chicago area\naround 3:00 pm, finish 11:00 - 1:00 pm two days later, \nleave 17 - 19 inches snow.bad ? Even City Chicago snow plows stopped \nsnow:see forecasts looked like, check news report \nday:https://www.nbcchicago.com/news/local/blizzard-unleashes-winter-fury/2096753/turns three predictions blizzard accurate \nlevel almost seemed uncanny : Start time, accumulation, \nend time spot . first time recall ever seeing \nprediction level accuracy. idea type \npredictive accuracy even possible. level accuracy \npredicting results totally blew away. never seen anything \nlevel accuracy, now wanted know done.searched searched accuracy high \nforecast.power method—whatever —obvious . realized\nwork weather, solution method work \nincredibly broad range situations. many areas\ninclude business forecasts, production work, modeling prices, much,\nmuch . point idea accurate prediction\ndone.months later person wrote Tom\nSkilling, chief\nmeteorologist WGN TV Chicago. Tom posted answer opened \nsolution . relevant part Tom Skilling’s\nanswer \n2011 storm forecast accurate:Weather Service developed interesting “SNOWFALL ENSEMBLE\nFORECAST PROBABILITY SYSTEM” draws upon wide range snow\naccumulation forecasts whole set different computer models.\n“blending” model projections, probability snowfalls\nfalling within certain ranges becomes possible. Also, “blending”\nmultiple forecasts “smooths” sometimes huge model disparities\namounts predicted. resulting probabilities therefore\nrepresent “best case” forecast.first step. Ensembles way achieved \nextraordinary prediction accuracy.next goal figure ensembles made. looked \ninformation, became obvious ensembles used ,\nwinning entry Netflix Prize Competition:Netflix Prize\nCompetition sponsored\nNetflix create method accurately predict user ratings \nfilms. minimum winning score needed beat Netflix method\n(named Cinematch) least 10%. Several years work went \nsolving problem, results even included several published\npapers. winning solution ensemble methods beat \nCinematch results 10.09%.now clear ensembles path forward. However,\nidea make ensembles.went graduate school study data science predictive\nanalytics. degree completed 2017, Northwestern\nUniversity. However, still sure ensembles models \nbuilt, find clear methods build (except \npre-made methods, random forests). true \npackages work, nothing found \nlooking : build ensembles models general. Despite\nplaying idea looking online, able build \nensembles wanted build.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"saturday-october-15-2022-at-458-pm.-the-exact-birth-of-the-ensembles-system","chapter":"2 Introduction and your first ensembles","heading":"2.2 Saturday, October 15, 2022 at 4:58 pm. The exact birth of the Ensembles system","text":"Everything changed Saturday, October 15, 2022 4:58 pm. \nplaying various methods make ensemble, got ensemble\nworked first time. results extremely\nmodest standards, clear foundation \nbuild general solution can work extremely wide\nrange areas. journal entry:might asking know day time. \nreasonable question. ’ve keeping journal since 19 years\nold, thousands entries. soon realized \ncorrectly build ensembles, made entry, contains key\nelements make ensemble, steps just \nmoment. Notice subject line journal matches text\n.One ways improve skills keep journal, ’ll\nlooking depth chapter future chapters.\njournal use MacJournal, though \nlarge number options available market.Birth ensembles, Saturday, October 15, 2022 4:58 pm","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"here-is-what-an-ensemble-of-models-looks-like-at-the-most-basic-level-using-the-boston-housing-data-set-as-an-example","chapter":"2 Introduction and your first ensembles","heading":"2.3 Here is what an ensemble of models looks like at the most basic level, using the Boston Housing data set as an example:","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"head-of-boston-housing-data-set","chapter":"2 Introduction and your first ensembles","heading":"2.3.1 Head of Boston Housing data set","text":"start first ensemble data set numerical\nvalues. first example use Boston Housing data set, \nMASS package. Boston Housing data set controversial (\ndiscuss controversies example making\nprofessional quality reports C-Suite), now works \nwell known data set begin journey ensembles.Overview basic steps make ensemble:using Boston Housing data set, let’s look \nBoston images:","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"the-steps-to-build-your-first-ensemble-from-scratch","chapter":"2 Introduction and your first ensembles","heading":"2.4 The steps to build your first ensemble from scratch","text":"Load packages need (MASS, tree)Load packages need (MASS, tree)Load Boston Housing data set, split train (60%) \ntest (40%) sections.Load Boston Housing data set, split train (60%) \ntest (40%) sections.Create linear model fitting linear model training\ndata, make predictions Boston Housing test data. Measure\naccuracy predictions actual values.Create linear model fitting linear model training\ndata, make predictions Boston Housing test data. Measure\naccuracy predictions actual values.Create model using trees fitting tree model training\ndata, making predictions Boston Housing test data.\nMeasure accuracy predictions actual values.Create model using trees fitting tree model training\ndata, making predictions Boston Housing test data.\nMeasure accuracy predictions actual values.Make new data frame. ensemble model\npredictions. One column linear predictions, one \ntree predictions.Make new data frame. ensemble model\npredictions. One column linear predictions, one \ntree predictions.Make new column true values—true values \nBoston Housing test data setMake new column true values—true values \nBoston Housing test data setOnce new ensemble data set, ’s simply another data\nset. different many ways data set (except \nmade).new ensemble data set, ’s simply another data\nset. different many ways data set (except \nmade).Break ensemble data set train (60%) test (40%)\nsections.Break ensemble data set train (60%) test (40%)\nsections.Fit linear model ensemble training data. Make predictions\nusing testing data, measure accuracy predictions\ntest data.Fit linear model ensemble training data. Make predictions\nusing testing data, measure accuracy predictions\ntest data.Summarize results.Summarize results.suggest reading basic steps make ensemble\ncouple times, make sure familiar steps.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"building-the-first-actual-ensemble","chapter":"2 Introduction and your first ensembles","heading":"2.5 Building the first actual ensemble","text":"Load packages need (MASS, tree):Load Boston Housing data set, split train (60%) test\n(40%) sections.Create linear model fitting linear model training data,\nmake predictions Boston Housing test data. Measure \naccuracy predictions actual values.Calculate error modelThe error rate linear model 6.108005. Let’s using\ntree method.Create model using trees fitting tree model training\ndata, making predictions Boston Housing test data. Measure\naccuracy predictions actual values.Calculate error rate tree model:error rate tree model lower (better). error\nrate tree model 5.478017.","code":"\nlibrary(MASS) # for the Boston Housing data set\nlibrary(tree) # To make models using trees\nlibrary(Metrics) # To calculate error rate (root mean squared error)\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ✖ dplyr::select() masks MASS::select()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\ndf <- MASS::Boston\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\n# Let's have a quick look at the train and test sets\nhead(train)\n#>      crim zn indus chas   nox    rm  age    dis rad tax\n#> 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296\n#> 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242\n#> 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242\n#> 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222\n#> 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222\n#> 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222\n#>   ptratio  black lstat medv\n#> 1    15.3 396.90  4.98 24.0\n#> 2    17.8 396.90  9.14 21.6\n#> 3    17.8 392.83  4.03 34.7\n#> 4    18.7 394.63  2.94 33.4\n#> 5    18.7 396.90  5.33 36.2\n#> 6    18.7 394.12  5.21 28.7\nhead(test)\n#>         crim zn indus chas   nox    rm   age    dis rad tax\n#> 401 25.04610  0  18.1    0 0.693 5.987 100.0 1.5888  24 666\n#> 402 14.23620  0  18.1    0 0.693 6.343 100.0 1.5741  24 666\n#> 403  9.59571  0  18.1    0 0.693 6.404 100.0 1.6390  24 666\n#> 404 24.80170  0  18.1    0 0.693 5.349  96.0 1.7028  24 666\n#> 405 41.52920  0  18.1    0 0.693 5.531  85.4 1.6074  24 666\n#> 406 67.92080  0  18.1    0 0.693 5.683 100.0 1.4254  24 666\n#>     ptratio  black lstat medv\n#> 401    20.2 396.90 26.77  5.6\n#> 402    20.2 396.90 20.32  7.2\n#> 403    20.2 376.11 20.31 12.1\n#> 404    20.2 396.90 19.77  8.3\n#> 405    20.2 329.46 27.38  8.5\n#> 406    20.2 384.97 22.98  5.0\nBoston_lm <- lm(medv ~ ., data = train) # Fit the model to the training data\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the model predictions\nhead(Boston_lm_predictions)\n#>       401       402       403       404       405       406 \n#> 12.618507 19.785728 20.919370 13.014507  6.946392  5.123039\nBoston_linear_RMSE <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions)\nBoston_linear_RMSE\n#> [1] 6.108005\nBoston_tree <- tree(medv ~ ., data = train) # Fit the model to the training data\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\n\n# Let's have a quick look at the predictions:\nhead(Boston_tree_predictions)\n#>      401      402      403      404      405      406 \n#> 13.30769 13.30769 13.30769 13.30769 13.30769 13.30769\nBoston_tree_RMSE <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions)\nBoston_tree_RMSE\n#> [1] 5.478017"},{"path":"introduction-and-your-first-ensembles.html","id":"were-ready-to-make-our-first-ensemble","chapter":"2 Introduction and your first ensembles","heading":"2.6 We’re ready to make our first ensemble!!","text":"Make new data frame. ensemble model predictions,\none column true values. One column linear\npredictions, one tree predictions. ’ll make third\ncolumn, true values.Make new column true values—true values \nBoston Housing test data setOnce new ensemble data set, ’s simply another data set. \ndifferent many ways data set (except made).Break ensemble data set train (60%) test (40%) sections.\nnothing special 60/40 split , may use \nnumbers wish.Fit linear model ensemble training data. Make predictions using\ntesting data, measure accuracy predictions \ntest data. Notice similar linear tree models.Summarize results.Clearly ensemble lowest error rate three models. \nensemble easily best three models \nlowest error rate models.","code":"\nensemble <- data.frame(\n  'linear' = Boston_lm_predictions,\n  'tree' = Boston_tree_predictions,\n  'y' = test$medv\n)\n\n# Let's have a look at the ensemble:\nhead(ensemble)\n#>        linear     tree    y\n#> 401 12.618507 13.30769  5.6\n#> 402 19.785728 13.30769  7.2\n#> 403 20.919370 13.30769 12.1\n#> 404 13.014507 13.30769  8.3\n#> 405  6.946392 13.30769  8.5\n#> 406  5.123039 13.30769  5.0\ndim(ensemble)\n#> [1] 105   3\nensemble_train <- ensemble[1:60, ]\nensemble_test <- ensemble[61:105, ]\n\nhead(ensemble_train)\n#>        linear     tree    y\n#> 401 12.618507 13.30769  5.6\n#> 402 19.785728 13.30769  7.2\n#> 403 20.919370 13.30769 12.1\n#> 404 13.014507 13.30769  8.3\n#> 405  6.946392 13.30769  8.5\n#> 406  5.123039 13.30769  5.0\nhead(ensemble_test)\n#>       linear     tree    y\n#> 461 23.88984 13.30769 16.4\n#> 462 23.29129 13.30769 17.7\n#> 463 22.54055 21.84327 19.5\n#> 464 25.50940 21.84327 20.2\n#> 465 22.71231 21.84327 21.4\n#> 466 20.83810 21.84327 19.9\n# Fit the model to the training data\nensemble_lm <- lm(y ~ ., data = ensemble_train)\n\n# Make predictions using the model on the test data\nensemble_lm_predictions <- predict(object = ensemble_lm, newdata = ensemble_test)\n\n# Calculate error rate for the ensemble predictions\nensemble_lm_rmse <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_lm_predictions)\n\n# Report the error rate for the ensemble\nensemble_lm_rmse\n#> [1] 4.826962\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble'),\n  'Error' = c(Boston_linear_RMSE, Boston_tree_RMSE, ensemble_lm_rmse)\n)\n\nresults\n#>      Model    Error\n#> 1   Linear 6.108005\n#> 2     Tree 5.478017\n#> 3 Ensemble 4.826962"},{"path":"introduction-and-your-first-ensembles.html","id":"try-it-yourself-make-an-ensemble-where-the-ensemble-is-made-using-trees-instead-of-linear-models.","chapter":"2 Introduction and your first ensembles","heading":"2.6.1 Try it yourself: Make an ensemble where the ensemble is made using trees instead of linear models.","text":"compare three results? Let’s update \nresults table","code":"\n# Fit the model to the training data\nensemble_tree <- tree(y ~ ., data = ensemble_train)\n\n# Make predictions using the model on the test data\nensemble_tree_predict <- predict(object = ensemble_tree, newdata = ensemble_test)\n\n# Let's look at the predictions\nhead(ensemble_tree_predict)\n#>      461      462      463      464      465      466 \n#> 14.80000 14.80000 18.94286 18.94286 18.94286 18.94286\n\n# Calculate the error rate\nensemble_tree_rmse <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_tree_predict)\n\nensemble_tree_rmse\n#> [1] 5.322011\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble_Linear', 'Ensemble_Tree'),\n  'Error' = c(Boston_linear_RMSE, Boston_tree_RMSE, ensemble_lm_rmse, ensemble_tree_rmse)\n)\n\nresults <- results %>% arrange(Error)\n\nresults\n#>             Model    Error\n#> 1 Ensemble_Linear 4.826962\n#> 2   Ensemble_Tree 5.322011\n#> 3            Tree 5.478017\n#> 4          Linear 6.108005"},{"path":"introduction-and-your-first-ensembles.html","id":"both-of-the-ensemble-models-beat-both-of-the-individual-models-in-this-example","chapter":"2 Introduction and your first ensembles","heading":"2.6.2 Both of the ensemble models beat both of the individual models in this example","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"principle-what-is-one-improvement-that-can-be-made-use-a-diverse-set-of-models-and-ensembles-to-get-the-best-possible-result","chapter":"2 Introduction and your first ensembles","heading":"2.7 Principle: What is one improvement that can be made? Use a diverse set of models and ensembles to get the best possible result","text":"shall see go learn build ensembles, \nnumerical method use build 27 individual models 13\nensembles total 40 results. goal get best\npossible results, diverse set models ensembles, 40\nresults numerical data, produce much better results \nlimited number models ensembles.principal looking classification\ndata, logistic, data, time series forecasting data. use \nlarge number individual models ensembles goal \nachieving best possible result.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"principle-randomizing-the-data-before-the-analysis-will-make-the-results-more-general-and-is-very-easy-to-do","chapter":"2 Introduction and your first ensembles","heading":"2.8 Principle: Randomizing the data before the analysis will make the results more general (and is very easy to do!)","text":"","code":"\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis"},{"path":"introduction-and-your-first-ensembles.html","id":"try-it-yourself-repeat-the-previous-analysis-but-randomize-the-rows-before-the-analysis.-otherwise-keep-the-process-the-same.-share-your-results-on-social-media.","chapter":"2 Introduction and your first ensembles","heading":"2.9 Try it yourself: Repeat the previous analysis, but randomize the rows before the analysis. Otherwise keep the process the same. Share your results on social media.","text":"’ll follow exact steps, except randomizing rows\nfirst.• Randomize rows• Break data train test sets• Fit model training set• Make predictions calculate error model test setThe fact results bit different first ensemble \nuseful. gives us another solid principle use analysis\nmethods:","code":"\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis\n\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\n# Fit the model to the training data\nBoston_lm <- lm(medv ~ ., data = train)\n\n# Make predictions using the model on the test data\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the linear model predictions:\n\nhead(Boston_lm_predictions)\n#>      311      429      258       91      498      128 \n#> 18.32366 14.22723 42.37954 26.94967 18.94004 15.20354\nBoston_linear_rmse <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions)\n\nBoston_tree <- tree(medv ~ ., data = train)\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\nBoston_tree_rmse <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions)\n\n# Let's have a quick look at the tree model predictions:\n\nhead(Boston_tree_predictions)\n#>      311      429      258       91      498      128 \n#> 21.72692 10.57826 47.08421 21.72692 21.72692 15.42821\nensemble <- data.frame( 'linear' = Boston_lm_predictions, 'tree' = Boston_tree_predictions, 'y_ensemble' = test$medv )\n\nensemble <- ensemble[sample(nrow(ensemble)), ] # Randomizes the rows of the ensemble\n\nensemble_train <- ensemble[1:60, ]\nensemble_test <- ensemble[61:105, ]\nensemble_lm <- lm(y_ensemble ~ ., data = ensemble_train)\n\n# Predictions for the ensemble linear model\n\nensemble_prediction <- predict(ensemble_lm, newdata = ensemble_test)\n\n# Root mean squared error for the ensemble linear model\n\nensemble_lm_rmse <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_prediction)\n\n# Same for tree models\n\nensemble_tree <- tree(y_ensemble ~ ., data = ensemble_train)\nensemble_tree_predictions <- predict(object = ensemble_tree, newdata = ensemble_test)\nensemble_tree_rmse <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_tree_predictions)\n\nresults <- list( 'Linear' = Boston_linear_rmse, 'Trees' = Boston_tree_rmse, 'Ensembles_Linear' = ensemble_lm_rmse, 'Ensemble_Tree' = ensemble_tree_rmse )\n\nresults\n#> $Linear\n#> [1] 3.793948\n#> \n#> $Trees\n#> [1] 5.02707\n#> \n#> $Ensembles_Linear\n#> [1] 4.823033\n#> \n#> $Ensemble_Tree\n#> [1] 4.103466"},{"path":"introduction-and-your-first-ensembles.html","id":"the-more-we-can-randomize-the-data-the-more-our-results-will-match-nature","chapter":"2 Introduction and your first ensembles","heading":"2.10 The more we can randomize the data, the more our results will match nature","text":"Just watch: Repeat results 100 times, return mean results\n(hint: ’s two small changes)","code":"\nfor (i in 1:100) {\n\n# First the linear model with randomized data\n\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis\n\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\nBoston_lm <- lm(medv ~ ., data = train)\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the linear model predictions:\n\nhead(Boston_lm_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_linear_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions)\n\nBoston_linear_rmse_mean <- mean(Boston_linear_rmse)\n\n# Let's use tree models\n\nBoston_tree <- tree(medv ~ ., data = train)\n\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\n\n# Let's have a quick look at the tree model predictions:\n\nhead(Boston_tree_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_tree_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions) \nBoston_tree_rmse_mean <- mean(Boston_tree_rmse)\n\nensemble <- data.frame('linear' = Boston_lm_predictions, 'tree' = Boston_tree_predictions, 'y_ensemble' = test$medv )\n\nensemble <- ensemble[sample(nrow(ensemble)), ] # Randomizes the rows of the ensemble\n\nensemble_train <- ensemble[1:60, ]\nensemble_test <- ensemble[61:105, ]\n\n# Ensemble linear modeling\n\nensemble_lm <- lm(y_ensemble ~ ., data = ensemble_train)\n\n# Predictions for the ensemble linear model\n\nensemble_prediction <- predict(ensemble_lm, newdata = ensemble_test)\n\n# Root mean squared error for the ensemble linear model\n\nensemble_lm_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_prediction)\n\nensemble_lm_rmse_mean <- mean(ensemble_lm_rmse)\n\nensemble_tree <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree, newdata = ensemble_test) \n\nensemble_tree_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = \nensemble_tree_predictions)\n\nensemble_tree_rmse_mean <- mean(ensemble_tree_rmse)\n\nresults <- data.frame(\n  'Linear' = Boston_linear_rmse_mean,\n  'Trees' = Boston_tree_rmse_mean,\n  'Ensembles_Linear' = ensemble_lm_rmse_mean,\n  'Ensemble_Tree' = ensemble_tree_rmse_mean )\n\n}\n\nresults\n#>     Linear   Trees Ensembles_Linear Ensemble_Tree\n#> 1 4.838286 4.66105         4.211636      5.199095\nwarnings() # No warnings!"},{"path":"introduction-and-your-first-ensembles.html","id":"principle-is-this-my-very-best-work","chapter":"2 Introduction and your first ensembles","heading":"2.11 Principle: “Is this my very best work?”","text":"best work build ensembles stage skills.\ngoing make number improvements solutions see\n, final result much stronger \nfar. Always strive best work, without excuses.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"where-do-i-get-help-with-errors-or-warnings","chapter":"2 Introduction and your first ensembles","heading":"2.12 “Where do I get help with errors or warnings?”","text":"extremely useful check code returns errors \nwarnings, fix fast possible. numerous sites \nhelp address errors code:https://stackoverflow.comhttps://forum.posit.cohttps://www.r-project.org/help.html","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"is-there-an-easy-way-to-save-all-trained-models","chapter":"2 Introduction and your first ensembles","heading":"2.13 Is there an easy way to save all trained models?","text":"Absolutely! simply add code end section \nsaves four trained models (linear, tree, ensemble_linear \nensemble_tree), follows:","code":"\nlibrary(MASS)\nlibrary(Metrics)\nlibrary(tree)\n\nensemble_lm_rmse <- 0\nensemble_tree_rmse <- 0\n\nfor (i in 1:100) {\n\n# Fit the linear model with randomized data\n\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis\n\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\nBoston_lm <- lm(medv ~ ., data = train)\n\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the linear model predictions:\n\nhead(Boston_lm_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_linear_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions) \nBoston_linear_rmse_mean <- mean(Boston_linear_rmse)\n\n# Let's use tree models\n\nBoston_tree <- tree(medv ~ ., data = train)\n\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\n\n# Let's have a quick look at the tree model predictions:\n\nhead(Boston_tree_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_tree_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions) \nBoston_tree_rmse_mean <- mean(Boston_tree_rmse)\n\nensemble <- data.frame( 'linear' = Boston_lm_predictions, 'tree' = Boston_tree_predictions, 'y_ensemble' = test$medv )\n\nensemble <- ensemble[sample(nrow(ensemble)), ] # Randomizes the rows of the ensemble\n\nensemble_train <- ensemble[1:60, ]\n\nensemble_test <- ensemble[61:105, ]\n\n# Ensemble linear modeling\n\nensemble_lm <- lm(y_ensemble ~ ., data = ensemble_train)\n\n# Predictions for the ensemble linear model\n\nensemble_prediction <- predict(ensemble_lm, newdata = ensemble_test)\n\n# Root mean squared error for the ensemble linear model\n\nensemble_lm_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_prediction)\n\nensemble_lm_rmse_mean <- mean(ensemble_lm_rmse)\n\nensemble_tree <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree, newdata = ensemble_test) \n\nensemble_tree_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_tree_predictions)\n\nensemble_tree_rmse_mean <- mean(ensemble_tree_rmse)\n\nresults <- list( 'Linear' = Boston_linear_rmse_mean, 'Trees' = Boston_tree_rmse_mean, 'Ensembles_Linear' = ensemble_lm_rmse_mean, 'Ensemble_Tree' = ensemble_tree_rmse_mean )\n\n}\n\nresults\n#> $Linear\n#> [1] 4.854109\n#> \n#> $Trees\n#> [1] 4.712546\n#> \n#> $Ensembles_Linear\n#> [1] 4.260444\n#> \n#> $Ensemble_Tree\n#> [1] 5.180599\nwarnings()\n\nBoston_lm <- Boston_lm\nBoston_tree <- Boston_tree\nensemble_lm <- ensemble_lm\nensemble_tree <- ensemble_tree"},{"path":"introduction-and-your-first-ensembles.html","id":"what-about-classification-logistic-and-time-series-data","chapter":"2 Introduction and your first ensembles","heading":"2.13.1 What about classification, logistic and time series data?","text":"subsequent chapters similar processes classification,\nlogistic time series data. ’s possible build ensembles \ntypes data. results extremely similar results\n’ve seen numerical data: ensembles won’t always\nbest results, best diverse set models \nensembles get best possible results.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"principle-ensembles-can-work-with-many-types-of-data-and-we-will-do-that-in-this-book","chapter":"2 Introduction and your first ensembles","heading":"2.13.2 Principle: Ensembles can work with many types of data, and we will do that in this book","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"can-it-make-predictions-on-totally-new-data-from-the-trained-modelsincluding-the-ensembles","chapter":"2 Introduction and your first ensembles","heading":"2.13.3 Can it make predictions on totally new data from the trained models—including the ensembles?","text":"solutions book independent use data. \nlook everything housing prices business analysis HR\nanalytics research medicine. One later examples \nexactly question asking—build individual ensemble\nmodels data, use pre-trained models make predictions\ntotally unseen data. develop set skills later \nbook, ’s minor extension ’re already seen \ncompleted.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"the-way-i-was-taught-how-to-write-code-was-totally-wrong-for-me-the-best-way-for-me-is-to-start-at-the-end-and-work-backward-from-there.-do-not-start-coding-looking-for-a-solution-instead-start-with-the-ending-and-work-backwards-from-there.","chapter":"2 Introduction and your first ensembles","heading":"2.13.4 The way I was taught how to write code was totally wrong for me: The best way for me is to start at the end and work backward from there. Do not start coding looking for a solution, instead, start with the ending and work backwards from there.","text":"Start end work backwards \nthereThe biggest lesson work make ensembles.\n’ve already seen steps, results \ncome. second biggest lesson everything taught \ndata science AI backwards actually works \nreal life. ’ve learned learn, applied skill\n(learning learn) wide range skills, including:• Running multi-million dollar division Fortune 1000 company,\nincluding full profit loss responsibility• Performing professional level many musical instruments• Able communicate English, Spanish sign language \nprofessional setting• Earning #1 place annual undergradate university mathematics\ncompetition—twice• Completing Master’s degree Guidance Counseling, allowing \nhelp many people path toward healthier life• Leader Oak Park, Illinois chapter Amnesty International \nten years, helping release several Prisoners Conscience• President Chicago Apple User Group ten years, helping many\npeople extremely good work hardware software• Leg press 1,000 pounds ten times row• Climbed mountain Colorado• Completed multiple skydives (looking forward )point learned learn, ’ve applied \nskill many areas. started learning data science/AI/coding, \ndifferent way creative whole life.\nway works start end, work backward \n, never give . Maybe best evidence success \nmethod fact:started write code led Ensembles package, \nfollowed steps: Start end, work backward , \nnever give . wound writing average 1,000 lines clean,\nerror free code per month 15 months. Ensembles package around\n15,000 lines clean, error free code.found attitude much important skill set, long\nshot.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"how-i-stuck-with-it-all-the-way-to-the-end-the-best-career-advice-i-ever-received-was-from-a-homeless-man-i-never-met-and-answers-the-question-of-what-most-strongly-predicts-success.","chapter":"2 Introduction and your first ensembles","heading":"2.13.5 How I stuck with it all the way to the end: The best career advice I ever received was from a homeless man I never met, and answers the question of what most strongly predicts success.","text":"Ashford SimpsonLearning building ensembles help make accurate\npredictions. ’s extrdmely good skill setting. \nfound important thing predict success. \nstudied, quite good works subject, \nacademic general population.favorite career advice—listened nearly every day \nworked Ensembles project—man homeless \ntime came words.Nick Ashford Willow Run, Michigan. moved New York, hoping\nget entertainment world dancer. Unfortunately ended\nhomeless streets New York. slept park benches, \ngot food soup kitchens.heard people White Rock Baptist Church feed (\nhomeless man) normal meal, Nick went one Sunday morning. \nmet people, especially choir members, started working \npiano player choir. name Valerie Simpson.Soon Nick Valerie writing songs church choir. Nick\nmentioned homeless, realized New York wasn’t\ngoing “”. determined. words put say:Ain’t mountain high enoughAin’t valley low enoughAin’t river wide enoughValerie took words, set music. sent song \nMotown, released Marvin Gaye Tammy Terrell covering \nvocals. later re-done Ashford Simpson Paul Riser, \nDiana Ross singing lead.short video summarizes experience, concludes\nfinale 1970 version song. attitude \nAshford Simpson expressed song extremely highly predictive \nsuccess, matter field endeavor. found extremely\nmotivating, used overcome obstacles challenges \njourney.skill knowing learn (continue \nshare book), attitude working matter \nhigh mountain long valley wide river, gives \nkeep moving toward success, success fully\nachieved.Later look make presentations, consider \nexample level quality can done:https://www.icloud.com/iclouddrive/002bNfVreagRYCYHAZ9GyQ02w#Ain’t%5FNo%5FMountain%5FHigh%5FEnough","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"exercises","chapter":"2 Introduction and your first ensembles","heading":"2.13.6 Exercises:","text":"Find data science Genesis. data science idea totally\nexcites gets bed every day. idea leads\ncreation many ideas. biggest boldest dreams\ncan possibly . idea strong \n. , benefit use \nreceive good create.Keep journal progress. ’s much easier see results\ntime record. Set journal today (\nweek). use Github journal. journal crazy\nideas, contradictory evidence, writing frustrations \nsuccesses, inspiration, one next thing worked , \nrock solid record path success. Seeing path \ntraversed huge motivation finishing project.best add journal entries regular schedule.Make ensemble using Boston Housing data set. Model \n13 columns data, median value home (14th\ncolumn) working chapter.Start planning comprehensive project. types data\ninterested ? patterns like \ndiscover? Begin looking online now possible data sets, \nlittle basic research. examples provided get\ncloser section book.","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3 Numerical data: How to make 23 individual models, and basic skills with functions","text":"begin building skills make ensembles \nmodels numerical data. However, going much easier \nmight appear first. Let’s see can make easy \npossible.work backwards make function need: Start endWe going start ending, beginning, work\nbackwards . method much, much easier working\nforward, see throughout book. might \nlittle uncomfortable first, skill allow complete\nwork faster rate work forward.’ll use Boston Housing data set, ’ll start Bagged\nRandom Forest function. now ’re going work one\nfunction, keep everything simple. essence, going run\nlike assembly line.want ending error rate model. Virtually customer\nwork going want know, “accurate ?” ’s \nstarting point.determine model accuracy? already previous\nchapter, finding root mean squared error individual models\nensemble models. ’re going steps , \nprocess familiar .get error rate model holdout data sets (test \nvalidation), ’re going need model (Bagged Random Forest \nfirst example), fit training data, use model make\npredictions test data. can measure error \npredictions, just . steps familiar \n. , please re-read previous chapter.need complete steps? ’re going go\nbackward (little) make function allow us work \ndata set.function need? Let’s make list:data (Boston housing)data (Boston housing)Column number (14, median value property)Column number (14, median value property)Train amountTrain amountTest amountTest amountValidation amountValidation amountNumber times resampleNumber times resampleOne key steps change name target variable\ny. initial name nearly anything, method changes\nname target variable y. allows us make one small\nchange allow easiest possible solution:","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"all-our-models-will-be-structured-the-same-way-y-.-data-train","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.1 All our models will be structured the same way: y ~ ., data = train","text":"means y (target value) function \nfeatures, data set training data set. \nvariations 27 models, basic structure \n.","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"having-the-same-structure-for-all-the-models-makes-it-much-easier-to-build-debug-and-deploy-the-completed-models.","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.2 Having the same structure for all the models makes it much easier to build, debug, and deploy the completed models.","text":"need start initial values, run.One extremely nice part creating models way enormous\nefficiency gives us. Bagged Random Forest model\nworking, able use similar (identical many\ncases!) processes models (Support Vector Machines).rock solid foundation lay beginning allow us \nsmooth easy experience foundation solid use \nbuild models. models mainly almost exact\nduplicates fist example.’steps follow:Load libraryLoad librarySet initial values 0Set initial values 0Create functionCreate functionSet random resamplingSet random resamplingBreak data train testBreak data train testFit model training data, make predictions measure\nerror test dataFit model training data, make predictions measure\nerror test dataReturn resultsReturn resultsCheck errors warningsCheck errors warningsTest different data setTest different data set","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"exercise-re-read-the-steps-above-how-we-will-work-backwards-to-come-up-with-the-function-we-need.","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.3 Exercise: Re-read the steps above how we will work backwards to come up with the function we need.","text":"","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bagged-random-forest","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.4 1. Bagged Random Forest","text":"Exercise: Try : Change values train, test \nvalidation, number resamples. See change \nresult.One : Find numerical data set, make bagged random\nforest function data set. (example, may use Auto\ndata set ISLR package. need remove last column,\nvehicle name. Model mpg function features using \nBagged Random Forest function, numerical data set work).Post: Share social first results making numerical function\n(screen shot/video optional stage, learning \nlater)example, “first data science function building making\nensembles later . Got everything run, errors. #AIEnsembles”Now build remaining 22 models numerical data. \nbuilt using structure, foundation.Now know build basic function, let’s build 22 \nsets tools need make ensemble, starting bagging:","code":"\nlibrary(e1071) # will allow us to use a tuned random forest model\nlibrary(Metrics) # Will allow us to calculate the root mean squared error\nlibrary(randomForest) # To use the random forest function\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\nlibrary(tidyverse) # Amazing set of tools for data science\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::combine()  masks randomForest::combine()\n#> ✖ dplyr::filter()   masks stats::filter()\n#> ✖ dplyr::lag()      masks stats::lag()\n#> ✖ ggplot2::margin() masks randomForest::margin()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n# Set initial values to 0. The function will return an error if any of these are left out.\n\nbag_rf_holdout_RMSE <- 0\nbag_rf_holdout_RMSE_mean <- 0\nbag_rf_train_RMSE <- 0\nbag_rf_test_RMSE <- 0\nbag_rf_validation_RMSE <- 0\n\n# Define the function\n\nnumerical_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n#Set up random resampling\n\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col())\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n#Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model to the training data, make predictions on the testing data, then calculate the error rates on the testing data sets.\nbag_rf_train_fit <- e1071::tune.randomForest(x = train, y = train$y, mtry = ncol(train) - 1)\nbag_rf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict( object = bag_rf_train_fit$best.model, newdata = train))\nbag_rf_train_RMSE_mean <- mean(bag_rf_train_RMSE)\nbag_rf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict( object = bag_rf_train_fit$best.model, newdata = test))\nbag_rf_test_RMSE_mean <- mean(bag_rf_test_RMSE)\n\n# Itemize the error on the holdout data sets, and calculate the mean of the results\nbag_rf_holdout_RMSE[i] <- mean(bag_rf_test_RMSE_mean)\nbag_rf_holdout_RMSE_mean <- mean(c(bag_rf_holdout_RMSE))\n\n# These are the predictions we will need when we make the ensembles\nbag_rf_test_predict_value <- as.numeric(predict(object = bag_rf_train_fit$best.model, newdata = test))\n\n\n# Return the mean of the results to the user\n\n} # closing brace for numresamples\n  return(bag_rf_holdout_RMSE_mean)\n\n} # closing brace for numerical_1 function\n\n# Here is our first numerical function in actual use. We will use 25 resamples\n\nnumerical_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 0.3043472\nwarnings() # no warnings, the best possible result"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bagging-bootstrap-aggregating","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.5 2. Bagging (bootstrap aggregating)","text":"","code":"\nlibrary(ipred) #for the bagging function\n\n# Set initial values to 0\nbagging_train_RMSE <- 0\nbagging_test_RMSE <- 0\nbagging_validation_RMSE <- 0\nbagging_holdout_RMSE <- 0\nbagging_test_predict_value <- 0\nbagging_validation_predict_value <- 0\n\n#Create the function:\n\nbagging_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\n\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model to the training data, calculate error, make predictions on the holdout data\n\nbagging_train_fit <- ipred::bagging(formula = y ~ ., data = train)\nbagging_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bagging_train_fit, newdata = train))\nbagging_train_RMSE_mean <- mean(bagging_train_RMSE)\nbagging_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bagging_train_fit, newdata = test))\nbagging_test_RMSE_mean <- mean(bagging_test_RMSE)\nbagging_holdout_RMSE[i] <- mean(bagging_test_RMSE_mean)\nbagging_holdout_RMSE_mean <- mean(bagging_holdout_RMSE)\ny_hat_bagging <- c(bagging_test_predict_value)\n\n} # closing braces for the resampling function\n  return(bagging_holdout_RMSE_mean)\n  \n} # closing braces for the bagging function\n\n# Test the function:\nbagging_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.20, numresamples = 25)\n#> [1] 4.012994\nwarnings() # no warnings"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bayesglm","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.6 3. BayesGLM","text":"","code":"\nlibrary(arm) # to use bayesglm function\n#> Loading required package: MASS\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russellconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\n\n# Set initial values to 0\nbayesglm_train_RMSE <- 0\nbayesglm_test_RMSE <- 0\nbayesglm_validation_RMSE <- 0\nbayesglm_holdout_RMSE <- 0\nbayesglm_test_predict_value <- 0\nbayesglm_validation_predict_value <- 0\n\n# Create the function:\nbayesglm_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n#Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n#Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = gaussian(link = \"identity\"))\nbayesglm_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesglm_train_fit, newdata = train))\nbayesglm_train_RMSE_mean <- mean(bayesglm_train_RMSE)\nbayesglm_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesglm_train_fit, newdata = test))\nbayesglm_test_RMSE_mean <- mean(bayesglm_test_RMSE) \ny_hat_bayesglm <- c(bayesglm_test_predict_value)\n\n} # closing braces for resampling\n  return(bayesglm_test_RMSE_mean)\n  \n} # closing braces for the function\n\nbayesglm_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.20, numresamples = 25)\n#> [1] 4.766784\nwarnings() # no warnings"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bayesrnn","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.7 4. BayesRNN","text":"","code":"\nlibrary(brnn) # so we can use the BayesRNN function\n#> Loading required package: Formula\n#> Loading required package: truncnorm\n\n#Set initial values to 0\n\nbayesrnn_train_RMSE <- 0\nbayesrnn_test_RMSE <- 0\nbayesrnn_validation_RMSE <- 0\nbayesrnn_holdout_RMSE <- 0\nbayesrnn_test_predict_value <- 0\nbayesrnn_validation_predict_value <- 0\n\n# Create the function:\n\nbayesrnn_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model on the training data, make predictions on the testing data\nbayesrnn_train_fit <- brnn::brnn(x = as.matrix(train), y = train$y)\nbayesrnn_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesrnn_train_fit, newdata = train))\nbayesrnn_train_RMSE_mean <- mean(bayesrnn_train_RMSE)\nbayesrnn_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesrnn_train_fit, newdata = test))\nbayesrnn_test_RMSE_mean <- mean(bayesrnn_test_RMSE)\n\ny_hat_bayesrnn <- c(bayesrnn_test_predict_value)\n\n} # Closing brace for number of resamples \n  return(bayesrnn_test_RMSE_mean)\n\n} # Closing brace for the function\n\nbayesrnn_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016579 \n#> gamma= 30.9146    alpha= 4.5072   beta= 19287.31 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7017412 \n#> gamma= 31.0361    alpha= 3.1851   beta= 14704.06 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015619 \n#> gamma= 31.2686    alpha= 5.5043   beta= 14369.86 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016138 \n#> gamma= 31.5051    alpha= 3.2518   beta= 14287.47 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016246 \n#> gamma= 31.4638    alpha= 5.7484   beta= 14045.49 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016751 \n#> gamma= 31.6047    alpha= 5.2739   beta= 13583.82 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015323 \n#> gamma= 30.2174    alpha= 3.217    beta= 19069.97 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016694 \n#> gamma= 30.7135    alpha= 5.0462   beta= 15276.49 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.701572 \n#> gamma= 30.4745    alpha= 4.9501   beta= 18702.41 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016356 \n#> gamma= 30.4193    alpha= 4.9632   beta= 18163.57 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015132 \n#> gamma= 31.4395    alpha= 5.3713   beta= 14690.09 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015874 \n#> gamma= 30.9879    alpha= 4.5776   beta= 33212.92 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016694 \n#> gamma= 30.3109    alpha= 5.1169   beta= 22330 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7014674 \n#> gamma= 31.0504    alpha= 5.3045   beta= 16203.19 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016579 \n#> gamma= 31.3034    alpha= 5.5572   beta= 14265.58 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015323 \n#> gamma= 31.5195    alpha= 5.4152   beta= 16280.12 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016246 \n#> gamma= 29.1298    alpha= 4.4154   beta= 19890.05 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016523 \n#> gamma= 31.4777    alpha= 5.7411   beta= 14662.8 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7014899 \n#> gamma= 31.6415    alpha= 4.6533   beta= 17256.76 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015469 \n#> gamma= 31.3987    alpha= 5.3242   beta= 14257.68 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016246 \n#> gamma= 31.5294    alpha= 5.1328   beta= 14909.11 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016356 \n#> gamma= 31.3673    alpha= 5.0348   beta= 16423.81 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015771 \n#> gamma= 31.6411    alpha= 5.7092   beta= 14018.46 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015519 \n#> gamma= 31.3911    alpha= 5.5519   beta= 15234.52 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7017288 \n#> gamma= 30.9045    alpha= 4.9779   beta= 14752.83\n#> [1] 0.1430853\n\nwarnings() # no warnings for BayesRNN function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"boosted-random-forest","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.8 5. Boosted Random Forest","text":"","code":"\nlibrary(e1071)\nlibrary(randomForest)\nlibrary(tidyverse)\n\n#Set initial values to 0\nboost_rf_train_RMSE <- 0\nboost_rf_test_RMSE <- 0\nboost_rf_validation_RMSE <- 0\nboost_rf_holdout_RMSE <- 0\nboost_rf_test_predict_value <- 0\nboost_rf_validation_predict_value <- 0\n\n#Create the function:\nboost_rf_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n#Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit boosted random forest model on the training data, make predictions on holdout data\n\nboost_rf_train_fit <- e1071::tune.randomForest(x = train, y = train$y, mtry = ncol(train) - 1)\nboost_rf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict( object = boost_rf_train_fit$best.model, newdata = train\n  ))\nboost_rf_train_RMSE_mean <- mean(boost_rf_train_RMSE)\nboost_rf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict( object = boost_rf_train_fit$best.model, newdata = test\n  ))\nboost_rf_test_RMSE_mean <- mean(boost_rf_test_RMSE)\n\n} # closing brace for numresamples\n  return(boost_rf_test_RMSE_mean)\n  \n} # closing brace for the function\n\nboost_rf_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 0.3278237\nwarnings() # no warnings for Boosted Random Forest function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"cubist","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.9 6. Cubist","text":"","code":"\nlibrary(Cubist)\n#> Loading required package: lattice\nlibrary(tidyverse)\n\n# Set initial values to 0\n\ncubist_train_RMSE <- 0\ncubist_test_RMSE <- 0\ncubist_validation_RMSE <- 0\ncubist_holdout_RMSE <- 0\ncubist_test_predict_value <- 0\n\n# Create the function:\n\ncubist_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model on the training data, make predictions on the holdout data\ncubist_train_fit <- Cubist::cubist(x = train[, 1:ncol(train) - 1], y = train$y)\ncubist_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = cubist_train_fit, newdata = train))\ncubist_train_RMSE_mean <- mean(cubist_train_RMSE)\ncubist_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = cubist_train_fit, newdata = test))\ncubist_test_RMSE_mean <- mean(cubist_test_RMSE)\n\n} # closing braces for numresamples\n  return(cubist_test_RMSE_mean)\n  \n} # closing braces for the function\n\ncubist_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.400205\nwarnings() # no warnings for individual cubist function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"elastic","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.10 7. Elastic","text":"","code":"\n\nlibrary(glmnet) # So we can run the elastic model\n#> Loaded glmnet 4.1-8\nlibrary(tidyverse)\n\n# Set initial values to 0\n\nelastic_train_RMSE <- 0\nelastic_test_RMSE <- 0\nelastic_validation_RMSE <- 0\nelastic_holdout_RMSE <- 0\nelastic_test_predict_value <- 0\nelastic_validation_predict_value <- 0\nelastic_test_RMSE <- 0\nelastic_test_RMSE_df <- data.frame(elastic_test_RMSE)\nelastic_validation_RMSE <- 0\nelastic_validation_RMSE_df <- data.frame(elastic_validation_RMSE)\nelastic_holdout_RMSE <- 0\nelastic_holdout_RMSE_df <- data.frame(elastic_holdout_RMSE)\n\n# Create the function:\nelastic_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\n# Set up the elastic model\n\ny <- train$y\nx <- data.matrix(train %>% dplyr::select(-y))\nelastic_model <- glmnet::glmnet(x, y, alpha = 0.5)\nelastic_cv <- cv.glmnet(x, y, alpha = 0.5)\nbest_elastic_lambda <- elastic_cv$lambda.min\nbest_elastic_model <- glmnet::glmnet(x, y, alpha = 0, lambda = best_elastic_lambda)\nelastic_test_pred <- predict(best_elastic_model, s = best_elastic_lambda, newx = data.matrix(test %>% dplyr::select(-y)))\n\nelastic_test_RMSE <- Metrics::rmse(actual = test$y, predicted = elastic_test_pred)\nelastic_test_RMSE_df <- rbind(elastic_test_RMSE_df, elastic_test_RMSE)\nelastic_test_RMSE_mean <- mean(elastic_test_RMSE_df$elastic_test_RMSE[2:nrow(elastic_test_RMSE_df)])\n\nelastic_holdout_RMSE <- mean(elastic_test_RMSE_mean)\nelastic_holdout_RMSE_df <- rbind(elastic_holdout_RMSE_df, elastic_holdout_RMSE)\nelastic_holdout_RMSE_mean <- mean(elastic_holdout_RMSE_df$elastic_holdout_RMSE[2:nrow(elastic_holdout_RMSE_df)])\n\n} # closing brace for numresample\n  return(elastic_holdout_RMSE_mean)\n  \n} # closing brace for the elastic function\n\nelastic_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 5.060079\nwarnings() # no warnings for individual elastic function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"generalized-additive-models-with-smoothing-splines","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.11 8. Generalized Additive Models with smoothing splines","text":"","code":"\nlibrary(gam) # for fitting generalized additive models\n#> Loading required package: splines\n#> Loading required package: foreach\n#> \n#> Attaching package: 'foreach'\n#> The following objects are masked from 'package:purrr':\n#> \n#>     accumulate, when\n#> Loaded gam 1.22-3\n\n# Set initial values to 0\n\ngam_train_RMSE <- 0\ngam_test_RMSE <- 0\ngam_holdout_RMSE <- 0\ngam_test_predict_value <- 0\n\n# Create the function:\ngam1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Set up random resampling\n\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\n# Set up to fit the model on the training data\n\nn_unique_vals <- purrr::map_dbl(df, dplyr::n_distinct)\n\n# Names of columns with >= 4 unique vals\nkeep <- names(n_unique_vals)[n_unique_vals >= 4]\n\ngam_data <- df %>% dplyr::select(dplyr::all_of(keep))\n\n# Model data\n\ntrain1 <- train %>% dplyr::select(dplyr::all_of(keep))\n\ntest1 <- test %>% dplyr::select(dplyr::all_of(keep))\n\nnames_df <- names(gam_data[, 1:ncol(gam_data) - 1])\nf2 <- stats::as.formula(paste0(\"y ~\", paste0(\"gam::s(\", names_df, \")\", collapse = \"+\")))\n\ngam_train_fit <- gam::gam(f2, data = train1)\ngam_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = gam_train_fit, newdata = train))\ngam_train_RMSE_mean <- mean(gam_train_RMSE)\ngam_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = gam_train_fit, newdata = test))\ngam_test_RMSE_mean <- mean(gam_test_RMSE)\ngam_holdout_RMSE[i] <- mean(gam_test_RMSE_mean)\ngam_holdout_RMSE_mean <- mean(gam_holdout_RMSE)\n\n} # closing braces for numresamples\n  return(gam_holdout_RMSE_mean)\n  \n} # closing braces for gam function\n\ngam1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 5.051043\nwarnings() # no warnings for individual gam function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"gradient-boosted","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.12 9. Gradient Boosted","text":"","code":"\nlibrary(gbm) # to allow use of gradient boosted models\n#> Loaded gbm 2.1.9\n#> This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\n# Set initial values to 0\ngb_train_RMSE <- 0\ngb_test_RMSE <- 0\ngb_validation_RMSE <- 0\ngb_holdout_RMSE <- 0\ngb_test_predict_value <- 0\ngb_validation_predict_value <- 0\n\ngb1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\ngb_train_fit <- gbm::gbm(train$y ~ ., data = train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\ngb_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = gb_train_fit, newdata = train))\ngb_train_RMSE_mean <- mean(gb_train_RMSE)\ngb_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = gb_train_fit, newdata = test))\ngb_test_RMSE_mean <- mean(gb_test_RMSE)\n\n} # closing brace for numresamples\n  return(gb_test_RMSE_mean)\n  \n} # closing brace for gb1 function\n\ngb1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> Using 100 trees...\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> [1] 3.579291\nwarnings() # no warnings for individual gradient boosted function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"k-nearest-neighbors-tuned","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.13 10. K-Nearest Neighbors (tuned)","text":"","code":"\n\nlibrary(e1071)\n\n# Set initial values to 0\nknn_train_RMSE <- 0\nknn_test_RMSE <- 0\nknn_validation_RMSE <- 0\nknn_holdout_RMSE <- 0\nknn_test_predict_value <- 0\nknn_validation_predict_value <- 0\n\nknn1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nknn_train_fit <- e1071::tune.gknn(x = train[, 1:ncol(train) - 1], y = train$y, scale = TRUE, k = c(1:25))\nknn_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict( object = knn_train_fit$best.model,\n    newdata = train[, 1:ncol(train) - 1], k = knn_train_fit$best_model$k))\nknn_train_RMSE_mean <- mean(knn_train_RMSE)\nknn_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict( object = knn_train_fit$best.model,\n    k = knn_train_fit$best_model$k, newdata = test[, 1:ncol(test) - 1]))\nknn_test_RMSE_mean <- mean(knn_test_RMSE)\nknn_holdout_RMSE[i] <- mean(c(knn_test_RMSE_mean))\nknn_holdout_RMSE_mean <- mean(knn_holdout_RMSE)\n\n} # closing brace for numresamples\n  return(knn_holdout_RMSE_mean)\n  \n} # closing brace for knn1 function\n\nknn1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 7.052156\nwarnings() # no warnings for individual knn function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"lasso","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.14 11. Lasso","text":"","code":"\nlibrary(glmnet) # So we can run the lasso model\n\n# Set initial values to 0\n\nlasso_train_RMSE <- 0\nlasso_test_RMSE <- 0\nlasso_validation_RMSE <- 0\nlasso_holdout_RMSE <- 0\nlasso_test_predict_value <- 0\nlasso_validation_predict_value <- 0\nlasso_test_RMSE <- 0\nlasso_test_RMSE_df <- data.frame(lasso_test_RMSE)\nlasso_validation_RMSE <- 0\nlasso_validation_RMSE_df <- data.frame(lasso_validation_RMSE)\nlasso_holdout_RMSE <- 0\nlasso_holdout_RMSE_df <- data.frame(lasso_holdout_RMSE)\n\n# Create the function:\nlasso_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Set up the lasso model\ny <- train$y\nx <- data.matrix(train %>% dplyr::select(-y))\nlasso_model <- glmnet::glmnet(x, y, alpha = 1.0)\nlasso_cv <- cv.glmnet(x, y, alpha = 1.0)\nbest_lasso_lambda <- lasso_cv$lambda.min\nbest_lasso_model <- glmnet::glmnet(x, y, alpha = 0, lambda = best_lasso_lambda)\nlasso_test_pred <- predict(best_lasso_model, s = best_lasso_lambda, newx = data.matrix(test %>% dplyr::select(-y)))\n\nlasso_test_RMSE <- Metrics::rmse(actual = test$y, predicted = lasso_test_pred)\nlasso_test_RMSE_df <- rbind(lasso_test_RMSE_df, lasso_test_RMSE)\nlasso_test_RMSE_mean <- mean(lasso_test_RMSE_df$lasso_test_RMSE[2:nrow(lasso_test_RMSE_df)])\n\nlasso_holdout_RMSE <- mean(lasso_test_RMSE_mean)\nlasso_holdout_RMSE_df <- rbind(lasso_holdout_RMSE_df, lasso_holdout_RMSE)\nlasso_holdout_RMSE_mean <- mean(lasso_holdout_RMSE_df$lasso_holdout_RMSE[2:nrow(lasso_holdout_RMSE_df)])\n\n} # closing brace for numresample\n  return(lasso_holdout_RMSE_mean)\n  \n} # closing brace for the lasso_1 function\n\nlasso_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 5.010819\nwarnings() # no warnings for individual lasso function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"linear-tuned","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.15 12. Linear (tuned)","text":"","code":"\n\nlibrary(e1071) # for tuned linear models\n\n# Set initial values to 0\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_holdout_RMSE <- 0\n\n# Set up the function\nlinear1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nlinear_train_fit <- e1071::tune.rpart(formula = y ~ ., data = train)\nlinear_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = linear_train_fit$best.model, newdata = train))\nlinear_train_RMSE_mean <- mean(linear_train_RMSE)\nlinear_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = linear_train_fit$best.model, newdata = test))\nlinear_holdout_RMSE_mean <- mean(linear_test_RMSE)\n\n} # closing brace for numresamples\n  return(linear_holdout_RMSE_mean)\n  \n} # closing brace for linear1 function\n\nlinear1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.805244\nwarnings() # no warnings for individual lasso function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"lqs","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.16 13. LQS","text":"","code":"\n\nlibrary(MASS) # to allow us to run LQS models\n\n# Set initial values to 0\n\nlqs_train_RMSE <- 0\nlqs_test_RMSE <- 0\nlqs_validation_RMSE <- 0\nlqs_holdout_RMSE <- 0\nlqs_test_predict_value <- 0\nlqs_validation_predict_value <- 0\n\nlqs1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nlqs_train_fit <- MASS::lqs(train$y ~ ., data = train)\nlqs_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = lqs_train_fit, newdata = train))\nlqs_train_RMSE_mean <- mean(lqs_train_RMSE)\nlqs_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = lqs_train_fit, newdata = test))\nlqs_test_RMSE_mean <- mean(lqs_test_RMSE)\n\ny_hat_lqs <- c(lqs_test_predict_value, lqs_validation_predict_value)\n\n} # Closing brace for numresamples\n    return(lqs_test_RMSE_mean)\n\n} # Closing brace for lqs1 function\n\nlqs1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 7.741189\nwarnings() # no warnings for individual lqs function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"neuralnet","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.17 14. Neuralnet","text":"","code":"\nlibrary(neuralnet)\n#> \n#> Attaching package: 'neuralnet'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     compute\n\n#Set initial values to 0\n\nneuralnet_train_RMSE <- 0\nneuralnet_test_RMSE <- 0\nneuralnet_validation_RMSE <- 0\nneuralnet_holdout_RMSE <- 0\nneuralnet_test_predict_value <- 0\nneuralnet_validation_predict_value <- 0\n\n# Fit the model to the training data\nneuralnet1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test data sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nmaxs <- apply(df, 2, max)\nmins <- apply(df, 2, min)\nscaled <- as.data.frame(scale(df, center = mins, scale = maxs - mins))\ntrain_ <- scaled[idx == 1, ]\ntest_ <- scaled[idx == 2, ]\nn <- names(train_)\nf <- as.formula(paste(\"y ~\", paste(n[!n %in% \"y\"], collapse = \" + \")))\nnn <- neuralnet(f, data = train_, hidden = c(5, 3), linear.output = TRUE)\npredict_test_nn <- neuralnet::compute(nn, test_[, 1:ncol(df) - 1])\npredict_test_nn_ <- predict_test_nn$net.result * (max(df$y) - min(df$y)) + min(df$y)\npredict_train_nn <- neuralnet::compute(nn, train_[, 1:ncol(df) - 1])\npredict_train_nn_ <- predict_train_nn$net.result * (max(df$y) - min(df$y)) + min(df$y)\nneuralnet_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict_train_nn_)\nneuralnet_train_RMSE_mean <- mean(neuralnet_train_RMSE)\nneuralnet_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict_test_nn_)\nneuralnet_test_RMSE_mean <- mean(neuralnet_test_RMSE)\n\nneuralnet_holdout_RMSE[i] <- mean(c(neuralnet_test_RMSE))\nneuralnet_holdout_RMSE_mean <- mean(neuralnet_holdout_RMSE)\n\n} # Closing brace for numresamples\n  return(neuralnet_holdout_RMSE_mean)\n  \n} # closing brace for neuralnet1 function\n\nneuralnet1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 3.934191\nwarnings() # no warnings for individual neuralnet function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"partial-least-squares","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.18 15. Partial Least Squares","text":"","code":"\n\nlibrary(pls)\n#> \n#> Attaching package: 'pls'\n#> The following objects are masked from 'package:arm':\n#> \n#>     coefplot, corrplot\n#> The following object is masked from 'package:stats':\n#> \n#>     loadings\n\n# Set initial values to 0\npls_train_RMSE <- 0\npls_test_RMSE <- 0\npls_validation_RMSE <- 0\npls_holdout_RMSE <- 0\npls_test_predict_value <- 0\npls_validation_predict_value <- 0\n\npls1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\npls_train_fit <- pls::plsr(train$y ~ ., data = train)\npls_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = pls_train_fit, newdata = train))\npls_train_RMSE_mean <- mean(pls_train_RMSE)\npls_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = pls_train_fit, newdata = test))\npls_test_RMSE_mean <- mean(pls_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return( pls_test_RMSE_mean)\n  \n} # Closing brace for pls1 function\n\npls1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 6.160096\nwarnings() # no warnings for individual pls function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"principal-components-regression","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.19 16. Principal Components Regression","text":"","code":"\n\nlibrary(pls) # To run pcr models\n\n#Set initial values to 0\npcr_train_RMSE <- 0\npcr_test_RMSE <- 0\npcr_validation_RMSE <- 0\npcr_holdout_RMSE <- 0\npcr_test_predict_value <- 0\npcr_validation_predict_value <- 0\n\npcr1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\npcr_train_fit <- pls::pcr(train$y ~ ., data = train)\npcr_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = pcr_train_fit, newdata = train))\npcr_train_RMSE_mean <- mean(pcr_train_RMSE)\npcr_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = pcr_train_fit, newdata = test))\npcr_test_RMSE_mean <- mean(pcr_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return(pcr_test_RMSE_mean)\n  \n} # Closing brace for PCR function\n\npcr1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 6.444914\nwarnings() # no warnings for individual pls function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"random-forest","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.20 17. Random Forest","text":"","code":"\nlibrary(randomForest)\n\n# Set initial values to 0\nrf_train_RMSE <- 0\nrf_test_RMSE <- 0\nrf_validation_RMSE <- 0\nrf_holdout_RMSE <- 0\nrf_test_predict_value <- 0\nrf_validation_predict_value <- 0\n\n# Set up the function\nrf1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nrf_train_fit <- tune.randomForest(x = train, y = train$y, data = train)\nrf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = rf_train_fit$best.model, newdata = train))\nrf_train_RMSE_mean <- mean(rf_train_RMSE)\nrf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = rf_train_fit$best.model, newdata = test))\nrf_test_RMSE_mean <- mean(rf_test_RMSE)\n\n} # Closing brace for numresamples loop\nreturn(rf_test_RMSE_mean)\n  \n} # Closing brace for rf1 function\n\nrf1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 1.767656\nwarnings() # no warnings for individual random forest function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"ridge-regression","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.21 18. Ridge Regression","text":"","code":"\n\nlibrary(glmnet) # So we can run the ridge model\n\n# Set initial values to 0\nridge_train_RMSE <- 0\nridge_test_RMSE <- 0\nridge_validation_RMSE <- 0\nridge_holdout_RMSE <- 0\nridge_test_predict_value <- 0\nridge_validation_predict_value <- 0\nridge_test_RMSE <- 0\nridge_test_RMSE_df <- data.frame(ridge_test_RMSE)\nridge_validation_RMSE <- 0\nridge_validation_RMSE_df <- data.frame(ridge_validation_RMSE)\nridge_holdout_RMSE <- 0\nridge_holdout_RMSE_df <- data.frame(ridge_holdout_RMSE)\n\n# Create the function:\nridge1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Set up the ridge model\ny <- train$y\nx <- data.matrix(train %>% dplyr::select(-y))\nridge_model <- glmnet::glmnet(x, y, alpha = 0)\nridge_cv <- cv.glmnet(x, y, alpha = 0)\nbest_ridge_lambda <- ridge_cv$lambda.min\nbest_ridge_model <- glmnet::glmnet(x, y, alpha = 0, lambda = best_ridge_lambda)\nridge_test_pred <- predict(best_ridge_model, s = best_ridge_lambda, newx = data.matrix(test %>% dplyr::select(-y)))\n\nridge_test_RMSE <- Metrics::rmse(actual = test$y, predicted = ridge_test_pred)\nridge_test_RMSE_df <- rbind(ridge_test_RMSE_df, ridge_test_RMSE)\nridge_test_RMSE_mean <- mean(ridge_test_RMSE_df$ridge_test_RMSE[2:nrow(ridge_test_RMSE_df)])\n\nridge_holdout_RMSE <- mean(ridge_test_RMSE_mean)\nridge_holdout_RMSE_df <- rbind(ridge_holdout_RMSE_df, ridge_holdout_RMSE)\nridge_holdout_RMSE_mean <- mean(ridge_holdout_RMSE_df$ridge_holdout_RMSE[2:nrow(ridge_holdout_RMSE_df)])\n\n} # closing brace for numresample\n  return(ridge_holdout_RMSE_mean)\n  \n} # closing brace for the ridge function\n\nridge1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.993241\nwarnings() # no warnings for individual ridge function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"robust-regression","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.22 19. Robust Regression","text":"","code":"\n\nlibrary(MASS) # To run rlm function for robust regression\n\n# Set initial values to 0\nrobust_train_RMSE <- 0\nrobust_test_RMSE <- 0\nrobust_validation_RMSE <- 0\nrobust_holdout_RMSE <- 0\nrobust_test_predict_value <- 0\nrobust_validation_predict_value <- 0\n\n# Make the function\nrobust1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nrobust_train_fit <- MASS::rlm(x = train[, 1:ncol(df) - 1], y = train$y)\nrobust_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = robust_train_fit$fitted.values)\nrobust_train_RMSE_mean <- mean(robust_train_RMSE)\nrobust_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = MASS::rlm(y ~ ., data = train), newdata = test))\nrobust_test_RMSE_mean <- mean(robust_test_RMSE) \n\n} # Closing brace for numresamples loop\nreturn(robust_test_RMSE_mean)\n  \n} # Closing brace for robust1 function\n\nrobust1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 5.054642\nwarnings() # no warnings for individual robust function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"rpart","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.23 20. Rpart","text":"","code":"\n\nlibrary(rpart)\n\n# Set initial values to 0\nrpart_train_RMSE <- 0\nrpart_test_RMSE <- 0\nrpart_validation_RMSE <- 0\nrpart_holdout_RMSE <- 0\nrpart_test_predict_value <- 0\nrpart_validation_predict_value <- 0\n\n# Make the function\nrpart1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nrpart_train_fit <- rpart::rpart(train$y ~ ., data = train)\nrpart_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = rpart_train_fit, newdata = train))\nrpart_train_RMSE_mean <- mean(rpart_train_RMSE)\nrpart_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = rpart_train_fit, newdata = test))\nrpart_test_RMSE_mean <- mean(rpart_test_RMSE)\n\n} # Closing loop for numresamples\nreturn(rpart_test_RMSE_mean)\n  \n} # Closing brace for rpart1 function\n\nrpart1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.793699\nwarnings() # no warnings for individual rpart function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"support-vector-machines","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.24 21. Support Vector Machines","text":"","code":"\n\nlibrary(e1071)\n\n# Set initial values to 0\nsvm_train_RMSE <- 0\nsvm_test_RMSE <- 0\nsvm_validation_RMSE <- 0\nsvm_holdout_RMSE <- 0\nsvm_test_predict_value <- 0\nsvm_validation_predict_value <- 0\n\n# Make the function\nsvm1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nsvm_train_fit <- e1071::tune.svm(x = train, y = train$y, data = train)\nsvm_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = svm_train_fit$best.model, newdata = train))\nsvm_train_RMSE_mean <- mean(svm_train_RMSE)\nsvm_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = svm_train_fit$best.model, newdata = test))\nsvm_test_RMSE_mean <- mean(svm_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return(svm_test_RMSE_mean)\n\n} # Closing brace for svm1 function\n\nsvm1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 2.426661\nwarnings() # no warnings for individual Support Vector Machines function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"trees","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.25 22. Trees","text":"","code":"\n\nlibrary(tree)\n\n# Set initial values to 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_validation_RMSE <- 0\ntree_holdout_RMSE <- 0\ntree_test_predict_value <- 0\ntree_validation_predict_value <- 0\n\n# Make the function\ntree1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\ntree_train_fit <- tree::tree(train$y ~ ., data = train)\ntree_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = tree_train_fit, newdata = train))\ntree_train_RMSE_mean <- mean(tree_train_RMSE)\ntree_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = tree_train_fit, newdata = test))\ntree_test_RMSE_mean <- mean(tree_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return(tree_test_RMSE_mean)\n  \n} # Closing brace for tree1 function\n\ntree1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.802842\nwarnings() # no warnings for individual tree function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"xgboost","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.26 23. XGBoost","text":"","code":"\nlibrary(xgboost)\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\n\n# Set initial values to 0\nxgb_train_RMSE <- 0\nxgb_test_RMSE <- 0\nxgb_validation_RMSE <- 0\nxgb_holdout_RMSE <- 0\nxgb_test_predict_value <- 0\nxgb_validation_predict_value <- 0\n\n# Create the function\nxgb1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\ntrain_x <- data.matrix(train[, -ncol(train)])\ntrain_y <- train[, ncol(train)]\n\n# define predictor and response variables in test set\ntest_x <- data.matrix(test[, -ncol(test)])\ntest_y <- test[, ncol(test)]\n\n# define final train, test and validation sets\n\nxgb_train <- xgboost::xgb.DMatrix(data = train_x, label = train_y)\nxgb_test <- xgboost::xgb.DMatrix(data = test_x, label = test_y)\n\n# define watchlist\nwatchlist <- list(train = xgb_train)\nwatchlist_test <- list(train = xgb_train, test = xgb_test)\n\n# fit XGBoost model and display training and validation data at each round\nxgb_model <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_test, nrounds = 70)\n\nxgboost_min <- which.min(xgb_model$evaluation_log$validation_rmse)\n\nxgb_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = xgb_model, newdata = train_x))\nxgb_train_RMSE_mean <- mean(xgb_train_RMSE)\nxgb_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = xgb_model, newdata = test_x))\nxgb_test_RMSE_mean <- mean(xgb_test_RMSE)\n\nxgb_holdout_RMSE[i] <- mean(xgb_test_RMSE_mean)\nxgb_holdout_RMSE_mean <- mean(xgb_holdout_RMSE)\n\n} # Closing brace for numresamples loop\n  return(xgb_holdout_RMSE_mean)\n  \n} # Closing brace for xgb1 function\n\nxgb1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1]  train-rmse:16.566847    test-rmse:18.295836 \n#> [2]  train-rmse:12.001877    test-rmse:13.600278 \n#> [3]  train-rmse:8.857524 test-rmse:10.447422 \n#> [4]  train-rmse:6.648565 test-rmse:8.334287 \n#> [5]  train-rmse:5.137718 test-rmse:7.013896 \n#> [6]  train-rmse:4.116261 test-rmse:6.110523 \n#> [7]  train-rmse:3.420488 test-rmse:5.520650 \n#> [8]  train-rmse:3.002864 test-rmse:5.227048 \n#> [9]  train-rmse:2.707789 test-rmse:5.022301 \n#> [10] train-rmse:2.521882 test-rmse:4.852310 \n#> [11] train-rmse:2.405066 test-rmse:4.791663 \n#> [12] train-rmse:2.320092 test-rmse:4.691732 \n#> [13] train-rmse:2.240695 test-rmse:4.632454 \n#> [14] train-rmse:2.156235 test-rmse:4.562374 \n#> [15] train-rmse:2.119126 test-rmse:4.524297 \n#> [16] train-rmse:2.079369 test-rmse:4.478217 \n#> [17] train-rmse:2.020774 test-rmse:4.452389 \n#> [18] train-rmse:1.946407 test-rmse:4.365773 \n#> [19] train-rmse:1.857629 test-rmse:4.303493 \n#> [20] train-rmse:1.835047 test-rmse:4.283662 \n#> [21] train-rmse:1.796012 test-rmse:4.277377 \n#> [22] train-rmse:1.763180 test-rmse:4.273819 \n#> [23] train-rmse:1.748976 test-rmse:4.272261 \n#> [24] train-rmse:1.689464 test-rmse:4.274587 \n#> [25] train-rmse:1.660090 test-rmse:4.261288 \n#> [26] train-rmse:1.648596 test-rmse:4.273753 \n#> [27] train-rmse:1.587294 test-rmse:4.261752 \n#> [28] train-rmse:1.557944 test-rmse:4.243167 \n#> [29] train-rmse:1.541092 test-rmse:4.250105 \n#> [30] train-rmse:1.512719 test-rmse:4.254290 \n#> [31] train-rmse:1.495594 test-rmse:4.235982 \n#> [32] train-rmse:1.471548 test-rmse:4.237962 \n#> [33] train-rmse:1.461947 test-rmse:4.230278 \n#> [34] train-rmse:1.440187 test-rmse:4.217669 \n#> [35] train-rmse:1.412271 test-rmse:4.217333 \n#> [36] train-rmse:1.394335 test-rmse:4.218771 \n#> [37] train-rmse:1.354841 test-rmse:4.211338 \n#> [38] train-rmse:1.344822 test-rmse:4.211056 \n#> [39] train-rmse:1.312267 test-rmse:4.203068 \n#> [40] train-rmse:1.280977 test-rmse:4.206494 \n#> [41] train-rmse:1.261975 test-rmse:4.198913 \n#> [42] train-rmse:1.248094 test-rmse:4.185089 \n#> [43] train-rmse:1.212051 test-rmse:4.189859 \n#> [44] train-rmse:1.199969 test-rmse:4.181182 \n#> [45] train-rmse:1.193709 test-rmse:4.180919 \n#> [46] train-rmse:1.163349 test-rmse:4.182952 \n#> [47] train-rmse:1.136975 test-rmse:4.158753 \n#> [48] train-rmse:1.112211 test-rmse:4.148918 \n#> [49] train-rmse:1.105006 test-rmse:4.154884 \n#> [50] train-rmse:1.088859 test-rmse:4.143711 \n#> [51] train-rmse:1.076607 test-rmse:4.150342 \n#> [52] train-rmse:1.072393 test-rmse:4.150674 \n#> [53] train-rmse:1.064686 test-rmse:4.150741 \n#> [54] train-rmse:1.053028 test-rmse:4.154434 \n#> [55] train-rmse:1.020980 test-rmse:4.166082 \n#> [56] train-rmse:0.998032 test-rmse:4.181283 \n#> [57] train-rmse:0.987385 test-rmse:4.176308 \n#> [58] train-rmse:0.962621 test-rmse:4.166486 \n#> [59] train-rmse:0.958672 test-rmse:4.172673 \n#> [60] train-rmse:0.935287 test-rmse:4.162383 \n#> [61] train-rmse:0.925546 test-rmse:4.157751 \n#> [62] train-rmse:0.920647 test-rmse:4.161055 \n#> [63] train-rmse:0.914087 test-rmse:4.164519 \n#> [64] train-rmse:0.891765 test-rmse:4.149640 \n#> [65] train-rmse:0.876797 test-rmse:4.151246 \n#> [66] train-rmse:0.866961 test-rmse:4.153165 \n#> [67] train-rmse:0.862838 test-rmse:4.153364 \n#> [68] train-rmse:0.846186 test-rmse:4.153644 \n#> [69] train-rmse:0.833005 test-rmse:4.153704 \n#> [70] train-rmse:0.818758 test-rmse:4.151786 \n#> [1]  train-rmse:17.476615    test-rmse:16.712516 \n#> [2]  train-rmse:12.706851    test-rmse:12.104079 \n#> [3]  train-rmse:9.325248 test-rmse:8.891638 \n#> [4]  train-rmse:6.967080 test-rmse:6.638769 \n#> [5]  train-rmse:5.361310 test-rmse:5.243285 \n#> [6]  train-rmse:4.273568 test-rmse:4.382146 \n#> [7]  train-rmse:3.571172 test-rmse:3.854521 \n#> [8]  train-rmse:3.096809 test-rmse:3.553356 \n#> [9]  train-rmse:2.785035 test-rmse:3.411058 \n#> [10] train-rmse:2.542470 test-rmse:3.289487 \n#> [11] train-rmse:2.396477 test-rmse:3.266160 \n#> [12] train-rmse:2.249223 test-rmse:3.219625 \n#> [13] train-rmse:2.165404 test-rmse:3.248116 \n#> [14] train-rmse:2.082891 test-rmse:3.199139 \n#> [15] train-rmse:2.024894 test-rmse:3.190043 \n#> [16] train-rmse:1.965081 test-rmse:3.156556 \n#> [17] train-rmse:1.882508 test-rmse:3.154637 \n#> [18] train-rmse:1.828034 test-rmse:3.162224 \n#> [19] train-rmse:1.778963 test-rmse:3.188254 \n#> [20] train-rmse:1.726786 test-rmse:3.180868 \n#> [21] train-rmse:1.702797 test-rmse:3.170957 \n#> [22] train-rmse:1.667876 test-rmse:3.193273 \n#> [23] train-rmse:1.636254 test-rmse:3.197338 \n#> [24] train-rmse:1.588035 test-rmse:3.199324 \n#> [25] train-rmse:1.561565 test-rmse:3.203509 \n#> [26] train-rmse:1.523799 test-rmse:3.181351 \n#> [27] train-rmse:1.490133 test-rmse:3.183189 \n#> [28] train-rmse:1.474789 test-rmse:3.183359 \n#> [29] train-rmse:1.432077 test-rmse:3.168793 \n#> [30] train-rmse:1.399415 test-rmse:3.179798 \n#> [31] train-rmse:1.368031 test-rmse:3.164018 \n#> [32] train-rmse:1.339634 test-rmse:3.155773 \n#> [33] train-rmse:1.322923 test-rmse:3.167707 \n#> [34] train-rmse:1.312939 test-rmse:3.162325 \n#> [35] train-rmse:1.291858 test-rmse:3.162030 \n#> [36] train-rmse:1.266042 test-rmse:3.157177 \n#> [37] train-rmse:1.249867 test-rmse:3.155634 \n#> [38] train-rmse:1.233007 test-rmse:3.162457 \n#> [39] train-rmse:1.202567 test-rmse:3.158863 \n#> [40] train-rmse:1.192454 test-rmse:3.166728 \n#> [41] train-rmse:1.185654 test-rmse:3.161986 \n#> [42] train-rmse:1.156329 test-rmse:3.155749 \n#> [43] train-rmse:1.132966 test-rmse:3.156878 \n#> [44] train-rmse:1.099172 test-rmse:3.160830 \n#> [45] train-rmse:1.084264 test-rmse:3.166902 \n#> [46] train-rmse:1.077450 test-rmse:3.176762 \n#> [47] train-rmse:1.069471 test-rmse:3.172082 \n#> [48] train-rmse:1.047696 test-rmse:3.161650 \n#> [49] train-rmse:1.029634 test-rmse:3.164930 \n#> [50] train-rmse:1.011847 test-rmse:3.161962 \n#> [51] train-rmse:0.988660 test-rmse:3.161577 \n#> [52] train-rmse:0.966914 test-rmse:3.157507 \n#> [53] train-rmse:0.958587 test-rmse:3.158530 \n#> [54] train-rmse:0.946078 test-rmse:3.158107 \n#> [55] train-rmse:0.936126 test-rmse:3.159700 \n#> [56] train-rmse:0.920098 test-rmse:3.164522 \n#> [57] train-rmse:0.903369 test-rmse:3.158688 \n#> [58] train-rmse:0.888368 test-rmse:3.159415 \n#> [59] train-rmse:0.880155 test-rmse:3.161047 \n#> [60] train-rmse:0.869179 test-rmse:3.160468 \n#> [61] train-rmse:0.862545 test-rmse:3.161371 \n#> [62] train-rmse:0.858606 test-rmse:3.162070 \n#> [63] train-rmse:0.843402 test-rmse:3.155911 \n#> [64] train-rmse:0.839111 test-rmse:3.155235 \n#> [65] train-rmse:0.820921 test-rmse:3.167764 \n#> [66] train-rmse:0.813685 test-rmse:3.167962 \n#> [67] train-rmse:0.799078 test-rmse:3.165993 \n#> [68] train-rmse:0.792911 test-rmse:3.161664 \n#> [69] train-rmse:0.783525 test-rmse:3.157303 \n#> [70] train-rmse:0.763116 test-rmse:3.158526 \n#> [1]  train-rmse:16.940826    test-rmse:17.620587 \n#> [2]  train-rmse:12.287029    test-rmse:12.895806 \n#> [3]  train-rmse:9.027421 test-rmse:9.716785 \n#> [4]  train-rmse:6.807202 test-rmse:7.638635 \n#> [5]  train-rmse:5.285410 test-rmse:6.357400 \n#> [6]  train-rmse:4.227195 test-rmse:5.469514 \n#> [7]  train-rmse:3.530453 test-rmse:4.865071 \n#> [8]  train-rmse:3.043198 test-rmse:4.480110 \n#> [9]  train-rmse:2.743274 test-rmse:4.283322 \n#> [10] train-rmse:2.503662 test-rmse:4.143151 \n#> [11] train-rmse:2.341390 test-rmse:4.081705 \n#> [12] train-rmse:2.223119 test-rmse:3.949371 \n#> [13] train-rmse:2.145158 test-rmse:3.869653 \n#> [14] train-rmse:2.070649 test-rmse:3.838803 \n#> [15] train-rmse:2.018632 test-rmse:3.823521 \n#> [16] train-rmse:1.932049 test-rmse:3.813291 \n#> [17] train-rmse:1.869901 test-rmse:3.774552 \n#> [18] train-rmse:1.834118 test-rmse:3.751064 \n#> [19] train-rmse:1.793967 test-rmse:3.744829 \n#> [20] train-rmse:1.752734 test-rmse:3.733358 \n#> [21] train-rmse:1.702178 test-rmse:3.752904 \n#> [22] train-rmse:1.654633 test-rmse:3.752624 \n#> [23] train-rmse:1.628374 test-rmse:3.750038 \n#> [24] train-rmse:1.595693 test-rmse:3.753647 \n#> [25] train-rmse:1.571161 test-rmse:3.746137 \n#> [26] train-rmse:1.544879 test-rmse:3.745566 \n#> [27] train-rmse:1.517060 test-rmse:3.732738 \n#> [28] train-rmse:1.485936 test-rmse:3.703911 \n#> [29] train-rmse:1.447648 test-rmse:3.696419 \n#> [30] train-rmse:1.413967 test-rmse:3.705468 \n#> [31] train-rmse:1.381059 test-rmse:3.706452 \n#> [32] train-rmse:1.344287 test-rmse:3.709454 \n#> [33] train-rmse:1.305949 test-rmse:3.701470 \n#> [34] train-rmse:1.267424 test-rmse:3.698444 \n#> [35] train-rmse:1.249651 test-rmse:3.686810 \n#> [36] train-rmse:1.242664 test-rmse:3.682350 \n#> [37] train-rmse:1.222246 test-rmse:3.676340 \n#> [38] train-rmse:1.200541 test-rmse:3.672680 \n#> [39] train-rmse:1.183422 test-rmse:3.674608 \n#> [40] train-rmse:1.162316 test-rmse:3.663248 \n#> [41] train-rmse:1.141830 test-rmse:3.655922 \n#> [42] train-rmse:1.128091 test-rmse:3.655636 \n#> [43] train-rmse:1.096402 test-rmse:3.649646 \n#> [44] train-rmse:1.085053 test-rmse:3.645754 \n#> [45] train-rmse:1.065475 test-rmse:3.635741 \n#> [46] train-rmse:1.058745 test-rmse:3.639901 \n#> [47] train-rmse:1.045324 test-rmse:3.631098 \n#> [48] train-rmse:1.020385 test-rmse:3.629003 \n#> [49] train-rmse:1.009853 test-rmse:3.629030 \n#> [50] train-rmse:0.989406 test-rmse:3.628485 \n#> [51] train-rmse:0.979389 test-rmse:3.630748 \n#> [52] train-rmse:0.960420 test-rmse:3.629449 \n#> [53] train-rmse:0.944153 test-rmse:3.632180 \n#> [54] train-rmse:0.932651 test-rmse:3.632644 \n#> [55] train-rmse:0.922669 test-rmse:3.633330 \n#> [56] train-rmse:0.903807 test-rmse:3.629754 \n#> [57] train-rmse:0.875761 test-rmse:3.624106 \n#> [58] train-rmse:0.862721 test-rmse:3.626499 \n#> [59] train-rmse:0.843045 test-rmse:3.622818 \n#> [60] train-rmse:0.829702 test-rmse:3.618196 \n#> [61] train-rmse:0.813128 test-rmse:3.612194 \n#> [62] train-rmse:0.806017 test-rmse:3.609211 \n#> [63] train-rmse:0.792542 test-rmse:3.600465 \n#> [64] train-rmse:0.781655 test-rmse:3.594607 \n#> [65] train-rmse:0.768599 test-rmse:3.583260 \n#> [66] train-rmse:0.759774 test-rmse:3.584345 \n#> [67] train-rmse:0.748496 test-rmse:3.585526 \n#> [68] train-rmse:0.733068 test-rmse:3.584602 \n#> [69] train-rmse:0.727257 test-rmse:3.583001 \n#> [70] train-rmse:0.714099 test-rmse:3.581297 \n#> [1]  train-rmse:17.385765    test-rmse:16.794756 \n#> [2]  train-rmse:12.670146    test-rmse:12.264586 \n#> [3]  train-rmse:9.314821 test-rmse:9.065502 \n#> [4]  train-rmse:7.000134 test-rmse:6.953405 \n#> [5]  train-rmse:5.405117 test-rmse:5.523697 \n#> [6]  train-rmse:4.326401 test-rmse:4.681973 \n#> [7]  train-rmse:3.623907 test-rmse:4.130183 \n#> [8]  train-rmse:3.178648 test-rmse:3.823766 \n#> [9]  train-rmse:2.855376 test-rmse:3.663213 \n#> [10] train-rmse:2.593623 test-rmse:3.573890 \n#> [11] train-rmse:2.439111 test-rmse:3.489961 \n#> [12] train-rmse:2.312032 test-rmse:3.455668 \n#> [13] train-rmse:2.221071 test-rmse:3.439968 \n#> [14] train-rmse:2.108566 test-rmse:3.380220 \n#> [15] train-rmse:2.004805 test-rmse:3.372307 \n#> [16] train-rmse:1.951018 test-rmse:3.361490 \n#> [17] train-rmse:1.891175 test-rmse:3.336966 \n#> [18] train-rmse:1.860596 test-rmse:3.342489 \n#> [19] train-rmse:1.820702 test-rmse:3.337561 \n#> [20] train-rmse:1.770482 test-rmse:3.301025 \n#> [21] train-rmse:1.727679 test-rmse:3.271536 \n#> [22] train-rmse:1.692517 test-rmse:3.275990 \n#> [23] train-rmse:1.643101 test-rmse:3.288711 \n#> [24] train-rmse:1.616868 test-rmse:3.286152 \n#> [25] train-rmse:1.578256 test-rmse:3.285970 \n#> [26] train-rmse:1.515484 test-rmse:3.289535 \n#> [27] train-rmse:1.478759 test-rmse:3.303692 \n#> [28] train-rmse:1.449194 test-rmse:3.311711 \n#> [29] train-rmse:1.408619 test-rmse:3.345546 \n#> [30] train-rmse:1.387780 test-rmse:3.338399 \n#> [31] train-rmse:1.354574 test-rmse:3.330419 \n#> [32] train-rmse:1.322153 test-rmse:3.345416 \n#> [33] train-rmse:1.310442 test-rmse:3.356520 \n#> [34] train-rmse:1.279219 test-rmse:3.359315 \n#> [35] train-rmse:1.262447 test-rmse:3.369878 \n#> [36] train-rmse:1.225963 test-rmse:3.364055 \n#> [37] train-rmse:1.196766 test-rmse:3.366739 \n#> [38] train-rmse:1.177675 test-rmse:3.365982 \n#> [39] train-rmse:1.170192 test-rmse:3.372613 \n#> [40] train-rmse:1.155881 test-rmse:3.374185 \n#> [41] train-rmse:1.127849 test-rmse:3.394838 \n#> [42] train-rmse:1.088085 test-rmse:3.387102 \n#> [43] train-rmse:1.069883 test-rmse:3.384299 \n#> [44] train-rmse:1.060051 test-rmse:3.390040 \n#> [45] train-rmse:1.033736 test-rmse:3.409566 \n#> [46] train-rmse:1.007798 test-rmse:3.414272 \n#> [47] train-rmse:0.997527 test-rmse:3.414263 \n#> [48] train-rmse:0.979780 test-rmse:3.409749 \n#> [49] train-rmse:0.949130 test-rmse:3.415667 \n#> [50] train-rmse:0.939405 test-rmse:3.414446 \n#> [51] train-rmse:0.921432 test-rmse:3.414354 \n#> [52] train-rmse:0.902725 test-rmse:3.418486 \n#> [53] train-rmse:0.889964 test-rmse:3.413368 \n#> [54] train-rmse:0.870040 test-rmse:3.412262 \n#> [55] train-rmse:0.853867 test-rmse:3.410928 \n#> [56] train-rmse:0.834157 test-rmse:3.407093 \n#> [57] train-rmse:0.824457 test-rmse:3.409571 \n#> [58] train-rmse:0.809492 test-rmse:3.410608 \n#> [59] train-rmse:0.799750 test-rmse:3.410038 \n#> [60] train-rmse:0.785987 test-rmse:3.407532 \n#> [61] train-rmse:0.774586 test-rmse:3.406803 \n#> [62] train-rmse:0.765715 test-rmse:3.406814 \n#> [63] train-rmse:0.746053 test-rmse:3.403769 \n#> [64] train-rmse:0.732835 test-rmse:3.408745 \n#> [65] train-rmse:0.721152 test-rmse:3.407695 \n#> [66] train-rmse:0.714833 test-rmse:3.404789 \n#> [67] train-rmse:0.704015 test-rmse:3.406468 \n#> [68] train-rmse:0.687991 test-rmse:3.407474 \n#> [69] train-rmse:0.682328 test-rmse:3.406557 \n#> [70] train-rmse:0.666911 test-rmse:3.406488 \n#> [1]  train-rmse:16.988515    test-rmse:17.627070 \n#> [2]  train-rmse:12.328131    test-rmse:12.957543 \n#> [3]  train-rmse:9.031109 test-rmse:9.525598 \n#> [4]  train-rmse:6.729942 test-rmse:7.223944 \n#> [5]  train-rmse:5.143532 test-rmse:5.704133 \n#> [6]  train-rmse:4.056384 test-rmse:4.679890 \n#> [7]  train-rmse:3.351660 test-rmse:4.103296 \n#> [8]  train-rmse:2.886013 test-rmse:3.758627 \n#> [9]  train-rmse:2.575575 test-rmse:3.524426 \n#> [10] train-rmse:2.374128 test-rmse:3.419636 \n#> [11] train-rmse:2.195364 test-rmse:3.359844 \n#> [12] train-rmse:2.074972 test-rmse:3.272500 \n#> [13] train-rmse:1.961812 test-rmse:3.194225 \n#> [14] train-rmse:1.901956 test-rmse:3.201507 \n#> [15] train-rmse:1.870940 test-rmse:3.186217 \n#> [16] train-rmse:1.802847 test-rmse:3.184240 \n#> [17] train-rmse:1.753162 test-rmse:3.158260 \n#> [18] train-rmse:1.704044 test-rmse:3.149618 \n#> [19] train-rmse:1.678835 test-rmse:3.137091 \n#> [20] train-rmse:1.612179 test-rmse:3.140177 \n#> [21] train-rmse:1.585593 test-rmse:3.130803 \n#> [22] train-rmse:1.572557 test-rmse:3.118183 \n#> [23] train-rmse:1.531643 test-rmse:3.140256 \n#> [24] train-rmse:1.509104 test-rmse:3.133420 \n#> [25] train-rmse:1.476578 test-rmse:3.138309 \n#> [26] train-rmse:1.461619 test-rmse:3.122291 \n#> [27] train-rmse:1.453615 test-rmse:3.123351 \n#> [28] train-rmse:1.420323 test-rmse:3.104772 \n#> [29] train-rmse:1.405515 test-rmse:3.108274 \n#> [30] train-rmse:1.367252 test-rmse:3.110382 \n#> [31] train-rmse:1.355996 test-rmse:3.104787 \n#> [32] train-rmse:1.330491 test-rmse:3.119113 \n#> [33] train-rmse:1.316774 test-rmse:3.121103 \n#> [34] train-rmse:1.303470 test-rmse:3.111014 \n#> [35] train-rmse:1.280759 test-rmse:3.119781 \n#> [36] train-rmse:1.269170 test-rmse:3.115679 \n#> [37] train-rmse:1.234440 test-rmse:3.117343 \n#> [38] train-rmse:1.203275 test-rmse:3.116117 \n#> [39] train-rmse:1.170265 test-rmse:3.119718 \n#> [40] train-rmse:1.161107 test-rmse:3.120461 \n#> [41] train-rmse:1.156858 test-rmse:3.122691 \n#> [42] train-rmse:1.122966 test-rmse:3.107496 \n#> [43] train-rmse:1.099592 test-rmse:3.105319 \n#> [44] train-rmse:1.083191 test-rmse:3.096163 \n#> [45] train-rmse:1.062763 test-rmse:3.106104 \n#> [46] train-rmse:1.049289 test-rmse:3.099127 \n#> [47] train-rmse:1.029801 test-rmse:3.096994 \n#> [48] train-rmse:1.012888 test-rmse:3.100607 \n#> [49] train-rmse:0.996024 test-rmse:3.107249 \n#> [50] train-rmse:0.980183 test-rmse:3.106304 \n#> [51] train-rmse:0.972154 test-rmse:3.102175 \n#> [52] train-rmse:0.966438 test-rmse:3.101945 \n#> [53] train-rmse:0.954552 test-rmse:3.104130 \n#> [54] train-rmse:0.948947 test-rmse:3.098030 \n#> [55] train-rmse:0.929715 test-rmse:3.096164 \n#> [56] train-rmse:0.918693 test-rmse:3.087866 \n#> [57] train-rmse:0.911486 test-rmse:3.085831 \n#> [58] train-rmse:0.896170 test-rmse:3.083192 \n#> [59] train-rmse:0.877669 test-rmse:3.084674 \n#> [60] train-rmse:0.867397 test-rmse:3.091211 \n#> [61] train-rmse:0.861446 test-rmse:3.089973 \n#> [62] train-rmse:0.845836 test-rmse:3.086549 \n#> [63] train-rmse:0.828992 test-rmse:3.081455 \n#> [64] train-rmse:0.812027 test-rmse:3.074534 \n#> [65] train-rmse:0.793364 test-rmse:3.075464 \n#> [66] train-rmse:0.783802 test-rmse:3.078706 \n#> [67] train-rmse:0.779578 test-rmse:3.077914 \n#> [68] train-rmse:0.771363 test-rmse:3.078589 \n#> [69] train-rmse:0.756333 test-rmse:3.074773 \n#> [70] train-rmse:0.742274 test-rmse:3.069944 \n#> [1]  train-rmse:17.321959    test-rmse:16.774258 \n#> [2]  train-rmse:12.561934    test-rmse:12.298570 \n#> [3]  train-rmse:9.286895 test-rmse:9.245498 \n#> [4]  train-rmse:6.933006 test-rmse:7.179272 \n#> [5]  train-rmse:5.328855 test-rmse:5.874374 \n#> [6]  train-rmse:4.253427 test-rmse:5.083352 \n#> [7]  train-rmse:3.548404 test-rmse:4.565694 \n#> [8]  train-rmse:3.072836 test-rmse:4.339920 \n#> [9]  train-rmse:2.714252 test-rmse:4.210879 \n#> [10] train-rmse:2.500005 test-rmse:4.114217 \n#> [11] train-rmse:2.324530 test-rmse:4.050798 \n#> [12] train-rmse:2.200729 test-rmse:4.035300 \n#> [13] train-rmse:2.112021 test-rmse:3.970368 \n#> [14] train-rmse:2.012419 test-rmse:3.940639 \n#> [15] train-rmse:1.951779 test-rmse:3.910923 \n#> [16] train-rmse:1.911819 test-rmse:3.911717 \n#> [17] train-rmse:1.852198 test-rmse:3.871663 \n#> [18] train-rmse:1.795860 test-rmse:3.886116 \n#> [19] train-rmse:1.769939 test-rmse:3.878583 \n#> [20] train-rmse:1.735634 test-rmse:3.863904 \n#> [21] train-rmse:1.693778 test-rmse:3.875972 \n#> [22] train-rmse:1.678357 test-rmse:3.872397 \n#> [23] train-rmse:1.635299 test-rmse:3.838241 \n#> [24] train-rmse:1.607056 test-rmse:3.839355 \n#> [25] train-rmse:1.582201 test-rmse:3.835966 \n#> [26] train-rmse:1.533953 test-rmse:3.832666 \n#> [27] train-rmse:1.523273 test-rmse:3.845692 \n#> [28] train-rmse:1.509610 test-rmse:3.842331 \n#> [29] train-rmse:1.494145 test-rmse:3.842357 \n#> [30] train-rmse:1.432359 test-rmse:3.837947 \n#> [31] train-rmse:1.401277 test-rmse:3.830856 \n#> [32] train-rmse:1.376588 test-rmse:3.831958 \n#> [33] train-rmse:1.349368 test-rmse:3.826925 \n#> [34] train-rmse:1.304012 test-rmse:3.807976 \n#> [35] train-rmse:1.286216 test-rmse:3.810429 \n#> [36] train-rmse:1.253230 test-rmse:3.812646 \n#> [37] train-rmse:1.237035 test-rmse:3.810238 \n#> [38] train-rmse:1.207467 test-rmse:3.793580 \n#> [39] train-rmse:1.180626 test-rmse:3.786512 \n#> [40] train-rmse:1.162318 test-rmse:3.779690 \n#> [41] train-rmse:1.150061 test-rmse:3.782207 \n#> [42] train-rmse:1.132723 test-rmse:3.780529 \n#> [43] train-rmse:1.103424 test-rmse:3.768010 \n#> [44] train-rmse:1.091152 test-rmse:3.744968 \n#> [45] train-rmse:1.075710 test-rmse:3.743391 \n#> [46] train-rmse:1.060600 test-rmse:3.740504 \n#> [47] train-rmse:1.045323 test-rmse:3.745132 \n#> [48] train-rmse:1.032176 test-rmse:3.742003 \n#> [49] train-rmse:1.019129 test-rmse:3.743353 \n#> [50] train-rmse:1.009357 test-rmse:3.739067 \n#> [51] train-rmse:0.998351 test-rmse:3.739515 \n#> [52] train-rmse:0.968744 test-rmse:3.737050 \n#> [53] train-rmse:0.948280 test-rmse:3.733943 \n#> [54] train-rmse:0.922214 test-rmse:3.735062 \n#> [55] train-rmse:0.894645 test-rmse:3.738614 \n#> [56] train-rmse:0.881368 test-rmse:3.735006 \n#> [57] train-rmse:0.862454 test-rmse:3.735164 \n#> [58] train-rmse:0.845040 test-rmse:3.736717 \n#> [59] train-rmse:0.824536 test-rmse:3.742466 \n#> [60] train-rmse:0.817390 test-rmse:3.741655 \n#> [61] train-rmse:0.799640 test-rmse:3.724825 \n#> [62] train-rmse:0.778461 test-rmse:3.727749 \n#> [63] train-rmse:0.768820 test-rmse:3.726508 \n#> [64] train-rmse:0.757031 test-rmse:3.717994 \n#> [65] train-rmse:0.750929 test-rmse:3.719656 \n#> [66] train-rmse:0.732423 test-rmse:3.722865 \n#> [67] train-rmse:0.715445 test-rmse:3.722268 \n#> [68] train-rmse:0.710682 test-rmse:3.718600 \n#> [69] train-rmse:0.702250 test-rmse:3.718964 \n#> [70] train-rmse:0.680833 test-rmse:3.715453 \n#> [1]  train-rmse:17.323344    test-rmse:16.887683 \n#> [2]  train-rmse:12.512398    test-rmse:12.409795 \n#> [3]  train-rmse:9.171131 test-rmse:9.399116 \n#> [4]  train-rmse:6.865599 test-rmse:7.366663 \n#> [5]  train-rmse:5.290222 test-rmse:6.117032 \n#> [6]  train-rmse:4.207435 test-rmse:5.199151 \n#> [7]  train-rmse:3.479597 test-rmse:4.719487 \n#> [8]  train-rmse:2.990186 test-rmse:4.396071 \n#> [9]  train-rmse:2.668102 test-rmse:4.194580 \n#> [10] train-rmse:2.455356 test-rmse:4.098731 \n#> [11] train-rmse:2.323447 test-rmse:3.989118 \n#> [12] train-rmse:2.215922 test-rmse:3.992581 \n#> [13] train-rmse:2.109730 test-rmse:3.948598 \n#> [14] train-rmse:2.054346 test-rmse:3.909207 \n#> [15] train-rmse:1.982701 test-rmse:3.879111 \n#> [16] train-rmse:1.932368 test-rmse:3.877432 \n#> [17] train-rmse:1.818206 test-rmse:3.829744 \n#> [18] train-rmse:1.757775 test-rmse:3.800581 \n#> [19] train-rmse:1.675877 test-rmse:3.767973 \n#> [20] train-rmse:1.654063 test-rmse:3.766804 \n#> [21] train-rmse:1.587751 test-rmse:3.739617 \n#> [22] train-rmse:1.552599 test-rmse:3.726380 \n#> [23] train-rmse:1.520064 test-rmse:3.711067 \n#> [24] train-rmse:1.508405 test-rmse:3.720312 \n#> [25] train-rmse:1.470924 test-rmse:3.735472 \n#> [26] train-rmse:1.442288 test-rmse:3.730755 \n#> [27] train-rmse:1.412341 test-rmse:3.733803 \n#> [28] train-rmse:1.401643 test-rmse:3.732055 \n#> [29] train-rmse:1.382986 test-rmse:3.732116 \n#> [30] train-rmse:1.366508 test-rmse:3.753377 \n#> [31] train-rmse:1.330571 test-rmse:3.753575 \n#> [32] train-rmse:1.311493 test-rmse:3.755554 \n#> [33] train-rmse:1.260804 test-rmse:3.765435 \n#> [34] train-rmse:1.252092 test-rmse:3.766213 \n#> [35] train-rmse:1.225012 test-rmse:3.764774 \n#> [36] train-rmse:1.210006 test-rmse:3.755831 \n#> [37] train-rmse:1.202187 test-rmse:3.760723 \n#> [38] train-rmse:1.188864 test-rmse:3.759230 \n#> [39] train-rmse:1.181029 test-rmse:3.761895 \n#> [40] train-rmse:1.168316 test-rmse:3.766661 \n#> [41] train-rmse:1.138756 test-rmse:3.758480 \n#> [42] train-rmse:1.110167 test-rmse:3.758335 \n#> [43] train-rmse:1.091759 test-rmse:3.761693 \n#> [44] train-rmse:1.061706 test-rmse:3.755408 \n#> [45] train-rmse:1.044165 test-rmse:3.750081 \n#> [46] train-rmse:1.033604 test-rmse:3.741435 \n#> [47] train-rmse:1.011237 test-rmse:3.747237 \n#> [48] train-rmse:0.983079 test-rmse:3.747406 \n#> [49] train-rmse:0.977700 test-rmse:3.750498 \n#> [50] train-rmse:0.970572 test-rmse:3.750838 \n#> [51] train-rmse:0.948303 test-rmse:3.754534 \n#> [52] train-rmse:0.933015 test-rmse:3.748746 \n#> [53] train-rmse:0.913438 test-rmse:3.754442 \n#> [54] train-rmse:0.909884 test-rmse:3.758545 \n#> [55] train-rmse:0.901655 test-rmse:3.770935 \n#> [56] train-rmse:0.894783 test-rmse:3.771958 \n#> [57] train-rmse:0.887991 test-rmse:3.763164 \n#> [58] train-rmse:0.881060 test-rmse:3.762108 \n#> [59] train-rmse:0.867273 test-rmse:3.751898 \n#> [60] train-rmse:0.858725 test-rmse:3.753705 \n#> [61] train-rmse:0.838373 test-rmse:3.763830 \n#> [62] train-rmse:0.830943 test-rmse:3.759233 \n#> [63] train-rmse:0.826514 test-rmse:3.766267 \n#> [64] train-rmse:0.823175 test-rmse:3.766559 \n#> [65] train-rmse:0.807660 test-rmse:3.767533 \n#> [66] train-rmse:0.794098 test-rmse:3.765165 \n#> [67] train-rmse:0.778397 test-rmse:3.770035 \n#> [68] train-rmse:0.772503 test-rmse:3.771494 \n#> [69] train-rmse:0.747584 test-rmse:3.767552 \n#> [70] train-rmse:0.740816 test-rmse:3.764540 \n#> [1]  train-rmse:17.044051    test-rmse:17.598582 \n#> [2]  train-rmse:12.361020    test-rmse:13.070777 \n#> [3]  train-rmse:9.094743 test-rmse:9.941374 \n#> [4]  train-rmse:6.823861 test-rmse:7.838050 \n#> [5]  train-rmse:5.263417 test-rmse:6.425471 \n#> [6]  train-rmse:4.215828 test-rmse:5.438853 \n#> [7]  train-rmse:3.502785 test-rmse:4.984603 \n#> [8]  train-rmse:3.026065 test-rmse:4.662811 \n#> [9]  train-rmse:2.721890 test-rmse:4.363687 \n#> [10] train-rmse:2.495576 test-rmse:4.231543 \n#> [11] train-rmse:2.338947 test-rmse:4.141785 \n#> [12] train-rmse:2.234066 test-rmse:4.081315 \n#> [13] train-rmse:2.158419 test-rmse:4.021757 \n#> [14] train-rmse:2.085335 test-rmse:4.008236 \n#> [15] train-rmse:2.012131 test-rmse:4.001397 \n#> [16] train-rmse:1.933621 test-rmse:3.986264 \n#> [17] train-rmse:1.896756 test-rmse:3.925873 \n#> [18] train-rmse:1.867964 test-rmse:3.925774 \n#> [19] train-rmse:1.829713 test-rmse:3.879238 \n#> [20] train-rmse:1.790533 test-rmse:3.855955 \n#> [21] train-rmse:1.746201 test-rmse:3.861145 \n#> [22] train-rmse:1.690864 test-rmse:3.817658 \n#> [23] train-rmse:1.658930 test-rmse:3.775611 \n#> [24] train-rmse:1.637712 test-rmse:3.777781 \n#> [25] train-rmse:1.595910 test-rmse:3.757286 \n#> [26] train-rmse:1.579251 test-rmse:3.748755 \n#> [27] train-rmse:1.564803 test-rmse:3.744023 \n#> [28] train-rmse:1.543134 test-rmse:3.721144 \n#> [29] train-rmse:1.510350 test-rmse:3.709961 \n#> [30] train-rmse:1.488938 test-rmse:3.700683 \n#> [31] train-rmse:1.472471 test-rmse:3.706287 \n#> [32] train-rmse:1.449769 test-rmse:3.709890 \n#> [33] train-rmse:1.430000 test-rmse:3.714988 \n#> [34] train-rmse:1.397398 test-rmse:3.707030 \n#> [35] train-rmse:1.363955 test-rmse:3.699587 \n#> [36] train-rmse:1.333442 test-rmse:3.698103 \n#> [37] train-rmse:1.299325 test-rmse:3.698002 \n#> [38] train-rmse:1.280210 test-rmse:3.698826 \n#> [39] train-rmse:1.262763 test-rmse:3.698071 \n#> [40] train-rmse:1.241038 test-rmse:3.699788 \n#> [41] train-rmse:1.224637 test-rmse:3.696921 \n#> [42] train-rmse:1.210522 test-rmse:3.684259 \n#> [43] train-rmse:1.202150 test-rmse:3.690122 \n#> [44] train-rmse:1.190017 test-rmse:3.688138 \n#> [45] train-rmse:1.166656 test-rmse:3.684822 \n#> [46] train-rmse:1.155027 test-rmse:3.684041 \n#> [47] train-rmse:1.135007 test-rmse:3.687548 \n#> [48] train-rmse:1.114548 test-rmse:3.682242 \n#> [49] train-rmse:1.092828 test-rmse:3.683085 \n#> [50] train-rmse:1.075588 test-rmse:3.680993 \n#> [51] train-rmse:1.059238 test-rmse:3.664334 \n#> [52] train-rmse:1.037551 test-rmse:3.660962 \n#> [53] train-rmse:1.022884 test-rmse:3.643521 \n#> [54] train-rmse:1.006510 test-rmse:3.640152 \n#> [55] train-rmse:0.972259 test-rmse:3.641487 \n#> [56] train-rmse:0.964384 test-rmse:3.643671 \n#> [57] train-rmse:0.933300 test-rmse:3.642574 \n#> [58] train-rmse:0.924543 test-rmse:3.646467 \n#> [59] train-rmse:0.912426 test-rmse:3.647027 \n#> [60] train-rmse:0.901072 test-rmse:3.644761 \n#> [61] train-rmse:0.894141 test-rmse:3.641944 \n#> [62] train-rmse:0.874174 test-rmse:3.650901 \n#> [63] train-rmse:0.861625 test-rmse:3.654305 \n#> [64] train-rmse:0.847851 test-rmse:3.636964 \n#> [65] train-rmse:0.831352 test-rmse:3.634862 \n#> [66] train-rmse:0.823780 test-rmse:3.629843 \n#> [67] train-rmse:0.812325 test-rmse:3.625395 \n#> [68] train-rmse:0.796279 test-rmse:3.614405 \n#> [69] train-rmse:0.776594 test-rmse:3.617129 \n#> [70] train-rmse:0.767391 test-rmse:3.613929 \n#> [1]  train-rmse:17.170895    test-rmse:17.540865 \n#> [2]  train-rmse:12.518578    test-rmse:13.151335 \n#> [3]  train-rmse:9.206967 test-rmse:9.794633 \n#> [4]  train-rmse:6.914678 test-rmse:7.547047 \n#> [5]  train-rmse:5.320936 test-rmse:5.979701 \n#> [6]  train-rmse:4.300025 test-rmse:5.106135 \n#> [7]  train-rmse:3.628148 test-rmse:4.486839 \n#> [8]  train-rmse:3.167276 test-rmse:4.080448 \n#> [9]  train-rmse:2.856795 test-rmse:3.851353 \n#> [10] train-rmse:2.638298 test-rmse:3.608062 \n#> [11] train-rmse:2.487609 test-rmse:3.467947 \n#> [12] train-rmse:2.366785 test-rmse:3.409284 \n#> [13] train-rmse:2.232873 test-rmse:3.368363 \n#> [14] train-rmse:2.129441 test-rmse:3.277761 \n#> [15] train-rmse:2.052034 test-rmse:3.226649 \n#> [16] train-rmse:2.009219 test-rmse:3.206701 \n#> [17] train-rmse:1.940478 test-rmse:3.175203 \n#> [18] train-rmse:1.893435 test-rmse:3.158259 \n#> [19] train-rmse:1.862652 test-rmse:3.172486 \n#> [20] train-rmse:1.801562 test-rmse:3.186940 \n#> [21] train-rmse:1.770672 test-rmse:3.188426 \n#> [22] train-rmse:1.732201 test-rmse:3.187165 \n#> [23] train-rmse:1.673226 test-rmse:3.164867 \n#> [24] train-rmse:1.656786 test-rmse:3.153857 \n#> [25] train-rmse:1.637156 test-rmse:3.159684 \n#> [26] train-rmse:1.578651 test-rmse:3.160574 \n#> [27] train-rmse:1.542109 test-rmse:3.153191 \n#> [28] train-rmse:1.521919 test-rmse:3.158687 \n#> [29] train-rmse:1.497534 test-rmse:3.150210 \n#> [30] train-rmse:1.480342 test-rmse:3.131334 \n#> [31] train-rmse:1.441255 test-rmse:3.116201 \n#> [32] train-rmse:1.430575 test-rmse:3.124205 \n#> [33] train-rmse:1.400415 test-rmse:3.127266 \n#> [34] train-rmse:1.364291 test-rmse:3.120414 \n#> [35] train-rmse:1.334748 test-rmse:3.113060 \n#> [36] train-rmse:1.302551 test-rmse:3.113071 \n#> [37] train-rmse:1.281874 test-rmse:3.115077 \n#> [38] train-rmse:1.244729 test-rmse:3.108771 \n#> [39] train-rmse:1.225519 test-rmse:3.106393 \n#> [40] train-rmse:1.209392 test-rmse:3.103222 \n#> [41] train-rmse:1.182575 test-rmse:3.103951 \n#> [42] train-rmse:1.171319 test-rmse:3.106968 \n#> [43] train-rmse:1.153272 test-rmse:3.093656 \n#> [44] train-rmse:1.125798 test-rmse:3.090910 \n#> [45] train-rmse:1.103927 test-rmse:3.097984 \n#> [46] train-rmse:1.097017 test-rmse:3.093829 \n#> [47] train-rmse:1.078219 test-rmse:3.091931 \n#> [48] train-rmse:1.065487 test-rmse:3.088405 \n#> [49] train-rmse:1.042989 test-rmse:3.094644 \n#> [50] train-rmse:1.029686 test-rmse:3.099256 \n#> [51] train-rmse:1.012739 test-rmse:3.089073 \n#> [52] train-rmse:1.002601 test-rmse:3.088385 \n#> [53] train-rmse:0.983105 test-rmse:3.093128 \n#> [54] train-rmse:0.969584 test-rmse:3.086581 \n#> [55] train-rmse:0.951424 test-rmse:3.081786 \n#> [56] train-rmse:0.938979 test-rmse:3.083971 \n#> [57] train-rmse:0.920503 test-rmse:3.076928 \n#> [58] train-rmse:0.908392 test-rmse:3.076756 \n#> [59] train-rmse:0.904085 test-rmse:3.078821 \n#> [60] train-rmse:0.880048 test-rmse:3.080675 \n#> [61] train-rmse:0.873038 test-rmse:3.082606 \n#> [62] train-rmse:0.853135 test-rmse:3.073058 \n#> [63] train-rmse:0.844088 test-rmse:3.069322 \n#> [64] train-rmse:0.829665 test-rmse:3.073606 \n#> [65] train-rmse:0.816083 test-rmse:3.071443 \n#> [66] train-rmse:0.802582 test-rmse:3.067585 \n#> [67] train-rmse:0.789541 test-rmse:3.060139 \n#> [68] train-rmse:0.777767 test-rmse:3.068019 \n#> [69] train-rmse:0.771780 test-rmse:3.067337 \n#> [70] train-rmse:0.758804 test-rmse:3.067511 \n#> [1]  train-rmse:17.120072    test-rmse:17.205633 \n#> [2]  train-rmse:12.441414    test-rmse:12.381690 \n#> [3]  train-rmse:9.157665 test-rmse:9.012166 \n#> [4]  train-rmse:6.856267 test-rmse:6.942229 \n#> [5]  train-rmse:5.292154 test-rmse:5.541123 \n#> [6]  train-rmse:4.218657 test-rmse:4.641700 \n#> [7]  train-rmse:3.491688 test-rmse:4.170801 \n#> [8]  train-rmse:3.020847 test-rmse:3.884707 \n#> [9]  train-rmse:2.713909 test-rmse:3.667915 \n#> [10] train-rmse:2.513685 test-rmse:3.561887 \n#> [11] train-rmse:2.342838 test-rmse:3.430305 \n#> [12] train-rmse:2.201219 test-rmse:3.381002 \n#> [13] train-rmse:2.124123 test-rmse:3.381588 \n#> [14] train-rmse:2.060950 test-rmse:3.357941 \n#> [15] train-rmse:2.006157 test-rmse:3.334989 \n#> [16] train-rmse:1.905435 test-rmse:3.314851 \n#> [17] train-rmse:1.860574 test-rmse:3.302205 \n#> [18] train-rmse:1.825841 test-rmse:3.286552 \n#> [19] train-rmse:1.764498 test-rmse:3.255861 \n#> [20] train-rmse:1.734601 test-rmse:3.260446 \n#> [21] train-rmse:1.691928 test-rmse:3.236840 \n#> [22] train-rmse:1.648116 test-rmse:3.235027 \n#> [23] train-rmse:1.631069 test-rmse:3.230025 \n#> [24] train-rmse:1.605170 test-rmse:3.225316 \n#> [25] train-rmse:1.579480 test-rmse:3.213046 \n#> [26] train-rmse:1.540571 test-rmse:3.221617 \n#> [27] train-rmse:1.503467 test-rmse:3.214518 \n#> [28] train-rmse:1.480958 test-rmse:3.210128 \n#> [29] train-rmse:1.463905 test-rmse:3.206344 \n#> [30] train-rmse:1.447949 test-rmse:3.197237 \n#> [31] train-rmse:1.416408 test-rmse:3.202577 \n#> [32] train-rmse:1.400635 test-rmse:3.208664 \n#> [33] train-rmse:1.380655 test-rmse:3.198639 \n#> [34] train-rmse:1.355856 test-rmse:3.181233 \n#> [35] train-rmse:1.335305 test-rmse:3.179564 \n#> [36] train-rmse:1.298753 test-rmse:3.182537 \n#> [37] train-rmse:1.268010 test-rmse:3.173512 \n#> [38] train-rmse:1.241321 test-rmse:3.177074 \n#> [39] train-rmse:1.213382 test-rmse:3.184895 \n#> [40] train-rmse:1.175449 test-rmse:3.180922 \n#> [41] train-rmse:1.141018 test-rmse:3.177054 \n#> [42] train-rmse:1.130889 test-rmse:3.178959 \n#> [43] train-rmse:1.120073 test-rmse:3.183034 \n#> [44] train-rmse:1.113177 test-rmse:3.178551 \n#> [45] train-rmse:1.084548 test-rmse:3.187671 \n#> [46] train-rmse:1.079891 test-rmse:3.188788 \n#> [47] train-rmse:1.047196 test-rmse:3.178450 \n#> [48] train-rmse:1.027513 test-rmse:3.174070 \n#> [49] train-rmse:1.001774 test-rmse:3.154042 \n#> [50] train-rmse:0.977110 test-rmse:3.153992 \n#> [51] train-rmse:0.965694 test-rmse:3.157393 \n#> [52] train-rmse:0.953294 test-rmse:3.152014 \n#> [53] train-rmse:0.930325 test-rmse:3.150492 \n#> [54] train-rmse:0.917532 test-rmse:3.152998 \n#> [55] train-rmse:0.897069 test-rmse:3.150624 \n#> [56] train-rmse:0.888930 test-rmse:3.149678 \n#> [57] train-rmse:0.878738 test-rmse:3.148072 \n#> [58] train-rmse:0.862263 test-rmse:3.144069 \n#> [59] train-rmse:0.846056 test-rmse:3.143937 \n#> [60] train-rmse:0.840649 test-rmse:3.143465 \n#> [61] train-rmse:0.825235 test-rmse:3.146202 \n#> [62] train-rmse:0.818611 test-rmse:3.145530 \n#> [63] train-rmse:0.801086 test-rmse:3.150825 \n#> [64] train-rmse:0.789049 test-rmse:3.158372 \n#> [65] train-rmse:0.784521 test-rmse:3.156429 \n#> [66] train-rmse:0.761073 test-rmse:3.152421 \n#> [67] train-rmse:0.746497 test-rmse:3.149011 \n#> [68] train-rmse:0.738125 test-rmse:3.151721 \n#> [69] train-rmse:0.720135 test-rmse:3.153331 \n#> [70] train-rmse:0.707547 test-rmse:3.163377 \n#> [1]  train-rmse:17.023882    test-rmse:17.563864 \n#> [2]  train-rmse:12.269273    test-rmse:13.191065 \n#> [3]  train-rmse:8.945738 test-rmse:10.222077 \n#> [4]  train-rmse:6.633549 test-rmse:8.228290 \n#> [5]  train-rmse:5.040055 test-rmse:7.011920 \n#> [6]  train-rmse:3.953076 test-rmse:6.163945 \n#> [7]  train-rmse:3.232299 test-rmse:5.642217 \n#> [8]  train-rmse:2.737867 test-rmse:5.318758 \n#> [9]  train-rmse:2.438273 test-rmse:5.118100 \n#> [10] train-rmse:2.224739 test-rmse:5.047133 \n#> [11] train-rmse:2.074463 test-rmse:4.937526 \n#> [12] train-rmse:1.976551 test-rmse:4.878560 \n#> [13] train-rmse:1.897620 test-rmse:4.853702 \n#> [14] train-rmse:1.816070 test-rmse:4.796739 \n#> [15] train-rmse:1.778009 test-rmse:4.791865 \n#> [16] train-rmse:1.739783 test-rmse:4.765143 \n#> [17] train-rmse:1.683148 test-rmse:4.747970 \n#> [18] train-rmse:1.660325 test-rmse:4.747637 \n#> [19] train-rmse:1.605003 test-rmse:4.734799 \n#> [20] train-rmse:1.581467 test-rmse:4.735818 \n#> [21] train-rmse:1.543347 test-rmse:4.720051 \n#> [22] train-rmse:1.516976 test-rmse:4.716742 \n#> [23] train-rmse:1.493828 test-rmse:4.708630 \n#> [24] train-rmse:1.475674 test-rmse:4.714964 \n#> [25] train-rmse:1.444951 test-rmse:4.717055 \n#> [26] train-rmse:1.397475 test-rmse:4.712563 \n#> [27] train-rmse:1.387247 test-rmse:4.701317 \n#> [28] train-rmse:1.358862 test-rmse:4.709803 \n#> [29] train-rmse:1.330462 test-rmse:4.694522 \n#> [30] train-rmse:1.307189 test-rmse:4.688540 \n#> [31] train-rmse:1.295407 test-rmse:4.675326 \n#> [32] train-rmse:1.281960 test-rmse:4.676934 \n#> [33] train-rmse:1.256713 test-rmse:4.671896 \n#> [34] train-rmse:1.229986 test-rmse:4.669061 \n#> [35] train-rmse:1.194477 test-rmse:4.670156 \n#> [36] train-rmse:1.160844 test-rmse:4.685237 \n#> [37] train-rmse:1.143638 test-rmse:4.681791 \n#> [38] train-rmse:1.111245 test-rmse:4.684169 \n#> [39] train-rmse:1.099496 test-rmse:4.673255 \n#> [40] train-rmse:1.064249 test-rmse:4.654325 \n#> [41] train-rmse:1.049932 test-rmse:4.650486 \n#> [42] train-rmse:1.037882 test-rmse:4.656002 \n#> [43] train-rmse:1.009303 test-rmse:4.644184 \n#> [44] train-rmse:0.989942 test-rmse:4.636316 \n#> [45] train-rmse:0.973487 test-rmse:4.632211 \n#> [46] train-rmse:0.963268 test-rmse:4.639360 \n#> [47] train-rmse:0.946897 test-rmse:4.632759 \n#> [48] train-rmse:0.929681 test-rmse:4.628163 \n#> [49] train-rmse:0.925963 test-rmse:4.626612 \n#> [50] train-rmse:0.909648 test-rmse:4.617172 \n#> [51] train-rmse:0.895271 test-rmse:4.610602 \n#> [52] train-rmse:0.867565 test-rmse:4.605412 \n#> [53] train-rmse:0.860844 test-rmse:4.610389 \n#> [54] train-rmse:0.848021 test-rmse:4.609577 \n#> [55] train-rmse:0.837087 test-rmse:4.603181 \n#> [56] train-rmse:0.823987 test-rmse:4.603746 \n#> [57] train-rmse:0.798760 test-rmse:4.593155 \n#> [58] train-rmse:0.789874 test-rmse:4.589935 \n#> [59] train-rmse:0.781605 test-rmse:4.593616 \n#> [60] train-rmse:0.777567 test-rmse:4.590960 \n#> [61] train-rmse:0.760254 test-rmse:4.593655 \n#> [62] train-rmse:0.741331 test-rmse:4.592714 \n#> [63] train-rmse:0.732111 test-rmse:4.592398 \n#> [64] train-rmse:0.729240 test-rmse:4.595556 \n#> [65] train-rmse:0.723922 test-rmse:4.589123 \n#> [66] train-rmse:0.710034 test-rmse:4.589352 \n#> [67] train-rmse:0.700937 test-rmse:4.590984 \n#> [68] train-rmse:0.680924 test-rmse:4.590127 \n#> [69] train-rmse:0.668874 test-rmse:4.582640 \n#> [70] train-rmse:0.660979 test-rmse:4.583414 \n#> [1]  train-rmse:16.632975    test-rmse:18.402482 \n#> [2]  train-rmse:12.084679    test-rmse:14.024370 \n#> [3]  train-rmse:8.872609 test-rmse:10.594402 \n#> [4]  train-rmse:6.666727 test-rmse:8.359604 \n#> [5]  train-rmse:5.135195 test-rmse:6.910258 \n#> [6]  train-rmse:4.118610 test-rmse:6.021983 \n#> [7]  train-rmse:3.421797 test-rmse:5.277892 \n#> [8]  train-rmse:2.984047 test-rmse:4.875201 \n#> [9]  train-rmse:2.666508 test-rmse:4.669088 \n#> [10] train-rmse:2.442604 test-rmse:4.400238 \n#> [11] train-rmse:2.263024 test-rmse:4.230482 \n#> [12] train-rmse:2.121456 test-rmse:4.079175 \n#> [13] train-rmse:2.009415 test-rmse:3.969395 \n#> [14] train-rmse:1.925757 test-rmse:3.935991 \n#> [15] train-rmse:1.877151 test-rmse:3.906905 \n#> [16] train-rmse:1.815046 test-rmse:3.890979 \n#> [17] train-rmse:1.722805 test-rmse:3.843355 \n#> [18] train-rmse:1.678251 test-rmse:3.822660 \n#> [19] train-rmse:1.638035 test-rmse:3.805111 \n#> [20] train-rmse:1.586498 test-rmse:3.772204 \n#> [21] train-rmse:1.555798 test-rmse:3.754926 \n#> [22] train-rmse:1.521831 test-rmse:3.739119 \n#> [23] train-rmse:1.485348 test-rmse:3.720224 \n#> [24] train-rmse:1.452430 test-rmse:3.720954 \n#> [25] train-rmse:1.401942 test-rmse:3.678907 \n#> [26] train-rmse:1.347862 test-rmse:3.626140 \n#> [27] train-rmse:1.333065 test-rmse:3.620101 \n#> [28] train-rmse:1.304386 test-rmse:3.602639 \n#> [29] train-rmse:1.276703 test-rmse:3.607050 \n#> [30] train-rmse:1.264345 test-rmse:3.608845 \n#> [31] train-rmse:1.250547 test-rmse:3.614718 \n#> [32] train-rmse:1.218531 test-rmse:3.599399 \n#> [33] train-rmse:1.177848 test-rmse:3.594882 \n#> [34] train-rmse:1.144331 test-rmse:3.618441 \n#> [35] train-rmse:1.132708 test-rmse:3.619260 \n#> [36] train-rmse:1.121853 test-rmse:3.618322 \n#> [37] train-rmse:1.107662 test-rmse:3.623112 \n#> [38] train-rmse:1.097314 test-rmse:3.622504 \n#> [39] train-rmse:1.091235 test-rmse:3.613645 \n#> [40] train-rmse:1.080512 test-rmse:3.610760 \n#> [41] train-rmse:1.053192 test-rmse:3.602607 \n#> [42] train-rmse:1.040840 test-rmse:3.602440 \n#> [43] train-rmse:1.023147 test-rmse:3.599051 \n#> [44] train-rmse:1.002725 test-rmse:3.593594 \n#> [45] train-rmse:0.975447 test-rmse:3.591580 \n#> [46] train-rmse:0.955695 test-rmse:3.587632 \n#> [47] train-rmse:0.945853 test-rmse:3.589384 \n#> [48] train-rmse:0.940748 test-rmse:3.586710 \n#> [49] train-rmse:0.924930 test-rmse:3.581747 \n#> [50] train-rmse:0.918651 test-rmse:3.579613 \n#> [51] train-rmse:0.906455 test-rmse:3.580222 \n#> [52] train-rmse:0.890968 test-rmse:3.579130 \n#> [53] train-rmse:0.872706 test-rmse:3.568474 \n#> [54] train-rmse:0.858351 test-rmse:3.564070 \n#> [55] train-rmse:0.849300 test-rmse:3.560762 \n#> [56] train-rmse:0.839321 test-rmse:3.562311 \n#> [57] train-rmse:0.822098 test-rmse:3.571710 \n#> [58] train-rmse:0.810739 test-rmse:3.568650 \n#> [59] train-rmse:0.797141 test-rmse:3.564591 \n#> [60] train-rmse:0.784108 test-rmse:3.568265 \n#> [61] train-rmse:0.775899 test-rmse:3.566012 \n#> [62] train-rmse:0.771441 test-rmse:3.564280 \n#> [63] train-rmse:0.760396 test-rmse:3.565345 \n#> [64] train-rmse:0.750564 test-rmse:3.566748 \n#> [65] train-rmse:0.742772 test-rmse:3.570976 \n#> [66] train-rmse:0.726596 test-rmse:3.574597 \n#> [67] train-rmse:0.719990 test-rmse:3.577241 \n#> [68] train-rmse:0.703763 test-rmse:3.587239 \n#> [69] train-rmse:0.688217 test-rmse:3.581395 \n#> [70] train-rmse:0.678489 test-rmse:3.584277 \n#> [1]  train-rmse:17.420586    test-rmse:16.803931 \n#> [2]  train-rmse:12.632942    test-rmse:12.057325 \n#> [3]  train-rmse:9.321768 test-rmse:9.048377 \n#> [4]  train-rmse:7.009684 test-rmse:6.770864 \n#> [5]  train-rmse:5.419812 test-rmse:5.420105 \n#> [6]  train-rmse:4.380042 test-rmse:4.697201 \n#> [7]  train-rmse:3.626264 test-rmse:4.264077 \n#> [8]  train-rmse:3.152862 test-rmse:3.967050 \n#> [9]  train-rmse:2.828154 test-rmse:3.805159 \n#> [10] train-rmse:2.581239 test-rmse:3.609835 \n#> [11] train-rmse:2.400240 test-rmse:3.508118 \n#> [12] train-rmse:2.247041 test-rmse:3.492614 \n#> [13] train-rmse:2.139544 test-rmse:3.480955 \n#> [14] train-rmse:2.047876 test-rmse:3.441028 \n#> [15] train-rmse:1.969279 test-rmse:3.418413 \n#> [16] train-rmse:1.928213 test-rmse:3.406006 \n#> [17] train-rmse:1.859272 test-rmse:3.376072 \n#> [18] train-rmse:1.793148 test-rmse:3.375488 \n#> [19] train-rmse:1.755568 test-rmse:3.356838 \n#> [20] train-rmse:1.716810 test-rmse:3.349448 \n#> [21] train-rmse:1.683483 test-rmse:3.332201 \n#> [22] train-rmse:1.657629 test-rmse:3.339370 \n#> [23] train-rmse:1.610634 test-rmse:3.316619 \n#> [24] train-rmse:1.571209 test-rmse:3.299268 \n#> [25] train-rmse:1.549698 test-rmse:3.306010 \n#> [26] train-rmse:1.538426 test-rmse:3.302109 \n#> [27] train-rmse:1.513778 test-rmse:3.302920 \n#> [28] train-rmse:1.469105 test-rmse:3.318813 \n#> [29] train-rmse:1.424691 test-rmse:3.299384 \n#> [30] train-rmse:1.407023 test-rmse:3.309983 \n#> [31] train-rmse:1.386734 test-rmse:3.318788 \n#> [32] train-rmse:1.350868 test-rmse:3.311731 \n#> [33] train-rmse:1.342330 test-rmse:3.314040 \n#> [34] train-rmse:1.307345 test-rmse:3.307154 \n#> [35] train-rmse:1.291813 test-rmse:3.306095 \n#> [36] train-rmse:1.280663 test-rmse:3.313352 \n#> [37] train-rmse:1.245434 test-rmse:3.308364 \n#> [38] train-rmse:1.230180 test-rmse:3.305575 \n#> [39] train-rmse:1.215316 test-rmse:3.307525 \n#> [40] train-rmse:1.183399 test-rmse:3.301347 \n#> [41] train-rmse:1.160984 test-rmse:3.299754 \n#> [42] train-rmse:1.147713 test-rmse:3.295631 \n#> [43] train-rmse:1.139061 test-rmse:3.289814 \n#> [44] train-rmse:1.112078 test-rmse:3.293741 \n#> [45] train-rmse:1.104979 test-rmse:3.293868 \n#> [46] train-rmse:1.078384 test-rmse:3.290930 \n#> [47] train-rmse:1.058113 test-rmse:3.284939 \n#> [48] train-rmse:1.035028 test-rmse:3.280781 \n#> [49] train-rmse:1.025479 test-rmse:3.277803 \n#> [50] train-rmse:0.995829 test-rmse:3.293224 \n#> [51] train-rmse:0.976585 test-rmse:3.295553 \n#> [52] train-rmse:0.967754 test-rmse:3.294619 \n#> [53] train-rmse:0.946435 test-rmse:3.292584 \n#> [54] train-rmse:0.934532 test-rmse:3.283227 \n#> [55] train-rmse:0.914604 test-rmse:3.281806 \n#> [56] train-rmse:0.909798 test-rmse:3.283366 \n#> [57] train-rmse:0.901885 test-rmse:3.287257 \n#> [58] train-rmse:0.883715 test-rmse:3.281733 \n#> [59] train-rmse:0.869019 test-rmse:3.278942 \n#> [60] train-rmse:0.851390 test-rmse:3.273285 \n#> [61] train-rmse:0.834642 test-rmse:3.279204 \n#> [62] train-rmse:0.826403 test-rmse:3.278465 \n#> [63] train-rmse:0.815742 test-rmse:3.275544 \n#> [64] train-rmse:0.799335 test-rmse:3.275844 \n#> [65] train-rmse:0.785954 test-rmse:3.271935 \n#> [66] train-rmse:0.775118 test-rmse:3.274313 \n#> [67] train-rmse:0.770264 test-rmse:3.265371 \n#> [68] train-rmse:0.763633 test-rmse:3.266875 \n#> [69] train-rmse:0.752399 test-rmse:3.266356 \n#> [70] train-rmse:0.733955 test-rmse:3.254622 \n#> [1]  train-rmse:17.206143    test-rmse:17.310606 \n#> [2]  train-rmse:12.475230    test-rmse:12.761037 \n#> [3]  train-rmse:9.211373 test-rmse:9.676766 \n#> [4]  train-rmse:6.937853 test-rmse:7.458836 \n#> [5]  train-rmse:5.354684 test-rmse:5.988964 \n#> [6]  train-rmse:4.312569 test-rmse:5.071245 \n#> [7]  train-rmse:3.603969 test-rmse:4.505517 \n#> [8]  train-rmse:3.130441 test-rmse:4.239380 \n#> [9]  train-rmse:2.824796 test-rmse:4.091811 \n#> [10] train-rmse:2.606360 test-rmse:3.995039 \n#> [11] train-rmse:2.468222 test-rmse:3.900546 \n#> [12] train-rmse:2.356540 test-rmse:3.883096 \n#> [13] train-rmse:2.228585 test-rmse:3.844089 \n#> [14] train-rmse:2.156309 test-rmse:3.795136 \n#> [15] train-rmse:2.066807 test-rmse:3.756831 \n#> [16] train-rmse:2.020746 test-rmse:3.755588 \n#> [17] train-rmse:1.973536 test-rmse:3.714579 \n#> [18] train-rmse:1.939959 test-rmse:3.742088 \n#> [19] train-rmse:1.909877 test-rmse:3.720112 \n#> [20] train-rmse:1.879773 test-rmse:3.717705 \n#> [21] train-rmse:1.845108 test-rmse:3.694559 \n#> [22] train-rmse:1.821461 test-rmse:3.700721 \n#> [23] train-rmse:1.740543 test-rmse:3.727705 \n#> [24] train-rmse:1.704768 test-rmse:3.713805 \n#> [25] train-rmse:1.622321 test-rmse:3.699541 \n#> [26] train-rmse:1.586920 test-rmse:3.665485 \n#> [27] train-rmse:1.561497 test-rmse:3.654759 \n#> [28] train-rmse:1.535832 test-rmse:3.634603 \n#> [29] train-rmse:1.505243 test-rmse:3.621200 \n#> [30] train-rmse:1.484575 test-rmse:3.616501 \n#> [31] train-rmse:1.459651 test-rmse:3.646291 \n#> [32] train-rmse:1.433545 test-rmse:3.635112 \n#> [33] train-rmse:1.394871 test-rmse:3.627458 \n#> [34] train-rmse:1.339169 test-rmse:3.621203 \n#> [35] train-rmse:1.301722 test-rmse:3.609021 \n#> [36] train-rmse:1.288915 test-rmse:3.603531 \n#> [37] train-rmse:1.273098 test-rmse:3.599050 \n#> [38] train-rmse:1.261219 test-rmse:3.596048 \n#> [39] train-rmse:1.239203 test-rmse:3.594390 \n#> [40] train-rmse:1.222750 test-rmse:3.600512 \n#> [41] train-rmse:1.188663 test-rmse:3.597116 \n#> [42] train-rmse:1.166662 test-rmse:3.589139 \n#> [43] train-rmse:1.156084 test-rmse:3.602857 \n#> [44] train-rmse:1.132052 test-rmse:3.592791 \n#> [45] train-rmse:1.122293 test-rmse:3.587474 \n#> [46] train-rmse:1.104472 test-rmse:3.581073 \n#> [47] train-rmse:1.097093 test-rmse:3.574353 \n#> [48] train-rmse:1.072563 test-rmse:3.567235 \n#> [49] train-rmse:1.042838 test-rmse:3.562499 \n#> [50] train-rmse:1.023958 test-rmse:3.565569 \n#> [51] train-rmse:1.012712 test-rmse:3.562736 \n#> [52] train-rmse:1.004095 test-rmse:3.558615 \n#> [53] train-rmse:0.981545 test-rmse:3.549809 \n#> [54] train-rmse:0.975209 test-rmse:3.548871 \n#> [55] train-rmse:0.967470 test-rmse:3.541378 \n#> [56] train-rmse:0.935473 test-rmse:3.534337 \n#> [57] train-rmse:0.927019 test-rmse:3.542098 \n#> [58] train-rmse:0.904968 test-rmse:3.538517 \n#> [59] train-rmse:0.898375 test-rmse:3.535053 \n#> [60] train-rmse:0.881136 test-rmse:3.534564 \n#> [61] train-rmse:0.869588 test-rmse:3.529943 \n#> [62] train-rmse:0.858381 test-rmse:3.529081 \n#> [63] train-rmse:0.843445 test-rmse:3.528588 \n#> [64] train-rmse:0.825035 test-rmse:3.539658 \n#> [65] train-rmse:0.801397 test-rmse:3.538800 \n#> [66] train-rmse:0.785303 test-rmse:3.536702 \n#> [67] train-rmse:0.764640 test-rmse:3.527970 \n#> [68] train-rmse:0.752227 test-rmse:3.522709 \n#> [69] train-rmse:0.725835 test-rmse:3.522032 \n#> [70] train-rmse:0.717273 test-rmse:3.528527 \n#> [1]  train-rmse:17.497372    test-rmse:16.705213 \n#> [2]  train-rmse:12.675198    test-rmse:12.169965 \n#> [3]  train-rmse:9.319557 test-rmse:8.980046 \n#> [4]  train-rmse:6.985055 test-rmse:6.970945 \n#> [5]  train-rmse:5.390176 test-rmse:5.667425 \n#> [6]  train-rmse:4.319376 test-rmse:4.787396 \n#> [7]  train-rmse:3.576026 test-rmse:4.186170 \n#> [8]  train-rmse:3.091486 test-rmse:3.923885 \n#> [9]  train-rmse:2.737457 test-rmse:3.746149 \n#> [10] train-rmse:2.494671 test-rmse:3.634128 \n#> [11] train-rmse:2.347501 test-rmse:3.570130 \n#> [12] train-rmse:2.189063 test-rmse:3.474114 \n#> [13] train-rmse:2.084397 test-rmse:3.450230 \n#> [14] train-rmse:2.012493 test-rmse:3.413229 \n#> [15] train-rmse:1.960176 test-rmse:3.396044 \n#> [16] train-rmse:1.907340 test-rmse:3.388413 \n#> [17] train-rmse:1.866869 test-rmse:3.386020 \n#> [18] train-rmse:1.820518 test-rmse:3.359068 \n#> [19] train-rmse:1.757013 test-rmse:3.358409 \n#> [20] train-rmse:1.713591 test-rmse:3.347679 \n#> [21] train-rmse:1.658347 test-rmse:3.324007 \n#> [22] train-rmse:1.628121 test-rmse:3.302077 \n#> [23] train-rmse:1.595465 test-rmse:3.295175 \n#> [24] train-rmse:1.573173 test-rmse:3.301710 \n#> [25] train-rmse:1.557127 test-rmse:3.296946 \n#> [26] train-rmse:1.512672 test-rmse:3.267271 \n#> [27] train-rmse:1.479931 test-rmse:3.266640 \n#> [28] train-rmse:1.444716 test-rmse:3.271621 \n#> [29] train-rmse:1.425789 test-rmse:3.270048 \n#> [30] train-rmse:1.410675 test-rmse:3.276684 \n#> [31] train-rmse:1.394562 test-rmse:3.268245 \n#> [32] train-rmse:1.367539 test-rmse:3.283266 \n#> [33] train-rmse:1.339492 test-rmse:3.284132 \n#> [34] train-rmse:1.320548 test-rmse:3.283232 \n#> [35] train-rmse:1.292458 test-rmse:3.304806 \n#> [36] train-rmse:1.250490 test-rmse:3.304315 \n#> [37] train-rmse:1.215407 test-rmse:3.306732 \n#> [38] train-rmse:1.196963 test-rmse:3.315427 \n#> [39] train-rmse:1.159897 test-rmse:3.310838 \n#> [40] train-rmse:1.127439 test-rmse:3.297007 \n#> [41] train-rmse:1.114539 test-rmse:3.290294 \n#> [42] train-rmse:1.092330 test-rmse:3.295700 \n#> [43] train-rmse:1.083105 test-rmse:3.298736 \n#> [44] train-rmse:1.067643 test-rmse:3.295123 \n#> [45] train-rmse:1.033607 test-rmse:3.298065 \n#> [46] train-rmse:1.007633 test-rmse:3.315789 \n#> [47] train-rmse:0.983573 test-rmse:3.312440 \n#> [48] train-rmse:0.964810 test-rmse:3.316091 \n#> [49] train-rmse:0.956272 test-rmse:3.316081 \n#> [50] train-rmse:0.950150 test-rmse:3.319850 \n#> [51] train-rmse:0.943727 test-rmse:3.330000 \n#> [52] train-rmse:0.933783 test-rmse:3.330933 \n#> [53] train-rmse:0.924993 test-rmse:3.329577 \n#> [54] train-rmse:0.910063 test-rmse:3.330154 \n#> [55] train-rmse:0.900702 test-rmse:3.327405 \n#> [56] train-rmse:0.885572 test-rmse:3.316851 \n#> [57] train-rmse:0.866154 test-rmse:3.324717 \n#> [58] train-rmse:0.850172 test-rmse:3.324540 \n#> [59] train-rmse:0.836721 test-rmse:3.334744 \n#> [60] train-rmse:0.824576 test-rmse:3.329172 \n#> [61] train-rmse:0.819755 test-rmse:3.331554 \n#> [62] train-rmse:0.809218 test-rmse:3.330982 \n#> [63] train-rmse:0.801793 test-rmse:3.338507 \n#> [64] train-rmse:0.789447 test-rmse:3.336580 \n#> [65] train-rmse:0.775831 test-rmse:3.334755 \n#> [66] train-rmse:0.763123 test-rmse:3.332028 \n#> [67] train-rmse:0.750921 test-rmse:3.327178 \n#> [68] train-rmse:0.727153 test-rmse:3.324443 \n#> [69] train-rmse:0.716003 test-rmse:3.324771 \n#> [70] train-rmse:0.707029 test-rmse:3.323727 \n#> [1]  train-rmse:17.746953    test-rmse:16.088614 \n#> [2]  train-rmse:12.848058    test-rmse:11.738874 \n#> [3]  train-rmse:9.445858 test-rmse:8.724812 \n#> [4]  train-rmse:7.082045 test-rmse:6.689311 \n#> [5]  train-rmse:5.473613 test-rmse:5.404417 \n#> [6]  train-rmse:4.382359 test-rmse:4.585756 \n#> [7]  train-rmse:3.663277 test-rmse:4.135243 \n#> [8]  train-rmse:3.151459 test-rmse:3.810141 \n#> [9]  train-rmse:2.818541 test-rmse:3.623605 \n#> [10] train-rmse:2.535063 test-rmse:3.503854 \n#> [11] train-rmse:2.347760 test-rmse:3.427004 \n#> [12] train-rmse:2.220339 test-rmse:3.383669 \n#> [13] train-rmse:2.120341 test-rmse:3.333476 \n#> [14] train-rmse:2.032737 test-rmse:3.324670 \n#> [15] train-rmse:1.965348 test-rmse:3.298366 \n#> [16] train-rmse:1.908704 test-rmse:3.267163 \n#> [17] train-rmse:1.854787 test-rmse:3.264975 \n#> [18] train-rmse:1.791170 test-rmse:3.243805 \n#> [19] train-rmse:1.740559 test-rmse:3.214075 \n#> [20] train-rmse:1.704997 test-rmse:3.181556 \n#> [21] train-rmse:1.643535 test-rmse:3.166293 \n#> [22] train-rmse:1.592413 test-rmse:3.165949 \n#> [23] train-rmse:1.565962 test-rmse:3.166594 \n#> [24] train-rmse:1.511071 test-rmse:3.183016 \n#> [25] train-rmse:1.462349 test-rmse:3.165835 \n#> [26] train-rmse:1.444158 test-rmse:3.168277 \n#> [27] train-rmse:1.419945 test-rmse:3.138727 \n#> [28] train-rmse:1.381454 test-rmse:3.132851 \n#> [29] train-rmse:1.342097 test-rmse:3.124041 \n#> [30] train-rmse:1.318796 test-rmse:3.107525 \n#> [31] train-rmse:1.293426 test-rmse:3.107286 \n#> [32] train-rmse:1.281636 test-rmse:3.080113 \n#> [33] train-rmse:1.247643 test-rmse:3.073300 \n#> [34] train-rmse:1.229100 test-rmse:3.081445 \n#> [35] train-rmse:1.207040 test-rmse:3.070193 \n#> [36] train-rmse:1.186971 test-rmse:3.064941 \n#> [37] train-rmse:1.174614 test-rmse:3.060739 \n#> [38] train-rmse:1.152327 test-rmse:3.052294 \n#> [39] train-rmse:1.131991 test-rmse:3.055758 \n#> [40] train-rmse:1.116384 test-rmse:3.052850 \n#> [41] train-rmse:1.110065 test-rmse:3.046765 \n#> [42] train-rmse:1.098135 test-rmse:3.046463 \n#> [43] train-rmse:1.077706 test-rmse:3.038278 \n#> [44] train-rmse:1.051197 test-rmse:3.043176 \n#> [45] train-rmse:1.038141 test-rmse:3.050361 \n#> [46] train-rmse:1.031067 test-rmse:3.046429 \n#> [47] train-rmse:1.010061 test-rmse:3.042669 \n#> [48] train-rmse:0.991994 test-rmse:3.041003 \n#> [49] train-rmse:0.974017 test-rmse:3.043991 \n#> [50] train-rmse:0.954146 test-rmse:3.043860 \n#> [51] train-rmse:0.940643 test-rmse:3.043554 \n#> [52] train-rmse:0.925621 test-rmse:3.048857 \n#> [53] train-rmse:0.907251 test-rmse:3.063045 \n#> [54] train-rmse:0.895753 test-rmse:3.064538 \n#> [55] train-rmse:0.881166 test-rmse:3.064388 \n#> [56] train-rmse:0.866438 test-rmse:3.067355 \n#> [57] train-rmse:0.860904 test-rmse:3.064627 \n#> [58] train-rmse:0.849280 test-rmse:3.071963 \n#> [59] train-rmse:0.834151 test-rmse:3.077752 \n#> [60] train-rmse:0.823731 test-rmse:3.074505 \n#> [61] train-rmse:0.806629 test-rmse:3.072093 \n#> [62] train-rmse:0.795235 test-rmse:3.063652 \n#> [63] train-rmse:0.782851 test-rmse:3.057359 \n#> [64] train-rmse:0.771702 test-rmse:3.057988 \n#> [65] train-rmse:0.761396 test-rmse:3.063430 \n#> [66] train-rmse:0.750726 test-rmse:3.064544 \n#> [67] train-rmse:0.737999 test-rmse:3.063073 \n#> [68] train-rmse:0.726631 test-rmse:3.069981 \n#> [69] train-rmse:0.709756 test-rmse:3.068084 \n#> [70] train-rmse:0.702123 test-rmse:3.072209 \n#> [1]  train-rmse:17.757345    test-rmse:16.205676 \n#> [2]  train-rmse:12.875820    test-rmse:11.608635 \n#> [3]  train-rmse:9.462338 test-rmse:8.523874 \n#> [4]  train-rmse:7.056919 test-rmse:6.321476 \n#> [5]  train-rmse:5.456711 test-rmse:5.045230 \n#> [6]  train-rmse:4.360768 test-rmse:4.358151 \n#> [7]  train-rmse:3.590120 test-rmse:3.811730 \n#> [8]  train-rmse:3.122081 test-rmse:3.503827 \n#> [9]  train-rmse:2.821735 test-rmse:3.434037 \n#> [10] train-rmse:2.600017 test-rmse:3.309007 \n#> [11] train-rmse:2.433657 test-rmse:3.292402 \n#> [12] train-rmse:2.328481 test-rmse:3.270210 \n#> [13] train-rmse:2.235187 test-rmse:3.215807 \n#> [14] train-rmse:2.159686 test-rmse:3.195468 \n#> [15] train-rmse:2.070844 test-rmse:3.175675 \n#> [16] train-rmse:2.011279 test-rmse:3.197592 \n#> [17] train-rmse:1.975715 test-rmse:3.198668 \n#> [18] train-rmse:1.928552 test-rmse:3.204729 \n#> [19] train-rmse:1.870943 test-rmse:3.208802 \n#> [20] train-rmse:1.821949 test-rmse:3.203317 \n#> [21] train-rmse:1.797158 test-rmse:3.200699 \n#> [22] train-rmse:1.752589 test-rmse:3.163103 \n#> [23] train-rmse:1.720680 test-rmse:3.155974 \n#> [24] train-rmse:1.660530 test-rmse:3.160287 \n#> [25] train-rmse:1.625455 test-rmse:3.158182 \n#> [26] train-rmse:1.601794 test-rmse:3.164349 \n#> [27] train-rmse:1.588628 test-rmse:3.175391 \n#> [28] train-rmse:1.537036 test-rmse:3.163229 \n#> [29] train-rmse:1.500139 test-rmse:3.181958 \n#> [30] train-rmse:1.470186 test-rmse:3.189175 \n#> [31] train-rmse:1.445744 test-rmse:3.181643 \n#> [32] train-rmse:1.422065 test-rmse:3.176888 \n#> [33] train-rmse:1.381573 test-rmse:3.190551 \n#> [34] train-rmse:1.350240 test-rmse:3.181641 \n#> [35] train-rmse:1.334542 test-rmse:3.180492 \n#> [36] train-rmse:1.317453 test-rmse:3.186737 \n#> [37] train-rmse:1.296085 test-rmse:3.202003 \n#> [38] train-rmse:1.269124 test-rmse:3.203128 \n#> [39] train-rmse:1.250360 test-rmse:3.193755 \n#> [40] train-rmse:1.223990 test-rmse:3.186283 \n#> [41] train-rmse:1.191271 test-rmse:3.195541 \n#> [42] train-rmse:1.180929 test-rmse:3.198217 \n#> [43] train-rmse:1.154820 test-rmse:3.204580 \n#> [44] train-rmse:1.144795 test-rmse:3.201658 \n#> [45] train-rmse:1.136989 test-rmse:3.204554 \n#> [46] train-rmse:1.123091 test-rmse:3.206007 \n#> [47] train-rmse:1.101891 test-rmse:3.189574 \n#> [48] train-rmse:1.085828 test-rmse:3.179985 \n#> [49] train-rmse:1.055641 test-rmse:3.189480 \n#> [50] train-rmse:1.042670 test-rmse:3.200033 \n#> [51] train-rmse:1.030204 test-rmse:3.198881 \n#> [52] train-rmse:1.009598 test-rmse:3.198746 \n#> [53] train-rmse:0.994990 test-rmse:3.206915 \n#> [54] train-rmse:0.989716 test-rmse:3.208727 \n#> [55] train-rmse:0.976126 test-rmse:3.205484 \n#> [56] train-rmse:0.964928 test-rmse:3.209258 \n#> [57] train-rmse:0.946495 test-rmse:3.203329 \n#> [58] train-rmse:0.921372 test-rmse:3.201259 \n#> [59] train-rmse:0.907611 test-rmse:3.204599 \n#> [60] train-rmse:0.889665 test-rmse:3.206765 \n#> [61] train-rmse:0.864126 test-rmse:3.207128 \n#> [62] train-rmse:0.853117 test-rmse:3.206127 \n#> [63] train-rmse:0.846385 test-rmse:3.204270 \n#> [64] train-rmse:0.826482 test-rmse:3.208616 \n#> [65] train-rmse:0.813973 test-rmse:3.204869 \n#> [66] train-rmse:0.807103 test-rmse:3.203939 \n#> [67] train-rmse:0.789126 test-rmse:3.203539 \n#> [68] train-rmse:0.781736 test-rmse:3.204418 \n#> [69] train-rmse:0.768352 test-rmse:3.194666 \n#> [70] train-rmse:0.748397 test-rmse:3.191436 \n#> [1]  train-rmse:17.153358    test-rmse:17.247926 \n#> [2]  train-rmse:12.441687    test-rmse:12.652032 \n#> [3]  train-rmse:9.187360 test-rmse:9.469573 \n#> [4]  train-rmse:6.922300 test-rmse:7.287823 \n#> [5]  train-rmse:5.318238 test-rmse:5.978485 \n#> [6]  train-rmse:4.268214 test-rmse:5.063714 \n#> [7]  train-rmse:3.581960 test-rmse:4.560218 \n#> [8]  train-rmse:3.042221 test-rmse:4.064226 \n#> [9]  train-rmse:2.724658 test-rmse:3.933850 \n#> [10] train-rmse:2.495427 test-rmse:3.809304 \n#> [11] train-rmse:2.316863 test-rmse:3.620897 \n#> [12] train-rmse:2.204720 test-rmse:3.567965 \n#> [13] train-rmse:2.092047 test-rmse:3.518681 \n#> [14] train-rmse:2.028509 test-rmse:3.495556 \n#> [15] train-rmse:1.955107 test-rmse:3.423853 \n#> [16] train-rmse:1.909453 test-rmse:3.412761 \n#> [17] train-rmse:1.827261 test-rmse:3.333483 \n#> [18] train-rmse:1.793152 test-rmse:3.316521 \n#> [19] train-rmse:1.756339 test-rmse:3.302823 \n#> [20] train-rmse:1.698962 test-rmse:3.318053 \n#> [21] train-rmse:1.673975 test-rmse:3.318148 \n#> [22] train-rmse:1.652197 test-rmse:3.300669 \n#> [23] train-rmse:1.627174 test-rmse:3.287246 \n#> [24] train-rmse:1.580979 test-rmse:3.260976 \n#> [25] train-rmse:1.547157 test-rmse:3.254116 \n#> [26] train-rmse:1.515574 test-rmse:3.249121 \n#> [27] train-rmse:1.471449 test-rmse:3.265800 \n#> [28] train-rmse:1.443393 test-rmse:3.275661 \n#> [29] train-rmse:1.417479 test-rmse:3.269999 \n#> [30] train-rmse:1.402362 test-rmse:3.274996 \n#> [31] train-rmse:1.394953 test-rmse:3.269014 \n#> [32] train-rmse:1.375341 test-rmse:3.261593 \n#> [33] train-rmse:1.353052 test-rmse:3.260496 \n#> [34] train-rmse:1.332705 test-rmse:3.262382 \n#> [35] train-rmse:1.324803 test-rmse:3.256628 \n#> [36] train-rmse:1.285419 test-rmse:3.225293 \n#> [37] train-rmse:1.258540 test-rmse:3.228784 \n#> [38] train-rmse:1.246189 test-rmse:3.224981 \n#> [39] train-rmse:1.229884 test-rmse:3.197306 \n#> [40] train-rmse:1.207481 test-rmse:3.191800 \n#> [41] train-rmse:1.194144 test-rmse:3.191253 \n#> [42] train-rmse:1.178394 test-rmse:3.189043 \n#> [43] train-rmse:1.159140 test-rmse:3.198097 \n#> [44] train-rmse:1.132824 test-rmse:3.184262 \n#> [45] train-rmse:1.115965 test-rmse:3.187304 \n#> [46] train-rmse:1.081659 test-rmse:3.183274 \n#> [47] train-rmse:1.049598 test-rmse:3.185394 \n#> [48] train-rmse:1.035387 test-rmse:3.185552 \n#> [49] train-rmse:1.023945 test-rmse:3.185979 \n#> [50] train-rmse:0.998014 test-rmse:3.184844 \n#> [51] train-rmse:0.981964 test-rmse:3.182987 \n#> [52] train-rmse:0.974405 test-rmse:3.184497 \n#> [53] train-rmse:0.961793 test-rmse:3.188064 \n#> [54] train-rmse:0.951966 test-rmse:3.185168 \n#> [55] train-rmse:0.945683 test-rmse:3.183075 \n#> [56] train-rmse:0.935008 test-rmse:3.181035 \n#> [57] train-rmse:0.932093 test-rmse:3.184060 \n#> [58] train-rmse:0.917324 test-rmse:3.189954 \n#> [59] train-rmse:0.906029 test-rmse:3.187350 \n#> [60] train-rmse:0.889095 test-rmse:3.189489 \n#> [61] train-rmse:0.878756 test-rmse:3.189283 \n#> [62] train-rmse:0.872081 test-rmse:3.190268 \n#> [63] train-rmse:0.861711 test-rmse:3.186995 \n#> [64] train-rmse:0.849970 test-rmse:3.188500 \n#> [65] train-rmse:0.832303 test-rmse:3.176611 \n#> [66] train-rmse:0.818736 test-rmse:3.185935 \n#> [67] train-rmse:0.805083 test-rmse:3.183172 \n#> [68] train-rmse:0.798027 test-rmse:3.176018 \n#> [69] train-rmse:0.788944 test-rmse:3.181227 \n#> [70] train-rmse:0.778040 test-rmse:3.174212 \n#> [1]  train-rmse:17.079528    test-rmse:17.377109 \n#> [2]  train-rmse:12.384860    test-rmse:12.625349 \n#> [3]  train-rmse:9.165314 test-rmse:9.311551 \n#> [4]  train-rmse:6.893540 test-rmse:7.021701 \n#> [5]  train-rmse:5.309133 test-rmse:5.393646 \n#> [6]  train-rmse:4.271522 test-rmse:4.430744 \n#> [7]  train-rmse:3.561669 test-rmse:3.837535 \n#> [8]  train-rmse:3.064646 test-rmse:3.522244 \n#> [9]  train-rmse:2.769855 test-rmse:3.251925 \n#> [10] train-rmse:2.561877 test-rmse:3.167872 \n#> [11] train-rmse:2.420761 test-rmse:3.088785 \n#> [12] train-rmse:2.296889 test-rmse:3.020767 \n#> [13] train-rmse:2.209150 test-rmse:2.976007 \n#> [14] train-rmse:2.157451 test-rmse:2.961612 \n#> [15] train-rmse:2.054361 test-rmse:2.959658 \n#> [16] train-rmse:2.005199 test-rmse:2.964704 \n#> [17] train-rmse:1.941198 test-rmse:2.960652 \n#> [18] train-rmse:1.881997 test-rmse:2.935266 \n#> [19] train-rmse:1.822826 test-rmse:2.920812 \n#> [20] train-rmse:1.790203 test-rmse:2.904936 \n#> [21] train-rmse:1.760105 test-rmse:2.891240 \n#> [22] train-rmse:1.707131 test-rmse:2.896128 \n#> [23] train-rmse:1.662741 test-rmse:2.901868 \n#> [24] train-rmse:1.633643 test-rmse:2.898332 \n#> [25] train-rmse:1.608720 test-rmse:2.891816 \n#> [26] train-rmse:1.566568 test-rmse:2.928903 \n#> [27] train-rmse:1.542651 test-rmse:2.920520 \n#> [28] train-rmse:1.514065 test-rmse:2.928540 \n#> [29] train-rmse:1.478750 test-rmse:2.930061 \n#> [30] train-rmse:1.462296 test-rmse:2.917730 \n#> [31] train-rmse:1.421458 test-rmse:2.910186 \n#> [32] train-rmse:1.392717 test-rmse:2.909048 \n#> [33] train-rmse:1.360096 test-rmse:2.901942 \n#> [34] train-rmse:1.348711 test-rmse:2.895804 \n#> [35] train-rmse:1.331264 test-rmse:2.881988 \n#> [36] train-rmse:1.301336 test-rmse:2.895174 \n#> [37] train-rmse:1.270702 test-rmse:2.877713 \n#> [38] train-rmse:1.246264 test-rmse:2.900819 \n#> [39] train-rmse:1.237540 test-rmse:2.903270 \n#> [40] train-rmse:1.204781 test-rmse:2.899446 \n#> [41] train-rmse:1.186700 test-rmse:2.888834 \n#> [42] train-rmse:1.167358 test-rmse:2.898562 \n#> [43] train-rmse:1.145142 test-rmse:2.901068 \n#> [44] train-rmse:1.137023 test-rmse:2.896725 \n#> [45] train-rmse:1.110583 test-rmse:2.896236 \n#> [46] train-rmse:1.088840 test-rmse:2.889204 \n#> [47] train-rmse:1.079157 test-rmse:2.888155 \n#> [48] train-rmse:1.070868 test-rmse:2.884162 \n#> [49] train-rmse:1.059954 test-rmse:2.880864 \n#> [50] train-rmse:1.042157 test-rmse:2.888545 \n#> [51] train-rmse:1.017776 test-rmse:2.878213 \n#> [52] train-rmse:0.988539 test-rmse:2.874488 \n#> [53] train-rmse:0.971234 test-rmse:2.868676 \n#> [54] train-rmse:0.956523 test-rmse:2.863017 \n#> [55] train-rmse:0.941375 test-rmse:2.862686 \n#> [56] train-rmse:0.927121 test-rmse:2.861085 \n#> [57] train-rmse:0.919926 test-rmse:2.855945 \n#> [58] train-rmse:0.912473 test-rmse:2.865741 \n#> [59] train-rmse:0.899319 test-rmse:2.865994 \n#> [60] train-rmse:0.886971 test-rmse:2.860850 \n#> [61] train-rmse:0.875922 test-rmse:2.856875 \n#> [62] train-rmse:0.870501 test-rmse:2.855029 \n#> [63] train-rmse:0.853772 test-rmse:2.853909 \n#> [64] train-rmse:0.847366 test-rmse:2.860050 \n#> [65] train-rmse:0.831146 test-rmse:2.859409 \n#> [66] train-rmse:0.818748 test-rmse:2.848222 \n#> [67] train-rmse:0.806568 test-rmse:2.842176 \n#> [68] train-rmse:0.799445 test-rmse:2.833018 \n#> [69] train-rmse:0.784608 test-rmse:2.814668 \n#> [70] train-rmse:0.774472 test-rmse:2.817666 \n#> [1]  train-rmse:17.174497    test-rmse:17.264833 \n#> [2]  train-rmse:12.466474    test-rmse:12.619905 \n#> [3]  train-rmse:9.148978 test-rmse:9.390790 \n#> [4]  train-rmse:6.885397 test-rmse:7.204675 \n#> [5]  train-rmse:5.279459 test-rmse:5.841540 \n#> [6]  train-rmse:4.242885 test-rmse:5.017919 \n#> [7]  train-rmse:3.486216 test-rmse:4.416454 \n#> [8]  train-rmse:3.015869 test-rmse:4.114004 \n#> [9]  train-rmse:2.708992 test-rmse:3.924625 \n#> [10] train-rmse:2.490774 test-rmse:3.806508 \n#> [11] train-rmse:2.333321 test-rmse:3.718458 \n#> [12] train-rmse:2.227711 test-rmse:3.679917 \n#> [13] train-rmse:2.141888 test-rmse:3.660030 \n#> [14] train-rmse:2.082735 test-rmse:3.674052 \n#> [15] train-rmse:1.996223 test-rmse:3.611874 \n#> [16] train-rmse:1.960814 test-rmse:3.588602 \n#> [17] train-rmse:1.903594 test-rmse:3.572304 \n#> [18] train-rmse:1.854117 test-rmse:3.560568 \n#> [19] train-rmse:1.821896 test-rmse:3.559986 \n#> [20] train-rmse:1.744679 test-rmse:3.563565 \n#> [21] train-rmse:1.720437 test-rmse:3.557131 \n#> [22] train-rmse:1.687881 test-rmse:3.559080 \n#> [23] train-rmse:1.647137 test-rmse:3.538544 \n#> [24] train-rmse:1.618445 test-rmse:3.541620 \n#> [25] train-rmse:1.557838 test-rmse:3.483289 \n#> [26] train-rmse:1.535155 test-rmse:3.493737 \n#> [27] train-rmse:1.495709 test-rmse:3.496462 \n#> [28] train-rmse:1.475953 test-rmse:3.497386 \n#> [29] train-rmse:1.451411 test-rmse:3.506798 \n#> [30] train-rmse:1.427954 test-rmse:3.507897 \n#> [31] train-rmse:1.402805 test-rmse:3.493114 \n#> [32] train-rmse:1.367876 test-rmse:3.466924 \n#> [33] train-rmse:1.340332 test-rmse:3.460579 \n#> [34] train-rmse:1.316756 test-rmse:3.466749 \n#> [35] train-rmse:1.281867 test-rmse:3.467192 \n#> [36] train-rmse:1.272643 test-rmse:3.455104 \n#> [37] train-rmse:1.254233 test-rmse:3.457215 \n#> [38] train-rmse:1.225753 test-rmse:3.449404 \n#> [39] train-rmse:1.209607 test-rmse:3.452840 \n#> [40] train-rmse:1.190480 test-rmse:3.455185 \n#> [41] train-rmse:1.179451 test-rmse:3.454639 \n#> [42] train-rmse:1.161244 test-rmse:3.452408 \n#> [43] train-rmse:1.149862 test-rmse:3.457055 \n#> [44] train-rmse:1.130977 test-rmse:3.462610 \n#> [45] train-rmse:1.111614 test-rmse:3.462926 \n#> [46] train-rmse:1.085415 test-rmse:3.457275 \n#> [47] train-rmse:1.070715 test-rmse:3.472729 \n#> [48] train-rmse:1.060586 test-rmse:3.472500 \n#> [49] train-rmse:1.053762 test-rmse:3.461818 \n#> [50] train-rmse:1.032720 test-rmse:3.463770 \n#> [51] train-rmse:1.007656 test-rmse:3.464170 \n#> [52] train-rmse:0.997822 test-rmse:3.464671 \n#> [53] train-rmse:0.985003 test-rmse:3.463193 \n#> [54] train-rmse:0.973618 test-rmse:3.464843 \n#> [55] train-rmse:0.953144 test-rmse:3.461074 \n#> [56] train-rmse:0.936272 test-rmse:3.460912 \n#> [57] train-rmse:0.921454 test-rmse:3.468553 \n#> [58] train-rmse:0.914720 test-rmse:3.476876 \n#> [59] train-rmse:0.905542 test-rmse:3.472793 \n#> [60] train-rmse:0.884523 test-rmse:3.474688 \n#> [61] train-rmse:0.862461 test-rmse:3.472315 \n#> [62] train-rmse:0.848348 test-rmse:3.478386 \n#> [63] train-rmse:0.835287 test-rmse:3.478017 \n#> [64] train-rmse:0.826955 test-rmse:3.480483 \n#> [65] train-rmse:0.816417 test-rmse:3.479547 \n#> [66] train-rmse:0.811387 test-rmse:3.490310 \n#> [67] train-rmse:0.806071 test-rmse:3.492417 \n#> [68] train-rmse:0.795524 test-rmse:3.500655 \n#> [69] train-rmse:0.782489 test-rmse:3.503188 \n#> [70] train-rmse:0.774461 test-rmse:3.500021 \n#> [1]  train-rmse:16.779350    test-rmse:18.207919 \n#> [2]  train-rmse:12.171179    test-rmse:13.445699 \n#> [3]  train-rmse:8.927165 test-rmse:10.283390 \n#> [4]  train-rmse:6.696618 test-rmse:8.345708 \n#> [5]  train-rmse:5.136253 test-rmse:6.780196 \n#> [6]  train-rmse:4.105116 test-rmse:5.797880 \n#> [7]  train-rmse:3.415105 test-rmse:5.308461 \n#> [8]  train-rmse:2.943451 test-rmse:4.918321 \n#> [9]  train-rmse:2.658312 test-rmse:4.680732 \n#> [10] train-rmse:2.454329 test-rmse:4.567930 \n#> [11] train-rmse:2.281712 test-rmse:4.379558 \n#> [12] train-rmse:2.198491 test-rmse:4.335912 \n#> [13] train-rmse:2.071788 test-rmse:4.304987 \n#> [14] train-rmse:1.977886 test-rmse:4.205243 \n#> [15] train-rmse:1.894389 test-rmse:4.182313 \n#> [16] train-rmse:1.854751 test-rmse:4.159849 \n#> [17] train-rmse:1.808117 test-rmse:4.152690 \n#> [18] train-rmse:1.756165 test-rmse:4.155184 \n#> [19] train-rmse:1.717520 test-rmse:4.161827 \n#> [20] train-rmse:1.684331 test-rmse:4.163554 \n#> [21] train-rmse:1.646353 test-rmse:4.162023 \n#> [22] train-rmse:1.623560 test-rmse:4.156331 \n#> [23] train-rmse:1.575575 test-rmse:4.121608 \n#> [24] train-rmse:1.552324 test-rmse:4.118739 \n#> [25] train-rmse:1.506545 test-rmse:4.111395 \n#> [26] train-rmse:1.482322 test-rmse:4.104987 \n#> [27] train-rmse:1.435784 test-rmse:4.090974 \n#> [28] train-rmse:1.413571 test-rmse:4.090047 \n#> [29] train-rmse:1.376245 test-rmse:4.078865 \n#> [30] train-rmse:1.343570 test-rmse:4.054152 \n#> [31] train-rmse:1.329362 test-rmse:4.052484 \n#> [32] train-rmse:1.287098 test-rmse:4.053874 \n#> [33] train-rmse:1.266878 test-rmse:4.055464 \n#> [34] train-rmse:1.230480 test-rmse:4.036984 \n#> [35] train-rmse:1.207075 test-rmse:4.024112 \n#> [36] train-rmse:1.175968 test-rmse:4.021759 \n#> [37] train-rmse:1.140574 test-rmse:3.998033 \n#> [38] train-rmse:1.127684 test-rmse:3.998154 \n#> [39] train-rmse:1.104108 test-rmse:3.994652 \n#> [40] train-rmse:1.082294 test-rmse:4.015869 \n#> [41] train-rmse:1.049164 test-rmse:4.023144 \n#> [42] train-rmse:1.021966 test-rmse:4.025873 \n#> [43] train-rmse:1.000787 test-rmse:4.022535 \n#> [44] train-rmse:0.985148 test-rmse:4.018350 \n#> [45] train-rmse:0.975522 test-rmse:4.013462 \n#> [46] train-rmse:0.955586 test-rmse:4.018598 \n#> [47] train-rmse:0.936245 test-rmse:4.013409 \n#> [48] train-rmse:0.926763 test-rmse:4.014300 \n#> [49] train-rmse:0.916715 test-rmse:4.005393 \n#> [50] train-rmse:0.902231 test-rmse:4.002152 \n#> [51] train-rmse:0.887179 test-rmse:4.001605 \n#> [52] train-rmse:0.873935 test-rmse:4.002588 \n#> [53] train-rmse:0.865345 test-rmse:4.001689 \n#> [54] train-rmse:0.857539 test-rmse:4.001055 \n#> [55] train-rmse:0.843897 test-rmse:4.009306 \n#> [56] train-rmse:0.829447 test-rmse:4.002423 \n#> [57] train-rmse:0.817708 test-rmse:4.001942 \n#> [58] train-rmse:0.804459 test-rmse:4.002660 \n#> [59] train-rmse:0.791290 test-rmse:3.990508 \n#> [60] train-rmse:0.784867 test-rmse:3.989261 \n#> [61] train-rmse:0.767499 test-rmse:3.980973 \n#> [62] train-rmse:0.749397 test-rmse:3.985475 \n#> [63] train-rmse:0.740022 test-rmse:3.986033 \n#> [64] train-rmse:0.731210 test-rmse:3.987539 \n#> [65] train-rmse:0.725246 test-rmse:3.995125 \n#> [66] train-rmse:0.712231 test-rmse:3.996640 \n#> [67] train-rmse:0.701003 test-rmse:3.995766 \n#> [68] train-rmse:0.687988 test-rmse:3.997611 \n#> [69] train-rmse:0.672621 test-rmse:3.992692 \n#> [70] train-rmse:0.660912 test-rmse:3.992390 \n#> [1]  train-rmse:17.644143    test-rmse:16.369328 \n#> [2]  train-rmse:12.794632    test-rmse:11.929539 \n#> [3]  train-rmse:9.413120 test-rmse:8.985036 \n#> [4]  train-rmse:7.030341 test-rmse:7.014112 \n#> [5]  train-rmse:5.395858 test-rmse:5.668539 \n#> [6]  train-rmse:4.284418 test-rmse:4.805353 \n#> [7]  train-rmse:3.532857 test-rmse:4.321955 \n#> [8]  train-rmse:3.062053 test-rmse:4.041404 \n#> [9]  train-rmse:2.727744 test-rmse:3.880189 \n#> [10] train-rmse:2.437574 test-rmse:3.759992 \n#> [11] train-rmse:2.271091 test-rmse:3.729054 \n#> [12] train-rmse:2.158544 test-rmse:3.660379 \n#> [13] train-rmse:2.056166 test-rmse:3.614323 \n#> [14] train-rmse:2.008425 test-rmse:3.581949 \n#> [15] train-rmse:1.963335 test-rmse:3.562357 \n#> [16] train-rmse:1.892000 test-rmse:3.550607 \n#> [17] train-rmse:1.838552 test-rmse:3.593554 \n#> [18] train-rmse:1.806830 test-rmse:3.585898 \n#> [19] train-rmse:1.736500 test-rmse:3.585468 \n#> [20] train-rmse:1.680566 test-rmse:3.569657 \n#> [21] train-rmse:1.641793 test-rmse:3.554016 \n#> [22] train-rmse:1.621415 test-rmse:3.546730 \n#> [23] train-rmse:1.602346 test-rmse:3.569991 \n#> [24] train-rmse:1.576039 test-rmse:3.560067 \n#> [25] train-rmse:1.541867 test-rmse:3.543751 \n#> [26] train-rmse:1.521339 test-rmse:3.536509 \n#> [27] train-rmse:1.495745 test-rmse:3.541574 \n#> [28] train-rmse:1.479090 test-rmse:3.548629 \n#> [29] train-rmse:1.461375 test-rmse:3.553321 \n#> [30] train-rmse:1.431670 test-rmse:3.549392 \n#> [31] train-rmse:1.404947 test-rmse:3.552035 \n#> [32] train-rmse:1.363493 test-rmse:3.549040 \n#> [33] train-rmse:1.341505 test-rmse:3.548294 \n#> [34] train-rmse:1.334772 test-rmse:3.545150 \n#> [35] train-rmse:1.304933 test-rmse:3.532957 \n#> [36] train-rmse:1.267487 test-rmse:3.538322 \n#> [37] train-rmse:1.256616 test-rmse:3.535697 \n#> [38] train-rmse:1.240113 test-rmse:3.522899 \n#> [39] train-rmse:1.221487 test-rmse:3.520452 \n#> [40] train-rmse:1.211621 test-rmse:3.527968 \n#> [41] train-rmse:1.187889 test-rmse:3.522384 \n#> [42] train-rmse:1.176825 test-rmse:3.516388 \n#> [43] train-rmse:1.151752 test-rmse:3.521675 \n#> [44] train-rmse:1.143956 test-rmse:3.520928 \n#> [45] train-rmse:1.125564 test-rmse:3.533644 \n#> [46] train-rmse:1.091267 test-rmse:3.526531 \n#> [47] train-rmse:1.080519 test-rmse:3.526824 \n#> [48] train-rmse:1.068470 test-rmse:3.523016 \n#> [49] train-rmse:1.027419 test-rmse:3.513712 \n#> [50] train-rmse:0.999058 test-rmse:3.510759 \n#> [51] train-rmse:0.980671 test-rmse:3.507549 \n#> [52] train-rmse:0.958796 test-rmse:3.499991 \n#> [53] train-rmse:0.952550 test-rmse:3.504307 \n#> [54] train-rmse:0.932789 test-rmse:3.490623 \n#> [55] train-rmse:0.924410 test-rmse:3.496458 \n#> [56] train-rmse:0.919869 test-rmse:3.498303 \n#> [57] train-rmse:0.902118 test-rmse:3.499676 \n#> [58] train-rmse:0.885793 test-rmse:3.499246 \n#> [59] train-rmse:0.863765 test-rmse:3.484370 \n#> [60] train-rmse:0.847728 test-rmse:3.477669 \n#> [61] train-rmse:0.836233 test-rmse:3.475560 \n#> [62] train-rmse:0.825694 test-rmse:3.471706 \n#> [63] train-rmse:0.817225 test-rmse:3.472442 \n#> [64] train-rmse:0.803572 test-rmse:3.472471 \n#> [65] train-rmse:0.796228 test-rmse:3.472661 \n#> [66] train-rmse:0.779853 test-rmse:3.468595 \n#> [67] train-rmse:0.773933 test-rmse:3.467307 \n#> [68] train-rmse:0.764835 test-rmse:3.459772 \n#> [69] train-rmse:0.761240 test-rmse:3.459294 \n#> [70] train-rmse:0.755777 test-rmse:3.459214 \n#> [1]  train-rmse:17.684751    test-rmse:16.500476 \n#> [2]  train-rmse:12.793272    test-rmse:11.937067 \n#> [3]  train-rmse:9.401096 test-rmse:8.867871 \n#> [4]  train-rmse:7.007000 test-rmse:6.741049 \n#> [5]  train-rmse:5.403779 test-rmse:5.400790 \n#> [6]  train-rmse:4.300519 test-rmse:4.586066 \n#> [7]  train-rmse:3.573068 test-rmse:4.168921 \n#> [8]  train-rmse:3.040646 test-rmse:3.871437 \n#> [9]  train-rmse:2.696866 test-rmse:3.704518 \n#> [10] train-rmse:2.477213 test-rmse:3.618700 \n#> [11] train-rmse:2.334902 test-rmse:3.575200 \n#> [12] train-rmse:2.231843 test-rmse:3.554308 \n#> [13] train-rmse:2.147690 test-rmse:3.488702 \n#> [14] train-rmse:2.063000 test-rmse:3.411661 \n#> [15] train-rmse:1.997862 test-rmse:3.390124 \n#> [16] train-rmse:1.950103 test-rmse:3.368649 \n#> [17] train-rmse:1.903784 test-rmse:3.362779 \n#> [18] train-rmse:1.873729 test-rmse:3.361938 \n#> [19] train-rmse:1.830452 test-rmse:3.356057 \n#> [20] train-rmse:1.797668 test-rmse:3.327144 \n#> [21] train-rmse:1.715509 test-rmse:3.297517 \n#> [22] train-rmse:1.651769 test-rmse:3.295284 \n#> [23] train-rmse:1.614815 test-rmse:3.294483 \n#> [24] train-rmse:1.589045 test-rmse:3.279542 \n#> [25] train-rmse:1.557201 test-rmse:3.268105 \n#> [26] train-rmse:1.509195 test-rmse:3.242756 \n#> [27] train-rmse:1.493377 test-rmse:3.237224 \n#> [28] train-rmse:1.454785 test-rmse:3.246490 \n#> [29] train-rmse:1.415584 test-rmse:3.247198 \n#> [30] train-rmse:1.400648 test-rmse:3.249404 \n#> [31] train-rmse:1.381394 test-rmse:3.253635 \n#> [32] train-rmse:1.350793 test-rmse:3.255294 \n#> [33] train-rmse:1.313821 test-rmse:3.237575 \n#> [34] train-rmse:1.284443 test-rmse:3.244345 \n#> [35] train-rmse:1.248565 test-rmse:3.249575 \n#> [36] train-rmse:1.230584 test-rmse:3.242797 \n#> [37] train-rmse:1.205088 test-rmse:3.241367 \n#> [38] train-rmse:1.189449 test-rmse:3.225965 \n#> [39] train-rmse:1.161576 test-rmse:3.236384 \n#> [40] train-rmse:1.132401 test-rmse:3.232241 \n#> [41] train-rmse:1.117285 test-rmse:3.238234 \n#> [42] train-rmse:1.108723 test-rmse:3.241522 \n#> [43] train-rmse:1.077248 test-rmse:3.257564 \n#> [44] train-rmse:1.069290 test-rmse:3.265557 \n#> [45] train-rmse:1.046985 test-rmse:3.257532 \n#> [46] train-rmse:1.041975 test-rmse:3.266573 \n#> [47] train-rmse:1.025338 test-rmse:3.261892 \n#> [48] train-rmse:1.019173 test-rmse:3.263357 \n#> [49] train-rmse:1.006527 test-rmse:3.264308 \n#> [50] train-rmse:0.999569 test-rmse:3.263293 \n#> [51] train-rmse:0.990109 test-rmse:3.265685 \n#> [52] train-rmse:0.976736 test-rmse:3.267067 \n#> [53] train-rmse:0.950653 test-rmse:3.272872 \n#> [54] train-rmse:0.942109 test-rmse:3.264780 \n#> [55] train-rmse:0.937606 test-rmse:3.274221 \n#> [56] train-rmse:0.922337 test-rmse:3.272372 \n#> [57] train-rmse:0.902798 test-rmse:3.268210 \n#> [58] train-rmse:0.889290 test-rmse:3.267985 \n#> [59] train-rmse:0.883539 test-rmse:3.270015 \n#> [60] train-rmse:0.862743 test-rmse:3.261608 \n#> [61] train-rmse:0.846788 test-rmse:3.266571 \n#> [62] train-rmse:0.835584 test-rmse:3.261188 \n#> [63] train-rmse:0.816480 test-rmse:3.257493 \n#> [64] train-rmse:0.805986 test-rmse:3.247675 \n#> [65] train-rmse:0.791130 test-rmse:3.246013 \n#> [66] train-rmse:0.779006 test-rmse:3.244313 \n#> [67] train-rmse:0.762253 test-rmse:3.239574 \n#> [68] train-rmse:0.747020 test-rmse:3.240649 \n#> [69] train-rmse:0.741704 test-rmse:3.240951 \n#> [70] train-rmse:0.728181 test-rmse:3.239402 \n#> [1]  train-rmse:17.500320    test-rmse:16.577935 \n#> [2]  train-rmse:12.700782    test-rmse:12.026178 \n#> [3]  train-rmse:9.374607 test-rmse:8.883171 \n#> [4]  train-rmse:7.019407 test-rmse:6.866400 \n#> [5]  train-rmse:5.400458 test-rmse:5.585809 \n#> [6]  train-rmse:4.289105 test-rmse:4.803949 \n#> [7]  train-rmse:3.574879 test-rmse:4.343962 \n#> [8]  train-rmse:3.081702 test-rmse:4.088238 \n#> [9]  train-rmse:2.760779 test-rmse:3.924238 \n#> [10] train-rmse:2.528432 test-rmse:3.842127 \n#> [11] train-rmse:2.371529 test-rmse:3.794781 \n#> [12] train-rmse:2.258979 test-rmse:3.748469 \n#> [13] train-rmse:2.167540 test-rmse:3.738536 \n#> [14] train-rmse:2.045130 test-rmse:3.704941 \n#> [15] train-rmse:1.995062 test-rmse:3.693663 \n#> [16] train-rmse:1.931012 test-rmse:3.685969 \n#> [17] train-rmse:1.894994 test-rmse:3.695818 \n#> [18] train-rmse:1.844343 test-rmse:3.687795 \n#> [19] train-rmse:1.798131 test-rmse:3.695430 \n#> [20] train-rmse:1.739249 test-rmse:3.654595 \n#> [21] train-rmse:1.700298 test-rmse:3.601839 \n#> [22] train-rmse:1.649804 test-rmse:3.576904 \n#> [23] train-rmse:1.630151 test-rmse:3.563748 \n#> [24] train-rmse:1.581735 test-rmse:3.553712 \n#> [25] train-rmse:1.567455 test-rmse:3.537648 \n#> [26] train-rmse:1.539873 test-rmse:3.529298 \n#> [27] train-rmse:1.525838 test-rmse:3.522211 \n#> [28] train-rmse:1.490469 test-rmse:3.526956 \n#> [29] train-rmse:1.443220 test-rmse:3.521233 \n#> [30] train-rmse:1.408247 test-rmse:3.518112 \n#> [31] train-rmse:1.391281 test-rmse:3.508094 \n#> [32] train-rmse:1.361160 test-rmse:3.513120 \n#> [33] train-rmse:1.336057 test-rmse:3.514023 \n#> [34] train-rmse:1.315465 test-rmse:3.508017 \n#> [35] train-rmse:1.290935 test-rmse:3.511331 \n#> [36] train-rmse:1.281778 test-rmse:3.508712 \n#> [37] train-rmse:1.263491 test-rmse:3.503052 \n#> [38] train-rmse:1.251507 test-rmse:3.493851 \n#> [39] train-rmse:1.241208 test-rmse:3.493632 \n#> [40] train-rmse:1.221975 test-rmse:3.485807 \n#> [41] train-rmse:1.196739 test-rmse:3.490003 \n#> [42] train-rmse:1.172633 test-rmse:3.490853 \n#> [43] train-rmse:1.152701 test-rmse:3.485805 \n#> [44] train-rmse:1.138961 test-rmse:3.487476 \n#> [45] train-rmse:1.109313 test-rmse:3.475582 \n#> [46] train-rmse:1.100207 test-rmse:3.469868 \n#> [47] train-rmse:1.090441 test-rmse:3.470839 \n#> [48] train-rmse:1.084244 test-rmse:3.469064 \n#> [49] train-rmse:1.077133 test-rmse:3.469469 \n#> [50] train-rmse:1.055232 test-rmse:3.461626 \n#> [51] train-rmse:1.037104 test-rmse:3.462026 \n#> [52] train-rmse:1.019209 test-rmse:3.470342 \n#> [53] train-rmse:1.012538 test-rmse:3.471178 \n#> [54] train-rmse:0.994863 test-rmse:3.461374 \n#> [55] train-rmse:0.965409 test-rmse:3.453446 \n#> [56] train-rmse:0.953566 test-rmse:3.450462 \n#> [57] train-rmse:0.940082 test-rmse:3.452440 \n#> [58] train-rmse:0.919081 test-rmse:3.452695 \n#> [59] train-rmse:0.903928 test-rmse:3.460760 \n#> [60] train-rmse:0.883478 test-rmse:3.462497 \n#> [61] train-rmse:0.877497 test-rmse:3.464765 \n#> [62] train-rmse:0.872967 test-rmse:3.463513 \n#> [63] train-rmse:0.858929 test-rmse:3.466758 \n#> [64] train-rmse:0.849939 test-rmse:3.462989 \n#> [65] train-rmse:0.832749 test-rmse:3.465008 \n#> [66] train-rmse:0.820749 test-rmse:3.467366 \n#> [67] train-rmse:0.799739 test-rmse:3.470420 \n#> [68] train-rmse:0.781536 test-rmse:3.467527 \n#> [69] train-rmse:0.776637 test-rmse:3.467608 \n#> [70] train-rmse:0.756403 test-rmse:3.466820 \n#> [1]  train-rmse:17.402750    test-rmse:16.672237 \n#> [2]  train-rmse:12.613865    test-rmse:12.068149 \n#> [3]  train-rmse:9.287956 test-rmse:8.946529 \n#> [4]  train-rmse:6.933049 test-rmse:6.872692 \n#> [5]  train-rmse:5.333376 test-rmse:5.543267 \n#> [6]  train-rmse:4.232659 test-rmse:4.710821 \n#> [7]  train-rmse:3.518554 test-rmse:4.225892 \n#> [8]  train-rmse:3.065487 test-rmse:3.984023 \n#> [9]  train-rmse:2.744047 test-rmse:3.822014 \n#> [10] train-rmse:2.538783 test-rmse:3.735906 \n#> [11] train-rmse:2.391563 test-rmse:3.711962 \n#> [12] train-rmse:2.249077 test-rmse:3.637363 \n#> [13] train-rmse:2.145327 test-rmse:3.639505 \n#> [14] train-rmse:2.071817 test-rmse:3.617094 \n#> [15] train-rmse:2.021594 test-rmse:3.594986 \n#> [16] train-rmse:1.948477 test-rmse:3.571381 \n#> [17] train-rmse:1.896602 test-rmse:3.555057 \n#> [18] train-rmse:1.823711 test-rmse:3.546802 \n#> [19] train-rmse:1.803024 test-rmse:3.535221 \n#> [20] train-rmse:1.757521 test-rmse:3.523680 \n#> [21] train-rmse:1.719815 test-rmse:3.505092 \n#> [22] train-rmse:1.673410 test-rmse:3.516841 \n#> [23] train-rmse:1.644435 test-rmse:3.522851 \n#> [24] train-rmse:1.620934 test-rmse:3.504999 \n#> [25] train-rmse:1.567626 test-rmse:3.513036 \n#> [26] train-rmse:1.537336 test-rmse:3.501669 \n#> [27] train-rmse:1.521802 test-rmse:3.507671 \n#> [28] train-rmse:1.477356 test-rmse:3.515000 \n#> [29] train-rmse:1.463061 test-rmse:3.511038 \n#> [30] train-rmse:1.443271 test-rmse:3.516023 \n#> [31] train-rmse:1.429034 test-rmse:3.500366 \n#> [32] train-rmse:1.409486 test-rmse:3.513899 \n#> [33] train-rmse:1.367020 test-rmse:3.530032 \n#> [34] train-rmse:1.351872 test-rmse:3.529120 \n#> [35] train-rmse:1.340334 test-rmse:3.534267 \n#> [36] train-rmse:1.306939 test-rmse:3.527196 \n#> [37] train-rmse:1.285643 test-rmse:3.530349 \n#> [38] train-rmse:1.265536 test-rmse:3.536605 \n#> [39] train-rmse:1.254320 test-rmse:3.527217 \n#> [40] train-rmse:1.240069 test-rmse:3.540603 \n#> [41] train-rmse:1.219325 test-rmse:3.545401 \n#> [42] train-rmse:1.204344 test-rmse:3.538206 \n#> [43] train-rmse:1.173548 test-rmse:3.537709 \n#> [44] train-rmse:1.166427 test-rmse:3.544395 \n#> [45] train-rmse:1.140547 test-rmse:3.563474 \n#> [46] train-rmse:1.128052 test-rmse:3.568552 \n#> [47] train-rmse:1.120925 test-rmse:3.571155 \n#> [48] train-rmse:1.092155 test-rmse:3.586326 \n#> [49] train-rmse:1.062622 test-rmse:3.584373 \n#> [50] train-rmse:1.044057 test-rmse:3.581142 \n#> [51] train-rmse:1.013747 test-rmse:3.573442 \n#> [52] train-rmse:1.004978 test-rmse:3.573550 \n#> [53] train-rmse:0.979008 test-rmse:3.571830 \n#> [54] train-rmse:0.973746 test-rmse:3.575230 \n#> [55] train-rmse:0.963112 test-rmse:3.577156 \n#> [56] train-rmse:0.948297 test-rmse:3.554859 \n#> [57] train-rmse:0.927097 test-rmse:3.557713 \n#> [58] train-rmse:0.903443 test-rmse:3.564630 \n#> [59] train-rmse:0.886617 test-rmse:3.571356 \n#> [60] train-rmse:0.870010 test-rmse:3.568536 \n#> [61] train-rmse:0.855544 test-rmse:3.565833 \n#> [62] train-rmse:0.848755 test-rmse:3.568257 \n#> [63] train-rmse:0.838522 test-rmse:3.567631 \n#> [64] train-rmse:0.817649 test-rmse:3.561768 \n#> [65] train-rmse:0.803753 test-rmse:3.558734 \n#> [66] train-rmse:0.799405 test-rmse:3.560965 \n#> [67] train-rmse:0.794757 test-rmse:3.556835 \n#> [68] train-rmse:0.779397 test-rmse:3.554966 \n#> [69] train-rmse:0.768486 test-rmse:3.557377 \n#> [70] train-rmse:0.762586 test-rmse:3.558965\n#> [1] 3.538308\nwarnings() # no warnings for individual XGBoost function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"exercise-pick-four-of-the-functions-and-apply-them-with-other-numerical-data-sets.","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.27 Exercise: Pick four of the functions and apply them with other numerical data sets.","text":"","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"post-the-results-of-your-work.","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.28 Post the results of your work.","text":"","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"test-four-of-your-functions-on-a-neutral-data-set-mean-0-sd-1.-post-your-results-compare-most-accurate-to-least-accurate-results.","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.29 Test four of your functions on a neutral data set (mean = 0, sd = 1). Post your results, compare most accurate to least accurate results.","text":"","code":"\n\nbland <- data.frame(\n  X1 = rnorm(n = 1000, mean = 0, sd = 1),\n  X2 = rnorm(n = 1000, mean = 0, sd = 1),\n  X3 = rnorm(n = 1000, mean = 0, sd = 1),\n  X4 = rnorm(n = 1000, mean = 0, sd = 1),\n  y = rnorm(n = 1000, mean = 0, sd = 1)\n)\n# test with cubist function:\ncubist_1(data = bland, colnum = 5, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 1.004373\n\n# apply with lqs function:\nlqs1(data = bland, colnum = 5, train_amount = 0.60, test_amount = 0.40, validation_amount = 0.00, numresamples = 25)\n#> [1] 1.052853"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"building-weighted-ensembles-to-model-numerical-data","chapter":"4 Building weighted ensembles to model numerical data","heading":"4 Building weighted ensembles to model numerical data","text":"last chapter learned make 23 individual models,\nincluding calculating error rate (root mean squared error), \npredictions holdout data (test validation).chapter show use results make weighted\nensembles can used model numerical data.Let’s start end. Let’s imagine finished product. list \nensembles individual models, error rates sorted decreasing\norder (best result top list)Therefore ’re going need weighted ensemble. weight \nuse?turns excellent answer available virtually \nwork part (great!). model mean error score.\nweight use reciprocal error.Let’s say two models. One error rate 5.0 \nerror rate 2.0. Clearly model error rate 2.0\nsuperior model error rate 5.0.building ensemble multiply values \nensemble 1/(error rate). give higher weights models \nhigher accuracy.Let’s see works extremely simple ensemble.section making weighted ensemble\nusing 17 models numerical data, using ensemble measure\naccuracy models holdout (test) data.","code":"\nlibrary(tree) # Allows us to use tree models\nlibrary(MASS) # For the Boston Housing data set library(Metrics)\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ✖ dplyr::select() masks MASS::select()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Set initial values to 0\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_RMSE <- 0\nlinear_test_predict_value <- 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_RMSE <- 0\ntree_holdout_RMSE <- 0\ntree_test_predict_value <- 0\n\nensemble_linear_RMSE <- 0\nensemble_linear_RMSE_mean <- 0\nensemble_tree_RMSE <- 0\nensemble_tree_RMSE_mean <- 0\n\nnumerical_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Move target column to far right\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Set up resampling\nfor (i in 1:numresamples) {\n  idx <- sample(seq(1, 2), size = nrow(data), replace = TRUE, prob = c(train_amount, test_amount))\n  train <- data[idx == 1, ]\n  test <- data[idx == 2, ]\n\n# Fit linear model on the training data, make predictions on the test data\nlinear_model <- lm(y ~ ., data = train)\nlinear_predictions <- predict(object = linear_model, newdata = test)\nlinear_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = linear_predictions)\nlinear_RMSE_mean <- mean(linear_RMSE)\n\n# Fit tree model on the training data, make predictions on the test data\ntree_model <- tree(y ~ ., data = train)\ntree_predictions <- predict(object = tree_model, newdata = test)\ntree_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = tree_predictions)\ntree_RMSE_mean <- mean(tree_RMSE)\n\n# Make the weighted ensemble\nensemble <- data.frame(\n  'linear' = linear_predictions / linear_RMSE_mean,\n  'tree' = tree_predictions / tree_RMSE_mean,\n  'y_ensemble' = test$y)\n\n# Split ensemble between train and test\nensemble_idx <- sample(seq(1, 2), size = nrow(ensemble), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble[ensemble_idx == 1, ]\nensemble_test <- ensemble[ensemble_idx == 2, ]\n\n# Fit the ensemble data on the ensemble training data, predict on ensemble test data\nensemble_linear_model <- lm(y_ensemble ~ ., data = ensemble_train)\n\nensemble_linear_predictions <- predict(object = ensemble_linear_model, newdata = ensemble_test)\n\nensemble_linear_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_linear_predictions)\n\nensemble_linear_RMSE_mean <- mean(ensemble_linear_RMSE)\n\n# Fit the tree model on the ensemble training data, predict on ensemble test data\nensemble_tree_model <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree_model, newdata = ensemble_test) \n\nensemble_tree_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_tree_predictions)\n\nensemble_tree_RMSE_mean <- mean(ensemble_tree_RMSE)\n\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble_Linear', 'Ensemble_tree'),\n  'Error_Rate' = c(linear_RMSE_mean, tree_RMSE_mean, ensemble_linear_RMSE_mean, ensemble_tree_RMSE_mean)\n)\n\nresults <- results %>% arrange(Error_Rate)\n\n} # Closing brace for numresamples\nreturn(list(results))\n\n} # Closing brace for the function\n\nnumerical_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [[1]]\n#>             Model Error_Rate\n#> 1 Ensemble_Linear   4.142535\n#> 2   Ensemble_tree   4.728397\n#> 3          Linear   4.829952\n#> 4            Tree   4.940936\n\nwarnings()"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"think-before-you-do-something.-this-will-help-when-we-start-at-the-end-and-work-backwards-toward-the-beginning.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.1 Think before you do something. This will help when we start at the end and work backwards toward the beginning.","text":"going make ensemble. ensemble going made \npredictions numerical models. ’ve already seen couple \nensembles. one extremely similar, involve \nmodels.Starting end, want error rate (root mean squared error)\nensemble prediction models.means ’ll need make ensemble. means ’ll need\nindividual model predictions, ’s ensemble made. \nensemble made individual model predictions, means ’ll\nneed individual models. already know , \nlast chapter.’re going build simple ensemble seven models, use\nensemble four different methods. Ensembles package\nactually works total 40 different models. process \nexactly , whether working seven individual models \n23 individual models, five ensemble models 17 ensemble models. \nstructre methods .let’s get started!seven individual models building :Linear (tuned)Linear (tuned)BayesglmBayesglmBayesrnnBayesrnnGradient BoostedGradient BoostedRandomForestRandomForestTreesTreesIt’s important understand many options possible. \nencouraged add least one modeling method ensemble,\nsee impacts results.","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"one-of-your-own-add-one-model-to-the-list-of-seven-individual-models-see-how-it-impacts-results.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.2 One of your own: Add one model to the list of seven individual models, see how it impacts results.","text":"","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"plan-ahead-as-much-as-you-can-that-makes-the-entire-model-building-process-much-easier.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.3 Plan ahead as much as you can, that makes the entire model building process much easier.","text":"code build ensembles. strongly recommend \n, checking every 5-10 lines make sure \nerrors.","code":"\n\n#Load packages we will need\n\nlibrary(arm) # Allows us to run bayesglm\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russellconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\nlibrary(brnn) # Allows us to run brnn\n#> Loading required package: Formula\n#> Loading required package: truncnorm\nlibrary(e1071) # Allows us to run several tuned model, such as linear and KNN\nlibrary(randomForest) # Allows us to run random forest models\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\nlibrary(tree) # Allows us to run tree models"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"a-few-other-packages-we-will-need-to-keep-everything-running-smoothly","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.3.1 A few other packages we will need to keep everything running smoothly","text":"","code":"\nlibrary(tidyverse) # Amazing set of tools for data science\nlibrary(MASS) # Gives us the Boston Housing data set\nlibrary(Metrics) # Allows us to calculate accuracy or error rates"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"build-the-function-that-will-build-the-individual-and-ensemble-models","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.3.2 Build the function that will build the individual and ensemble models","text":"’s cool part setting way. \ntotally different data set, need put information\nfunction, everything runs. Check :","code":"\n\nnumerical <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Make the target column the right most column, change the column name to y:\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ]\n\n# Set initial values to 0 for both individual and ensemble methods:\n\nbayesglm_train_RMSE <- 0\nbayesglm_test_RMSE <- 0\nbayesglm_validation_RMSE <- 0\nbayesglm_sd <- 0\nbayesglm_overfitting <- 0\nbayesglm_duration <- 0\nbayesglm_duration_mean <- 0\nbayesglm_holdout_mean <- 0\nbayesglm_holdout_RMSE <- 0\nbayesglm_holdout_RMSE_mean <- 0\n\nbayesrnn_train_RMSE <- 0\nbayesrnn_test_RMSE <- 0\nbayesrnn_validation_RMSE <- 0\nbayesrnn_sd <- 0\nbayesrnn_overfitting <- 0\nbayesrnn_duration <- 0\nbayesrnn_duration_mean <- 0\nbayesrnn_holdout_mean <- 0\nbayesrnn_holdout_RMSE <- 0\nbayesrnn_holdout_RMSE_mean <- 0\n\ngb_train_RMSE <- 0\ngb_test_RMSE <- 0\ngb_validation_RMSE <- 0\ngb_sd <- 0\ngb_overfitting <- 0\ngb_duration <- 0\ngb_duration_mean <- 0\ngb_holdout_mean <- 0\ngb_holdout_RMSE <- 0\ngb_holdout_RMSE_mean <- 0\n\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_validation_RMSE <- 0\nlinear_sd <- 0\nlinear_overfitting <- 0\nlinear_duration <- 0\nlinear_holdout_RMSE <- 0\nlinear_holdout_RMSE_mean <- 0\n\nrf_train_RMSE <- 0\nrf_test_RMSE <- 0\nrf_validation_RMSE <- 0\nrf_sd <- 0\nrf_overfitting <- 0\nrf_duration <- 0\nrf_duration_mean <- 0\nrf_holdout_mean <- 0\nrf_holdout_RMSE <- 0\nrf_holdout_RMSE_mean <- 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_validation_RMSE <- 0\ntree_sd <- 0\ntree_overfitting <- 0\ntree_duration <- 0\ntree_duration_mean <- 0\ntree_holdout_mean <- 0\ntree_holdout_RMSE <- 0\ntree_holdout_RMSE_mean <- 0\n\nensemble_bayesglm_train_RMSE <- 0\nensemble_bayesglm_test_RMSE <- 0\nensemble_bayesglm_validation_RMSE <- 0\nensemble_bayesglm_sd <- 0\nensemble_bayesglm_overfitting <- 0\nensemble_bayesglm_duration <- 0\nensemble_bayesglm_holdout_RMSE <- 0\nensemble_bayesglm_holdout_RMSE_mean <- 0\nensemble_bayesglm_predict_value_mean <- 0\n\nensemble_bayesrnn_train_RMSE <- 0\nensemble_bayesrnn_test_RMSE <- 0\nensemble_bayesrnn_validation_RMSE <- 0\nensemble_bayesrnn_sd <- 0\nensemble_bayesrnn_overfitting <- 0\nensemble_bayesrnn_duration <- 0\nensemble_bayesrnn_holdout_RMSE <- 0\nensemble_bayesrnn_holdout_RMSE_mean <- 0\nensemble_bayesrnn_predict_value_mean <- 0\n\nensemble_gb_train_RMSE <- 0\nensemble_gb_test_RMSE <- 0\nensemble_gb_validation_RMSE <- 0\nensemble_gb_sd <- 0\nensemble_gb_overfitting <- 0\nensemble_gb_duration <- 0\nensemble_gb_holdout_RMSE <- 0\nensemble_gb_holdout_RMSE_mean <- 0\nensemble_gb_predict_value_mean <- 0\n\nensemble_linear_train_RMSE <- 0\nensemble_linear_test_RMSE <- 0\nensemble_linear_validation_RMSE <- 0\nensemble_linear_sd <- 0\nensemble_linear_overfitting <- 0\nensemble_linear_duration <- 0\nensemble_linear_holdout_RMSE <- 0\nensemble_linear_holdout_RMSE_mean <- 0\n\nensemble_rf_train_RMSE <- 0\nensemble_rf_test_RMSE <- 0\nensemble_rf_test_RMSE_mean <- 0\nensemble_rf_validation_RMSE <- 0\nensemble_rf_sd <- 0\nensemble_rf_overfitting <- 0\nensemble_rf_duration <- 0\nensemble_rf_holdout_RMSE <- 0\nensemble_rf_holdout_RMSE_mean <- 0\n\nensemble_tree_train_RMSE <- 0\nensemble_tree_test_RMSE <- 0\nensemble_tree_validation_RMSE <- 0\nensemble_tree_sd <- 0\nensemble_tree_overfitting <- 0\nensemble_tree_duration <- 0\nensemble_tree_holdout_RMSE <- 0\nensemble_tree_holdout_RMSE_mean <- 0\n\n#Let's build the function that does all the resampling and puts everything together:\n\nfor (i in 1:numresamples) {\n\n# Randomly split the data between train and test\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Bayesglm\n\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = gaussian(link = \"identity\"))\nbayesglm_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesglm_train_fit, newdata = train))\nbayesglm_train_RMSE_mean <- mean(bayesglm_train_RMSE)\nbayesglm_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesglm_train_fit, newdata = test))\nbayesglm_test_RMSE_mean <- mean(bayesglm_test_RMSE)\nbayesglm_holdout_RMSE[i] <- mean(bayesglm_test_RMSE_mean)\nbayesglm_holdout_RMSE_mean <- mean(bayesglm_holdout_RMSE)\nbayesglm_test_predict_value <- as.numeric(predict(object = bayesglm_train_fit, newdata = test))\ny_hat_bayesglm <- c(bayesglm_test_predict_value)\n\n# Bayesrnn\n\nbayesrnn_train_fit <- brnn::brnn(x = as.matrix(train), y = train$y)\nbayesrnn_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesrnn_train_fit, newdata = train))\nbayesrnn_train_RMSE_mean <- mean(bayesrnn_train_RMSE)\nbayesrnn_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesrnn_train_fit, newdata = test))\nbayesrnn_test_RMSE_mean <- mean(bayesrnn_test_RMSE)\nbayesrnn_holdout_RMSE[i] <- mean(c(bayesrnn_test_RMSE_mean))\nbayesrnn_holdout_RMSE_mean <- mean(bayesrnn_holdout_RMSE)\nbayesrnn_train_predict_value <- as.numeric(predict(object = bayesrnn_train_fit, newdata = train))\nbayesrnn_test_predict_value <- as.numeric(predict(object = bayesrnn_train_fit, newdata = test))\nbayesrnn_predict_value_mean <- mean(c(bayesrnn_test_predict_value))\ny_hat_bayesrnn <- c(bayesrnn_test_predict_value)\n\n# Gradient boosted\n\ngb_train_fit <- gbm::gbm(train$y ~ ., data = train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\ngb_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = gb_train_fit, newdata = train))\ngb_train_RMSE_mean <- mean(gb_train_RMSE)\ngb_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = gb_train_fit, newdata = test))\ngb_test_RMSE_mean <- mean(gb_test_RMSE)\ngb_holdout_RMSE[i] <- mean(c(gb_test_RMSE_mean))\ngb_holdout_RMSE_mean <- mean(gb_holdout_RMSE)\ngb_train_predict_value <- as.numeric(predict(object = gb_train_fit, newdata = train))\ngb_test_predict_value <- as.numeric(predict(object = gb_train_fit, newdata = test)) \ngb_predict_value_mean <- mean(c(gb_test_predict_value))\ny_hat_gb <- c(gb_test_predict_value)\n\n# Tuned linear models\n\nlinear_train_fit <- e1071::tune.rpart(formula = y ~ ., data = train)\nlinear_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = linear_train_fit$best.model, newdata = train))\nlinear_train_RMSE_mean <- mean(linear_train_RMSE)\nlinear_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = linear_train_fit$best.model, newdata = test))\nlinear_test_RMSE_mean <- mean(linear_test_RMSE)\nlinear_holdout_RMSE[i] <- mean(c(linear_test_RMSE_mean))\nlinear_holdout_RMSE_mean <- mean(linear_holdout_RMSE)\nlinear_train_predict_value <- as.numeric(predict(object = linear_train_fit$best.model, newdata = train))\nlinear_test_predict_value <- as.numeric(predict(object = linear_train_fit$best.model, newdata = test))\ny_hat_linear <- c(linear_test_predict_value)\n\n# RandomForest\n\nrf_train_fit <- e1071::tune.randomForest(x = train, y = train$y, data = train)\nrf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = rf_train_fit$best.model, newdata = train))\nrf_train_RMSE_mean <- mean(rf_train_RMSE)\nrf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = rf_train_fit$best.model, newdata = test))\nrf_test_RMSE_mean <- mean(rf_test_RMSE)\nrf_holdout_RMSE[i] <- mean(c(rf_test_RMSE_mean))\nrf_holdout_RMSE_mean <- mean(rf_holdout_RMSE)\nrf_train_predict_value <- predict(object = rf_train_fit$best.model, newdata = train)\nrf_test_predict_value <- predict(object = rf_train_fit$best.model, newdata = test)\ny_hat_rf <- c(rf_test_predict_value)\n\n# Trees\n\ntree_train_fit <- tree::tree(train$y ~ ., data = train)\ntree_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = tree_train_fit, newdata = train))\ntree_train_RMSE_mean <- mean(tree_train_RMSE)\ntree_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = tree_train_fit, newdata = test))\ntree_test_RMSE_mean <- mean(tree_test_RMSE)\ntree_holdout_RMSE[i] <- mean(c(tree_test_RMSE_mean))\ntree_holdout_RMSE_mean <- mean(tree_holdout_RMSE)\ntree_train_predict_value <- as.numeric(predict(object = tree::tree(y ~ ., data = train), newdata = train))\ntree_test_predict_value <- as.numeric(predict(object = tree::tree(y ~ ., data = train), newdata = test))\ny_hat_tree <- c(tree_test_predict_value)\n\n# Make the weighted ensemble:\n\nensemble <- data.frame(\n  \"BayesGLM\" = y_hat_bayesglm * 1 / bayesglm_holdout_RMSE_mean,\n  \"BayesRNN\" = y_hat_bayesrnn * 1 / bayesrnn_holdout_RMSE_mean,\n  \"GBM\" = y_hat_gb * 1 / gb_holdout_RMSE_mean,\n  \"Linear\" = y_hat_linear * 1 / linear_holdout_RMSE_mean,\n  \"RandomForest\" = y_hat_rf * 1 / rf_holdout_RMSE_mean,\n  \"Tree\" = y_hat_tree * 1 / tree_holdout_RMSE_mean\n  )\n\nensemble$Row_mean <- rowMeans(ensemble)\n  ensemble$y_ensemble <- c(test$y)\n  y_ensemble <- c(test$y)\n\n# Split the ensemble into train and test, according to user choices:\n\nensemble_idx <- sample(seq(1, 2), size = nrow(ensemble), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble[ensemble_idx == 1, ]\nensemble_test <- ensemble[ensemble_idx == 2, ]\n\n# Ensemble BayesGLM\n\nensemble_bayesglm_train_fit <- arm::bayesglm(y_ensemble ~ ., data = ensemble_train, family = gaussian(link = \"identity\"))\nensemble_bayesglm_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_bayesglm_train_fit, newdata = ensemble_train))\nensemble_bayesglm_train_RMSE_mean <- mean(ensemble_bayesglm_train_RMSE)\nensemble_bayesglm_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_bayesglm_train_fit, newdata = ensemble_test))\nensemble_bayesglm_test_RMSE_mean <- mean(ensemble_bayesglm_test_RMSE)\nensemble_bayesglm_holdout_RMSE[i] <- mean(c(ensemble_bayesglm_test_RMSE_mean))\nensemble_bayesglm_holdout_RMSE_mean <- mean(ensemble_bayesglm_holdout_RMSE)\n\n# Ensemble BayesRNN\n\nensemble_bayesrnn_train_fit <- brnn::brnn(x = as.matrix(ensemble_train), y = ensemble_train$y_ensemble)\nensemble_bayesrnn_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_bayesrnn_train_fit, newdata = ensemble_train))\nensemble_bayesrnn_train_RMSE_mean <- mean(ensemble_bayesrnn_train_RMSE)\nensemble_bayesrnn_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_bayesrnn_train_fit, newdata = ensemble_test))\nensemble_bayesrnn_test_RMSE_mean <- mean(ensemble_bayesrnn_test_RMSE)\nensemble_bayesrnn_holdout_RMSE[i] <- mean(c(ensemble_bayesrnn_test_RMSE_mean))\nensemble_bayesrnn_holdout_RMSE_mean <- mean(ensemble_bayesrnn_holdout_RMSE)\n\n# Ensemble Graident Boosted\n\nensemble_gb_train_fit <- gbm::gbm(ensemble_train$y_ensemble ~ ., data = ensemble_train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\nensemble_gb_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_gb_train_fit, newdata = ensemble_train))\nensemble_gb_train_RMSE_mean <- mean(ensemble_gb_train_RMSE)\nensemble_gb_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_gb_train_fit, newdata = ensemble_test))\nensemble_gb_test_RMSE_mean <- mean(ensemble_gb_test_RMSE)\nensemble_gb_holdout_RMSE[i] <- mean(c(ensemble_gb_test_RMSE_mean))\nensemble_gb_holdout_RMSE_mean <- mean(ensemble_gb_holdout_RMSE)\n\n# Ensemble using Tuned Random Forest\n\nensemble_rf_train_fit <- e1071::tune.randomForest(x = ensemble_train, y = ensemble_train$y_ensemble, data = ensemble_train)\nensemble_rf_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_rf_train_fit$best.model, newdata = ensemble_train))\nensemble_rf_train_RMSE_mean <- mean(ensemble_rf_train_RMSE)\nensemble_rf_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_rf_train_fit$best.model, newdata = ensemble_test))\nensemble_rf_test_RMSE_mean <- mean(ensemble_rf_test_RMSE)\nensemble_rf_holdout_RMSE[i] <- mean(c(ensemble_rf_test_RMSE_mean))\nensemble_rf_holdout_RMSE_mean <- mean(ensemble_rf_holdout_RMSE)\n\n# Trees\n\nensemble_tree_train_fit <- tree::tree(ensemble_train$y_ensemble ~ ., data = ensemble_train)\nensemble_tree_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_tree_train_fit, newdata = ensemble_train))\nensemble_tree_train_RMSE_mean <- mean(ensemble_tree_train_RMSE)\nensemble_tree_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_tree_train_fit, newdata = ensemble_test))\nensemble_tree_test_RMSE_mean <- mean(ensemble_tree_test_RMSE)\nensemble_tree_holdout_RMSE[i] <- mean(c(ensemble_tree_test_RMSE_mean))\nensemble_tree_holdout_RMSE_mean <- mean(ensemble_tree_holdout_RMSE)\n\nsummary_results <- data.frame(\n  'Model' = c('BayesGLM', 'BayesRNN', 'Gradient_Boosted', 'Linear', 'Random_Forest', 'Trees', 'Ensemble_BayesGLM', 'Ensemble_BayesRNN', 'Ensemble_Gradient_Boosted', 'Ensemble_Random_Forest', 'Ensemble_Trees'), \n  'Error' = c(bayesglm_holdout_RMSE_mean, bayesrnn_holdout_RMSE_mean, gb_holdout_RMSE_mean, linear_holdout_RMSE_mean, rf_holdout_RMSE_mean, tree_holdout_RMSE_mean, ensemble_bayesglm_holdout_RMSE_mean, ensemble_bayesrnn_holdout_RMSE_mean, ensemble_gb_holdout_RMSE_mean, ensemble_rf_holdout_RMSE_mean, ensemble_tree_holdout_RMSE_mean) )\n\nsummary_results <- summary_results %>% arrange(Error)\n\n} # closing brace for numresamples\nreturn(summary_results)\n\n} # closing brace for numerical function\n\nnumerical(data = MASS::Boston, colnum = 14, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015979 \n#> gamma= 31.0592    alpha= 5.2255   beta= 16345.73\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7043456 \n#> gamma= 13.0833    alpha= 2.0427   beta= 5622.241\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015132 \n#> gamma= 30.7103    alpha= 4.8028   beta= 21458.26\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7046363 \n#> gamma= 14.3717    alpha= 2.2275   beta= 7483.324\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015926 \n#> gamma= 31.3551    alpha= 5.3838   beta= 14651.49\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.704124 \n#> gamma= 12.9512    alpha= 1.9297   beta= 8531.317\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015669 \n#> gamma= 28.7269    alpha= 4.3173   beta= 14167.72\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7038309 \n#> gamma= 14.1897    alpha= 2.1638   beta= 11022.44\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016085 \n#> gamma= 30.9911    alpha= 4.6469   beta= 13982.35\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7041953 \n#> gamma= 13.1431    alpha= 1.9324   beta= 5765.634\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#>                        Model     Error\n#> 1          Ensemble_BayesGLM 0.1321454\n#> 2                   BayesRNN 0.1408040\n#> 3          Ensemble_BayesRNN 0.2389492\n#> 4     Ensemble_Random_Forest 0.9385837\n#> 5              Random_Forest 1.6994395\n#> 6             Ensemble_Trees 1.8551031\n#> 7  Ensemble_Gradient_Boosted 2.8566052\n#> 8           Gradient_Boosted 3.1890256\n#> 9                     Linear 4.7726583\n#> 10                     Trees 4.8282863\n#> 11                  BayesGLM 4.8738526\n\nwarnings()\n\nnumerical(data = ISLR::Auto[, 1:ncol(ISLR::Auto)-1], colnum = 1, numresamples = 25, train_amount = 0.50, test_amount = 0.50)\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024926 \n#> gamma= 18.4162    alpha= 2.7514   beta= 13109.59\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.70502 \n#> gamma= 13.9302    alpha= 2.5764   beta= 4233.778\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024799 \n#> gamma= 18.7396    alpha= 2.5691   beta= 11432.25\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7049182 \n#> gamma= 13.1686    alpha= 2.4255   beta= 4584.367\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7026276 \n#> gamma= 17.5839    alpha= 3.1297   beta= 14069.74\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7043456 \n#> gamma= 12.4475    alpha= 2.4651   beta= 5270.222\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025856 \n#> gamma= 18.8981    alpha= 3.1775   beta= 9580.519\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7044656 \n#> gamma= 12.8729    alpha= 2.072    beta= 11330.64\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7021313 \n#> gamma= 18.7676    alpha= 3.5901   beta= 12531.76\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7061688 \n#> gamma= 11.3326    alpha= 2.4941   beta= 3858.152\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024673 \n#> gamma= 18.274     alpha= 1.9759   beta= 17960.72\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.70502 \n#> gamma= 12.6674    alpha= 2.0891   beta= 6155.332\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024302 \n#> gamma= 18.5515    alpha= 3.4186   beta= 14243.8\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7055354 \n#> gamma= 13.8508    alpha= 2.5064   beta= 4256.149\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7026858 \n#> gamma= 19.3318    alpha= 3.6784   beta= 8425.08\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.704124 \n#> gamma= 13.3745    alpha= 2.593    beta= 5848.52\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024548 \n#> gamma= 19.4772    alpha= 3.7208   beta= 9429.915\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7045924 \n#> gamma= 13.0337    alpha= 2.5198   beta= 4784.62\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025719 \n#> gamma= 19.0825    alpha= 3.3722   beta= 9364.237\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7044656 \n#> gamma= 14.1169    alpha= 2.3046   beta= 5773.569\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025856 \n#> gamma= 19.3196    alpha= 3.4654   beta= 9802.053\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.704681 \n#> gamma= 13.2377    alpha= 2.2026   beta= 5345.472\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025185 \n#> gamma= 18.6502    alpha= 3.5727   beta= 10209.71\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7049686 \n#> gamma= 12.381     alpha= 2.1155   beta= 5665.373\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025449 \n#> gamma= 18.6783    alpha= 2.9818   beta= 12973.07\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7044656 \n#> gamma= 13.8315    alpha= 2.0513   beta= 7035.462\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025584 \n#> gamma= 18.9933    alpha= 3.2416   beta= 9231.745\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7047266 \n#> gamma= 11.8112    alpha= 1.9485   beta= 6097.915\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024548 \n#> gamma= 19.1122    alpha= 3.1686   beta= 9241.642\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7048205 \n#> gamma= 12.8979    alpha= 2.1488   beta= 6987.878\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024673 \n#> gamma= 19.4256    alpha= 3.698    beta= 9353.561\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7055354 \n#> gamma= 10.7933    alpha= 1.7317   beta= 5212.225\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024799 \n#> gamma= 17.015     alpha= 2.9886   beta= 10479.61\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7053523 \n#> gamma= 13.0587    alpha= 2.2748   beta= 5016.278\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024673 \n#> gamma= 18.9127    alpha= 2.9877   beta= 16940.02\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7049182 \n#> gamma= 13.3138    alpha= 2.7722   beta= 5059.017\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024926 \n#> gamma= 19.3027    alpha= 2.8318   beta= 9719.859\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7057316 \n#> gamma= 11.9986    alpha= 2.0843   beta= 3711.19\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7026419 \n#> gamma= 18.7109    alpha= 3.1853   beta= 9516.249\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7052367 \n#> gamma= 13.4531    alpha= 2.4635   beta= 5533.873\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025317 \n#> gamma= 18.9174    alpha= 3.0516   beta= 9411.772\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7049182 \n#> gamma= 13.1981    alpha= 2.1471   beta= 7517.678\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025856 \n#> gamma= 18.6904    alpha= 2.8206   beta= 13973.04\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7043456 \n#> gamma= 12.1392    alpha= 2.1957   beta= 8680.461\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025995 \n#> gamma= 19.0408    alpha= 3.3839   beta= 11170.28\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7049686 \n#> gamma= 13.169     alpha= 2.0876   beta= 7091.306\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024673 \n#> gamma= 18.9979    alpha= 2.8482   beta= 10179.61\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7045071 \n#> gamma= 12.4243    alpha= 2.6426   beta= 4855.048\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023942 \n#> gamma= 19.4829    alpha= 3.7004   beta= 9447.426\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7052367 \n#> gamma= 12.5163    alpha= 2.1979   beta= 4306.634\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#>                        Model     Error\n#> 1          Ensemble_BayesGLM 0.1293269\n#> 2                   BayesRNN 0.1310772\n#> 3          Ensemble_BayesRNN 0.2064510\n#> 4     Ensemble_Random_Forest 0.8823964\n#> 5  Ensemble_Gradient_Boosted 1.4839729\n#> 6             Ensemble_Trees 1.6717371\n#> 7              Random_Forest 1.6945471\n#> 8           Gradient_Boosted 2.9664228\n#> 9                   BayesGLM 3.4312923\n#> 10                     Trees 3.7725029\n#> 11                    Linear 3.7907192"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"one-of-your-own-add-a-model-to-the-individual-models-and-a-model-to-the-ensemble-of-models","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.4 One of your own: Add a model to the individual models, and a model to the ensemble of models","text":"One : Change data, run , comment results","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"post-your-results-on-social-media-in-a-way-that-a-non-technical-person-can-understand-them.-for-example","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.5 Post your results on social media in a way that a non-technical person can understand them. For example:","text":"“Just ran six individual six ensemble models, easy , \nerrors warnings. plan ensembles data sets soon.\n#AIEnsembles","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"exercises-to-help-you-improve-your-skills","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.6 Exercises to help you improve your skills:","text":"Build individual numerical model using following model\nmethods (’s perfectly OK check prior sections book, \nexample delayed repetition):Gradient Boosted (gmb library)Rpart (rpart library)Support Vector Machines (tuned e1071 library)One model method choosingBuild ensemble using four methods, test using Boston\nHousing data set. Compare results ensemble one made\ntext chapter.Apply function made different numerical data set. can\ndone one line code, ensemble set .","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"post-the-results-of-your-new-ensemble-on-social-media-in-a-way-that-helps-others-understand-the-results-or-methods.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.7 Post the results of your new ensemble on social media in a way that helps others understand the results or methods.","text":"","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"helping-leaders-make-the-best-possible-decisions-what-are-the-options-benefits-and-costs-how-strong-are-the-resultsrecommendations","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.8 Helping leaders make the best possible decisions: What are the options, benefits and costs? How strong are the results/recommendations?","text":"","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"helping-leaders-make-the-best-possible-decisions-margins-of-errorreasons-for-error","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.9 Helping leaders make the best possible decisions: Margins of error/reasons for error","text":"","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"helping-leaders-make-the-best-possible-decisions-highest-accuracy-strongest-predictors","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.10 Helping leaders make the best possible decisions: Highest accuracy, strongest predictor(s)","text":"","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"helping-your-customermanagerboard-to-start-thinking-analytically","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.11 Helping your customer/manager/board to start thinking analytically","text":"","code":""},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"classification-data-how-to-make-14-individual-classification-models","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5 Classification data: How to make 14 individual classification models","text":"Ensembles numerical data give results often superior individual model. ’ve seen results ensembles numerical data beat nearly individual models, measures lowest error rate.Now going classification data. build 15 individual models classification data chapter.basic series steps numerical data. follow steps, complete process models classification data.Load librarySet initial values 0Create functionBreak data train test setsSet random resamplingFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setAll models structured way close identical possible. high level consistency makes easier spot errors.","code":""},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"what-is-classification-data","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.1 What is classification data?","text":"Classification models set models identify class specific observation. example, Carseats data set, Shelve Location example data type:look ShelveLoc column, set numbers, one three locations shelf: Bad, Medium Good. Classification models statistical models predict class data.Classification models similar numerical models. follow basic steps numerical models, use classification models instead.One big difference accuracy measured classification data. numerical models used root mean squared error. measure classification simply accuracy result. can measured directly matrix values. example:C50 test results:accuracy determined calculating number correct responses divided total number responses. correct responses along main diagonal.example, model correct predicted 83 bad responses, however also predicted response bad actually Good, also predicted Bad actually Medium. correct responses main diagonal, responses errors. use calculation accuracy model results.case, (83 + 75 + 272) / (83 + 1 + 105 + 4 + 75 + 56 + 106 + 90 + 272) = 0.5429293. create function calculate result automatically classification model. Clearly higher accuracy, better results. best possible accuracy result 1.00.","code":"\nlibrary(ISLR)\nhead(Carseats)\n#>   Sales CompPrice Income Advertising Population Price\n#> 1  9.50       138     73          11        276   120\n#> 2 11.22       111     48          16        260    83\n#> 3 10.06       113     35          10        269    80\n#> 4  7.40       117    100           4        466    97\n#> 5  4.15       141     64           3        340   128\n#> 6 10.81       124    113          13        501    72\n#>   ShelveLoc Age Education Urban  US\n#> 1       Bad  42        17   Yes Yes\n#> 2      Good  65        10   Yes Yes\n#> 3    Medium  59        12   Yes Yes\n#> 4    Medium  55        14   Yes Yes\n#> 5       Bad  38        13   Yes  No\n#> 6       Bad  78        16    No Yes"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"build-our-first-classification-model-from-the-structure","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.2 Build our first classification model from the structure:","text":"Now structure, let’s build model:Now see one individual classification model made, let’s make 11 (total 12).","code":"\n\n# Load libraries\n\n# Set initial values to 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\n\n# Set up resamples\n\n# Randomize the rows\n\n# Split data into train and test sets\n\n# Fit the model on the training data\n\n# Check accuracy and make predictions from the model, applied to the test data\n\n# Calculate overall model accuracy\n\n# Calculate table\n# Print table results\n\n# Return accuracy results\n\n# Closing braces for numresamples loop\n# Closing brace for classification1 function\n\n# Test the function\n\n# Check for errors\n\n# Load libraries\nlibrary(C50)\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Set initial values to 0:\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_validation_accuracy <- 0\nC50_overfitting <- 0\nC50_holdout <- 0\nC50_table_total <- 0\n\n# Build the function:\nC50_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Changes target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\nC50_train_pred <- predict(C50_train_fit, train)\nC50_train_table <- table(C50_train_pred, y_train)\nC50_train_accuracy[i] <- sum(diag(C50_train_table)) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\nC50_train_mean <- mean(diag(C50_train_table)) / mean(C50_train_table)\n\n# Check accuracy and make predictions from the model, applied to the test data\nC50_test_pred <- predict(C50_train_fit, test)\nC50_test_table <- table(C50_test_pred, y_test)\nC50_test_accuracy[i] <- sum(diag(C50_test_table)) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\nC50_test_mean <- mean(diag(C50_test_table)) / mean(C50_test_table)\n\n# Calculate accuracy\nC50_holdout[i] <- mean(c(C50_test_accuracy_mean))\nC50_holdout_mean <- mean(C50_holdout)\n\n# Calculate table\nC50_table <- C50_test_table\nC50_table_total <- C50_table_total + C50_table\n\nprint(C50_table_total)\n\n# Return accuracy result\nreturn(c(C50_holdout_mean))\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\nC50_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>              y_test\n#> C50_test_pred Bad Good Medium\n#>        Bad      8    3     11\n#>        Good     0   10      6\n#>        Medium  12    8     36\n#> [1] 0.5744681"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"adabag-for-classification-data","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.3 Adabag for classification data","text":"","code":"\n\n# Load libraries\nlibrary(ipred)\n\n# Set initial values to 0\nadabag_train_accuracy <- 0\nadabag_test_accuracy <- 0\nadabag_validation_accuracy <- 0\nadabag_holdout <- 0\nadabag_table_total <- 0\n\n# Build the function\nadabag_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nadabag_train_fit <- ipred::bagging(formula = y ~ ., data = train01)\nadabag_train_pred <- predict(object = adabag_train_fit, newdata = train)\nadabag_train_table <- table(adabag_train_pred, y_train)\nadabag_train_accuracy[i] <- sum(diag(adabag_train_table)) / sum(adabag_train_table)\nadabag_train_accuracy_mean <- mean(adabag_train_accuracy)\n  \n# Check accuracy and make predictions from the model, applied to the test data\nadabag_test_pred <- predict(object = adabag_train_fit, newdata = test01)\nadabag_test_table <- table(adabag_test_pred, y_test)\nadabag_test_accuracy[i] <- sum(diag(adabag_test_table)) / sum(adabag_test_table)\nadabag_test_accuracy_mean <- mean(adabag_test_accuracy)\nadabag_test_mean <- mean(diag(adabag_test_table)) / mean(adabag_test_table)\n\n# Calculate accuracy\nadabag_holdout[i] <- mean(c(adabag_test_accuracy_mean))\nadabag_holdout_mean <- mean(adabag_holdout)\n\n# Calculate table\nadabag_table <- adabag_test_table\nadabag_table_total <- adabag_table_total + adabag_table\n\nprint(adabag_table_total)\n\n# Return accuracy results\nreturn(adabag_holdout_mean)\n\n} # Closing braces for numresamples loop\n\n} # Closing brace for classification1 function\n\n# Test the function\nadabag_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> adabag_test_pred Bad Good Medium\n#>           Bad     10    1      9\n#>           Good     1   11     10\n#>           Medium  15    7     51\n#> [1] 0.626087\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"bagged-random-forest-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.4 Bagged Random Forest","text":"","code":"\n\n# Load libraries\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\n\n# Set initial values to 0\nbag_rf_train_accuracy <- 0\nbag_rf_test_accuracy <- 0\nbag_rf_validation_accuracy <- 0\nbag_rf_overfitting <- 0\nbag_rf_holdout <- 0\nbag_rf_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nbag_rf_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nbag_rf_train_fit <- randomForest::randomForest(y ~ ., data = train01, mtry = ncol(train))\nbag_rf_train_pred <- predict(bag_rf_train_fit, train, type = \"class\")\nbag_rf_train_table <- table(bag_rf_train_pred, y_train)\nbag_rf_train_accuracy[i] <- sum(diag(bag_rf_train_table)) / sum(bag_rf_train_table)\nbag_rf_train_accuracy_mean <- mean(bag_rf_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nbag_rf_test_pred <- predict(bag_rf_train_fit, test, type = \"class\")\nbag_rf_test_table <- table(bag_rf_test_pred, y_test)\nbag_rf_test_accuracy[i] <- sum(diag(bag_rf_test_table)) / sum(bag_rf_test_table)\nbag_rf_test_accuracy_mean <- mean(bag_rf_test_accuracy)\n\n# Calculate model accuracy\nbag_rf_holdout[i] <- mean(c(bag_rf_test_accuracy_mean))\nbag_rf_holdout_mean <- mean(bag_rf_holdout)\n\n# Calculate table\nbag_rf_table <- bag_rf_test_table\nbag_rf_table_total <- bag_rf_table_total + bag_rf_table\n\n# Print table results\nprint(bag_rf_table_total)\n\n# Return accuracy results\nreturn(bag_rf_holdout_mean)\n\n}# Closing braces for numresamples loop\n}# Closing brace for classification1 function\n\n# Test the function\nbag_rf_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> bag_rf_test_pred Bad Good Medium\n#>           Bad      9    0      9\n#>           Good     0   16      6\n#>           Medium   6    8     58\n#> [1] 0.7410714\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"linear-model","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.5 Linear model","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n#> \n#> Attaching package: 'MachineShop'\n#> The following object is masked from 'package:purrr':\n#> \n#>     lift\n#> The following object is masked from 'package:stats':\n#> \n#>     ppr\n\n# Set initial values to 0\nlinear_train_accuracy <- 0\nlinear_validation_accuracy <- 0\nlinear_test_accuracy <- 0\nlinear_test_accuracy_mean <- 0\nlinear_holdout <- 0\nlinear_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nlinear1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nlinear_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"LMModel\")\nlinear_train_pred <- predict(object = linear_train_fit, newdata = train01)\nlinear_train_table <- table(linear_train_pred, y_train)\nlinear_train_accuracy[i] <- sum(diag(linear_train_table)) / sum(linear_train_table)\nlinear_train_accuracy_mean <- mean(linear_train_accuracy)\nlinear_train_mean <- mean(diag(linear_train_table)) / mean(linear_train_table)\n\n# Check accuracy and make predictions from the model, applied to the test data\nlinear_test_pred <- predict(object = linear_train_fit, newdata = test01)\nlinear_test_table <- table(linear_test_pred, y_test)\nlinear_test_accuracy[i] <- sum(diag(linear_test_table)) / sum(linear_test_table)\nlinear_test_accuracy_mean <- mean(linear_test_accuracy)\n\n# Calculate overall model accuracy\nlinear_holdout[i] <- mean(c(linear_test_accuracy_mean))\nlinear_holdout_mean <- mean(linear_holdout)\n\n# Calculate table\nlinear_table <- linear_test_table\nlinear_table_total <- linear_table_total + linear_table\n\n# Print table results\nprint(linear_table_total)\n\n# Return accuracy results\nreturn(linear_holdout_mean)\n\n}# Closing braces for numresamples loop\n}# Closing brace for classification1 function\n\n# Test the function\nlinear1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> linear_test_pred Bad Good Medium\n#>           Bad      7    0      1\n#>           Good     0   14      0\n#>           Medium  19    8     44\n#> [1] 0.6989247\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"naive-bayes-model","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.6 Naive Bayes model","text":"","code":"\n\n# Load libraries\nlibrary(e1071)\n\n# Set initial values to 0\nn_bayes_train_accuracy <- 0\nn_bayes_test_accuracy <- 0\nn_bayes_accuracy <- 0\nn_bayes_test_accuracy_mean <- 0\nn_bayes_holdout <- 0\nn_bayes_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nn_bayes_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nn_bayes_train_fit <- e1071::naiveBayes(y_train ~ ., data = train)\nn_bayes_train_pred <- predict(n_bayes_train_fit, train)\nn_bayes_train_table <- table(n_bayes_train_pred, y_train)\nn_bayes_train_accuracy[i] <- sum(diag(n_bayes_train_table)) / sum(n_bayes_train_table)\nn_bayes_train_accuracy_mean <- mean(n_bayes_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nn_bayes_test_pred <- predict(n_bayes_train_fit, test)\nn_bayes_test_table <- table(n_bayes_test_pred, y_test)\nn_bayes_test_accuracy[i] <- sum(diag(n_bayes_test_table)) / sum(n_bayes_test_table)\nn_bayes_test_accuracy_mean <- mean(n_bayes_test_accuracy)\n\n# Calculate overall model accuracy\nn_bayes_holdout[i] <- mean(c(n_bayes_test_accuracy_mean))\nn_bayes_holdout_mean <- mean(n_bayes_holdout)\n\n# Calculate table\nn_bayes_table <- n_bayes_test_table\nn_bayes_table_total <- n_bayes_table_total + n_bayes_table\n\n# Print table results\nprint(n_bayes_table_total)\n\n# Return accuracy results\nreturn(n_bayes_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nn_bayes_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                  y_test\n#> n_bayes_test_pred Bad Good Medium\n#>            Bad      6    2     10\n#>            Good     1   14      3\n#>            Medium  12   12     45\n#> [1] 0.6190476\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"partial-least-squares-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.7 Partial Least Squares","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\npls_train_accuracy <- 0\npls_test_accuracy <- 0\npls_accuracy <- 0\npls_test_accuracy_mean <- 0\npls_holdout <- 0\npls_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\npls_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\npls_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"PLSModel\")\npls_train_predict <- predict(object = pls_train_fit, newdata = train01)\npls_train_table <- table(pls_train_predict, y_train)\npls_train_accuracy[i] <- sum(diag(pls_train_table)) / sum(pls_train_table)\npls_train_accuracy_mean <- mean(pls_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\npls_test_predict <- predict(object = pls_train_fit, newdata = test01)\npls_test_table <- table(pls_test_predict, y_test)\npls_test_accuracy[i] <- sum(diag(pls_test_table)) / sum(pls_test_table)\npls_test_accuracy_mean <- mean(pls_test_accuracy)\npls_test_pred <- pls_test_predict\n\n# Calculate overall model accuracy\npls_holdout[i] <- mean(c(pls_test_accuracy_mean))\npls_holdout_mean <- mean(pls_holdout)\n\n# Calculate table\npls_table <- pls_test_table\npls_table_total <- pls_table_total + pls_table\n\n# Print table results\nprint(pls_table_total)\n\n# Return accuracy results\nreturn(pls_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\npls_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> pls_test_predict Bad Good Medium\n#>           Bad      0    0      0\n#>           Good     0    0      0\n#>           Medium  29   10     56\n#> [1] 0.5894737\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"penalized-discriminant-analysis-model","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.8 Penalized Discriminant Analysis Model","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\npda_train_accuracy <- 0\npda_test_accuracy <- 0\npda_accuracy <- 0\npda_test_accuracy_mean <- 0\npda_holdout <- 0\npda_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\npda_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\npda_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"PDAModel\")\npda_train_predict <- predict(object = pda_train_fit, newdata = train01)\npda_train_table <- table(pda_train_predict, y_train)\npda_train_accuracy[i] <- sum(diag(pda_train_table)) / sum(pda_train_table)\npda_train_accuracy_mean <- mean(pda_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\npda_test_predict <- predict(object = pda_train_fit, newdata = test01)\npda_test_table <- table(pda_test_predict, y_test)\npda_test_accuracy[i] <- sum(diag(pda_test_table)) / sum(pda_test_table)\npda_test_accuracy_mean <- mean(pda_test_accuracy)\npda_test_pred <- pda_test_predict\n\n# Calculate overall model accuracy\npda_holdout[i] <- mean(c(pda_test_accuracy_mean))\npda_holdout_mean <- mean(pda_holdout)\n\n# Calculate table\npda_table <- pda_test_table\npda_table_total <- pda_table_total + pda_table\n\n# Print table results\nprint(pda_table_total)\n\n# Return accuracy results\nreturn(pda_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\npda_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> pda_test_predict Bad Good Medium\n#>           Bad     21    0      3\n#>           Good     0   19      3\n#>           Medium   8    5     57\n#> [1] 0.8362069\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"random-forest-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.9 Random Forest","text":"","code":"\n\n# Load libraries\nlibrary(randomForest)\n\n# Set initial values to 0\nrf_train_accuracy <- 0\nrf_test_accuracy <- 0\nrf_accuracy <- 0\nrf_test_accuracy_mean <- 0\nrf_holdout <- 0\nrf_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nrf_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nrf_train_fit <- randomForest::randomForest(x = train, y = y_train, data = df)\nrf_train_pred <- predict(rf_train_fit, train, type = \"class\")\nrf_train_table <- table(rf_train_pred, y_train)\nrf_train_accuracy[i] <- sum(diag(rf_train_table)) / sum(rf_train_table)\nrf_train_accuracy_mean <- mean(rf_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nrf_test_pred <- predict(rf_train_fit, test, type = \"class\")\nrf_test_table <- table(rf_test_pred, y_test)\nrf_test_accuracy[i] <- sum(diag(rf_test_table)) / sum(rf_test_table)\nrf_test_accuracy_mean <- mean(rf_test_accuracy)\n\n# Calculate overall model accuracy\nrf_holdout[i] <- mean(c(rf_test_accuracy_mean))\nrf_holdout_mean <- mean(rf_holdout)\n\n# Calculate table\nrf_table <- rf_test_table\nrf_table_total <- rf_table_total + rf_table\n\n# Print table results\nprint(rf_table_total)\n\n# Return accuracy results\nreturn(rf_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nrf_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>             y_test\n#> rf_test_pred Bad Good Medium\n#>       Bad      7    0      0\n#>       Good     1   10      2\n#>       Medium  20   11     49\n#> [1] 0.66\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"ranger","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.10 Ranger","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\nranger_train_accuracy <- 0\nranger_test_accuracy <- 0\nranger_accuracy <- 0\nranger_test_accuracy_mean <- 0\nranger_holdout <- 0\nranger_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nranger_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n  \n# Fit the model on the training data\nranger_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"RangerModel\")\nranger_train_predict <- predict(object = ranger_train_fit, newdata = train01)\nranger_train_table <- table(ranger_train_predict, y_train)\nranger_train_accuracy[i] <- sum(diag(ranger_train_table)) / sum(ranger_train_table)\nranger_train_accuracy_mean <- mean(ranger_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nranger_test_predict <- predict(object = ranger_train_fit, newdata = test01)\nranger_test_table <- table(ranger_test_predict, y_test)\nranger_test_accuracy[i] <- sum(diag(ranger_test_table)) / sum(ranger_test_table)\nranger_test_accuracy_mean <- mean(ranger_test_accuracy)\nranger_test_pred <- ranger_test_predict\n\n# Calculate overall model accuracy\nranger_holdout[i] <- mean(c(ranger_test_accuracy_mean))\nranger_holdout_mean <- mean(ranger_holdout)\n\n# Calculate table\nranger_table <- ranger_test_table\nranger_table_total <- ranger_table_total + ranger_table\n\n# Print table results\nprint(ranger_table_total)\n\n# Return accuracy results\nreturn(ranger_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nranger_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                    y_test\n#> ranger_test_predict Bad Good Medium\n#>              Bad      5    0      4\n#>              Good     0   12      2\n#>              Medium  17   14     36\n#> [1] 0.5888889\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"regularized-discriminant-analysis","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.11 Regularized Discriminant Analysis","text":"","code":"\n\n# Load libraries\nlibrary(klaR)\n#> Loading required package: MASS\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\n\n# Set initial values to 0\nrda_train_accuracy <- 0\nrda_test_accuracy <- 0\nrda_accuracy <- 0\nrda_test_accuracy_mean <- 0\nrda_holdout <- 0\nrda_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nrda_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nrda_train_fit <- klaR::rda(y_train ~ ., data = train)\nrda_train_pred <- predict(object = rda_train_fit, newdata = train)\nrda_train_table <- table(rda_train_pred$class, y_train)\nrda_train_accuracy[i] <- sum(diag(rda_train_table)) / sum(rda_train_table)\nrda_train_accuracy_mean <- mean(rda_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nrda_test_pred <- predict(object = rda_train_fit, newdata = test)\nrda_test_table <- table(rda_test_pred$class, y_test)\nrda_test_accuracy[i] <- sum(diag(rda_test_table)) / sum(rda_test_table)\nrda_test_accuracy_mean <- mean(rda_test_accuracy)\n\n# Calculate overall model accuracy\nrda_holdout[i] <- mean(c(rda_test_accuracy_mean))\nrda_holdout_mean <- mean(rda_holdout)\n\n# Calculate table\nrda_table <- rda_test_table\nrda_table_total <- rda_table_total + rda_table\n\n# Print table results\nprint(rda_table_total)\n\n# Return accuracy results\nreturn(rda_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nrda_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>         y_test\n#>          Bad Good Medium\n#>   Bad      0    0      0\n#>   Good     0    0      0\n#>   Medium  22   18     57\n#> [1] 0.5876289\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"rpart-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.12 Rpart","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\nrpart_train_accuracy <- 0\nrpart_test_accuracy <- 0\nrpart_accuracy <- 0\nrpart_test_accuracy_mean <- 0\nrpart_holdout <- 0\nrpart_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nrpart_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nrpart_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"RPartModel\")\nrpart_train_predict <- predict(object = rpart_train_fit, newdata = train01)\nrpart_train_table <- table(rpart_train_predict, y_train)\nrpart_train_accuracy[i] <- sum(diag(rpart_train_table)) / sum(rpart_train_table)\nrpart_train_accuracy_mean <- mean(rpart_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nrpart_test_predict <- predict(object = rpart_train_fit, newdata = test01)\nrpart_test_table <- table(rpart_test_predict, y_test)\nrpart_test_accuracy[i] <- sum(diag(rpart_test_table)) / sum(rpart_test_table)\nrpart_test_accuracy_mean <- mean(rpart_test_accuracy)\nrpart_test_pred <- rpart_test_predict\n\n# Calculate overall model accuracy\nrpart_holdout[i] <- mean(c(rpart_test_accuracy_mean))\nrpart_holdout_mean <- mean(rpart_holdout)\n\n# Calculate table\nrpart_table <- rpart_test_table\nrpart_table_total <- rpart_table_total + rpart_table\n  \n# Print table results\nprint(rpart_table_total)\n  \n# Return accuracy results\nreturn(rpart_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nrpart_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                   y_test\n#> rpart_test_predict Bad Good Medium\n#>             Bad      5    0      3\n#>             Good     0   11      6\n#>             Medium  18   11     34\n#> [1] 0.5681818\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"support-vector-machines-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.13 Support Vector Machines","text":"","code":"\n\n# Load libraries\nlibrary(e1071)\n\n# Set initial values to 0\nsvm_train_accuracy <- 0\nsvm_test_accuracy <- 0\nsvm_accuracy <- 0\nsvm_test_accuracy_mean <- 0\nsvm_holdout <- 0\nsvm_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nsvm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nsvm_train_fit <- e1071::svm(y_train ~ ., data = train, kernel = \"radial\", gamma = 1, cost = 1)\nsvm_train_pred <- predict(svm_train_fit, train, type = \"class\")\nsvm_train_table <- table(svm_train_pred, y_train)\nsvm_train_accuracy[i] <- sum(diag(svm_train_table)) / sum(svm_train_table)\nsvm_train_accuracy_mean <- mean(svm_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nsvm_test_pred <- predict(svm_train_fit, test, type = \"class\")\nsvm_test_table <- table(svm_test_pred, y_test)\nsvm_test_accuracy[i] <- sum(diag(svm_test_table)) / sum(svm_test_table)\nsvm_test_accuracy_mean <- mean(svm_test_accuracy)\n\n# Calculate overall model accuracy\nsvm_holdout[i] <- mean(c(svm_test_accuracy_mean))\nsvm_holdout_mean <- mean(svm_holdout)\n\n# Calculate table\nsvm_table <- svm_test_table\nsvm_table_total <- svm_table_total + svm_table\n\n# Print table results\nprint(svm_table_total)\n\n# Return accuracy results\nreturn(svm_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nsvm_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>              y_test\n#> svm_test_pred Bad Good Medium\n#>        Bad      0    0      0\n#>        Good     0    0      0\n#>        Medium  27   16     55\n#> [1] 0.5612245\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"trees-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.14 Trees","text":"","code":"\n\n# Load libraries\nlibrary(tree)\n\n# Set initial values to 0\ntree_train_accuracy <- 0\ntree_test_accuracy <- 0\ntree_accuracy <- 0\ntree_test_accuracy_mean <- 0\ntree_holdout <- 0\ntree_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\ntree_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\ntree_train_fit <- tree::tree(y_train ~ ., data = train)\ntree_train_pred <- predict(tree_train_fit, train, type = \"class\")\ntree_train_table <- table(tree_train_pred, y_train)\ntree_train_accuracy[i] <- sum(diag(tree_train_table)) / sum(tree_train_table)\ntree_train_accuracy_mean <- mean(tree_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\ntree_test_pred <- predict(tree_train_fit, test, type = \"class\")\ntree_test_table <- table(tree_test_pred, y_test)\ntree_test_accuracy[i] <- sum(diag(tree_test_table)) / sum(tree_test_table)\ntree_test_accuracy_mean <- mean(tree_test_accuracy)\n\n# Calculate overall model accuracy\ntree_holdout[i] <- mean(c(tree_test_accuracy_mean))\ntree_holdout_mean <- mean(tree_holdout)\n\n# Calculate table\ntree_table <- tree_test_table\ntree_table_total <- tree_table_total + tree_table  \n\n# Print table results\nprint(tree_table_total)\n\n# Return accuracy results\nreturn(tree_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\ntree_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>               y_test\n#> tree_test_pred Bad Good Medium\n#>         Bad      3    0      4\n#>         Good     0    9      4\n#>         Medium  22   12     38\n#> [1] 0.5434783\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"xgboost-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.15 XGBoost","text":"","code":"\n\n# Load libraries\nlibrary(xgboost)\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\n\n# Set initial values to 0\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_accuracy <- 0\nxgb_test_accuracy_mean <- 0\nxgb_holdout <- 0\nxgb_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nxgb_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_train), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_train + 1]\nxgb_train_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_train_accuracy[i] <- sum(diag(xgb_train_table)) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_test + 1]\nxgb_test_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_test_accuracy[i] <- sum(diag(xgb_test_table)) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\n# Calculate overall model accuracy\nxgb_holdout[i] <- mean(c(xgb_test_accuracy_mean))\nxgb_holdout_mean <- mean(xgb_holdout)\n\n# Calculate table\nxgb_table <- xgb_test_table\nxgb_table_total <- xgb_table_total + xgb_table\n\n# Print table results\nprint(xgb_table_total)\n\n# Return accuracy results\nreturn(xgb_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nxgb_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>         \n#>          Bad Good Medium\n#>   Bad      3    0      4\n#>   Good     3   14      8\n#>   Medium  16   12     39\n#> [1] 0.5656566\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"post-your-results","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.16 Post your results","text":"","code":""},{"path":"building-ensembles-of-classification-models.html","id":"building-ensembles-of-classification-models","chapter":"6 Building ensembles of classification models","heading":"6 Building ensembles of classification models","text":"section building two ensembles classification models. use six classification models, six ensembles, total 12 results.","code":""},{"path":"building-ensembles-of-classification-models.html","id":"lets-start-at-the-end-and-work-backwards","chapter":"6 Building ensembles of classification models","heading":"6.0.1 Let’s start at the end and work backwards","text":"know want finish predictions ensemble classification models. Therefore need ensemble classification models. Therefore need classification models. Let’s choose five classification models individual models, use five ensemble. Note may use modeling method wish ensemble, since ’s data.final result look something like :Predictions holdout data classification model 1Predictions holdout data classification model 2Predictions holdout data classification model 3Predictions holdout data classification model 4Predictions holdout data classification model 5Use predictions make ensembleUse ensemble models make predictions ensemble holdout dataReport resultsLet’s come list five classification models use:Bagged Random ForestC50RangerSupport Vector MachinesXGBoostNote nothing special using five models. number models may used, set good start.solution also add mean duration model finished report.Since basic outline, know want end, ready begin.","code":"\n\n# Load libraries\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::combine()  masks randomForest::combine()\n#> ✖ dplyr::filter()   masks stats::filter()\n#> ✖ dplyr::lag()      masks stats::lag()\n#> ✖ ggplot2::margin() masks randomForest::margin()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(tree)\n\n# Set initial values to 0\nbag_rf_train_accuracy <- 0\nbag_rf_test_accuracy <- 0\nbag_rf_validation_accuracy <- 0\nbag_rf_overfitting <- 0\nbag_rf_holdout <- 0\nbag_rf_table_total <- 0\nbag_rf_duration <- 0\n\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_validation_accuracy <- 0\nC50_overfitting <- 0\nC50_holdout <- 0\nC50_table_total <- 0\nC50_duration <- 0\n\nranger_train_accuracy <- 0\nranger_test_accuracy <- 0\nranger_accuracy <- 0\nranger_test_accuracy_mean <- 0\nranger_holdout <- 0\nranger_table_total <- 0\nranger_duration <- 0\n\nsvm_train_accuracy <- 0\nsvm_test_accuracy <- 0\nsvm_accuracy <- 0\nsvm_test_accuracy_mean <- 0\nsvm_holdout <- 0\nsvm_table_total <- 0\nsvm_duration <- 0\n\nensemble_bag_rf_train_accuracy <- 0\nensemble_bag_rf_test_accuracy <- 0\nensemble_bag_rf_validation_accuracy <- 0\nensemble_bag_rf_overfitting <- 0\nensemble_bag_rf_holdout <- 0\nensemble_bag_rf_table_total <- 0\nensemble_bag_rf_duration <- 0\n\nensemble_C50_train_accuracy <- 0\nensemble_C50_test_accuracy <- 0\nensemble_C50_validation_accuracy <- 0\nensemble_C50_overfitting <- 0\nensemble_C50_holdout <- 0\nensemble_C50_table_total <- 0\nensemble_C50_duration <- 0\n\nensemble_ranger_train_accuracy <- 0\nensemble_ranger_test_accuracy <- 0\nensemble_ranger_validation_accuracy <- 0\nensemble_ranger_overfitting <- 0\nensemble_ranger_holdout <- 0\nensemble_ranger_table_total <- 0\nensemble_ranger_duration <- 0\n\nensemble_rf_train_accuracy <- 0\nensemble_rf_test_accuracy <- 0\nensemble_rf_validation_accuracy <- 0\nensemble_rf_overfitting <- 0\nensemble_rf_holdout <- 0\nensemble_rf_table_total <- 0\nensemble_rf_duration <- 0\n\nensemble_svm_train_accuracy <- 0\nensemble_svm_test_accuracy <- 0\nensemble_svm_validation_accuracy <- 0\nensemble_svm_overfitting <- 0\nensemble_svm_holdout <- 0\nensemble_svm_table_total <- 0\nensemble_svm_duration <- 0\n\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nclassification_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nbag_rf_start <- Sys.time()\nbag_rf_train_fit <- randomForest::randomForest(y ~ ., data = train01, mtry = ncol(train))\nbag_rf_train_pred <- predict(bag_rf_train_fit, train, type = \"class\")\nbag_rf_train_table <- table(bag_rf_train_pred, y_train)\nbag_rf_train_accuracy[i] <- sum(diag(bag_rf_train_table)) / sum(bag_rf_train_table)\nbag_rf_train_accuracy_mean <- mean(bag_rf_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nbag_rf_test_pred <- predict(bag_rf_train_fit, test, type = \"class\")\nbag_rf_test_table <- table(bag_rf_test_pred, y_test)\nbag_rf_test_accuracy[i] <- sum(diag(bag_rf_test_table)) / sum(bag_rf_test_table)\nbag_rf_test_accuracy_mean <- mean(bag_rf_test_accuracy)\n\n# Calculate model accuracy\nbag_rf_holdout[i] <- mean(c(bag_rf_test_accuracy_mean))\nbag_rf_holdout_mean <- mean(bag_rf_holdout)\n\n# Calculate table\nbag_rf_table <- bag_rf_test_table\nbag_rf_table_total <- bag_rf_table_total + bag_rf_table\n\nbag_rf_end <- Sys.time()\nbag_rf_duration[i] <- bag_rf_end - bag_rf_start\nbag_rf_duration_mean <- mean(bag_rf_duration)\n\n# C50 model\nC50_start <- Sys.time()\n# Fit the model on the training data\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\nC50_train_pred <- predict(C50_train_fit, train)\nC50_train_table <- table(C50_train_pred, y_train)\nC50_train_accuracy[i] <- sum(diag(C50_train_table)) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\nC50_train_mean <- mean(diag(C50_train_table)) / mean(C50_train_table)\n\n# Check accuracy and make predictions from the model, applied to the test data\nC50_test_pred <- predict(C50_train_fit, test)\nC50_test_table <- table(C50_test_pred, y_test)\nC50_test_accuracy[i] <- sum(diag(C50_test_table)) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\nC50_test_mean <- mean(diag(C50_test_table)) / mean(C50_test_table)\n\n# Calculate accuracy\nC50_holdout[i] <- mean(c(C50_test_accuracy_mean))\nC50_holdout_mean <- mean(C50_holdout)\n\nC50_end <- Sys.time()\nC50_duration[i] <- C50_end - C50_start\nC50_duration_mean <- mean(C50_duration)\n\n# Ranger model\n\nranger_start <- Sys.time()\n\n# Fit the model on the training data\nranger_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"RangerModel\")\nranger_train_predict <- predict(object = ranger_train_fit, newdata = train01)\nranger_train_table <- table(ranger_train_predict, y_train)\nranger_train_accuracy[i] <- sum(diag(ranger_train_table)) / sum(ranger_train_table)\nranger_train_accuracy_mean <- mean(ranger_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nranger_test_predict <- predict(object = ranger_train_fit, newdata = test01)\nranger_test_table <- table(ranger_test_predict, y_test)\nranger_test_accuracy[i] <- sum(diag(ranger_test_table)) / sum(ranger_test_table)\nranger_test_accuracy_mean <- mean(ranger_test_accuracy)\nranger_test_pred <- ranger_test_predict\n\n# Calculate overall model accuracy\nranger_holdout[i] <- mean(c(ranger_test_accuracy_mean))\nranger_holdout_mean <- mean(ranger_holdout)\n\nranger_end <- Sys.time()\nranger_duration[i] <- ranger_end - ranger_start\nranger_duration_mean <- mean(ranger_duration)\n\n# Support vector machines\nsvm_start <- Sys.time()\n\nsvm_train_fit <- e1071::svm(y_train ~ ., data = train, kernel = \"radial\", gamma = 1, cost = 1)\nsvm_train_pred <- predict(svm_train_fit, train, type = \"class\")\nsvm_train_table <- table(svm_train_pred, y_train)\nsvm_train_accuracy[i] <- sum(diag(svm_train_table)) / sum(svm_train_table)\nsvm_train_accuracy_mean <- mean(svm_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nsvm_test_pred <- predict(svm_train_fit, test, type = \"class\")\nsvm_test_table <- table(svm_test_pred, y_test)\nsvm_test_accuracy[i] <- sum(diag(svm_test_table)) / sum(svm_test_table)\nsvm_test_accuracy_mean <- mean(svm_test_accuracy)\n\n# Calculate overall model accuracy\nsvm_holdout[i] <- mean(c(svm_test_accuracy_mean))\nsvm_holdout_mean <- mean(svm_holdout)\n\nsvm_end <- Sys.time()\nsvm_duration[i] <- svm_end - svm_start\nsvm_duration_mean <- mean(svm_duration)\n\n# XGBoost\nxgb_start <- Sys.time()\n\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_accuracy <- 0\nxgb_test_accuracy_mean <- 0\nxgb_holdout <- 0\nxgb_table_total <- 0\nxgb_duration <- 0\n\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_train), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_train + 1]\nxgb_train_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_train_accuracy[i] <- sum(diag(xgb_train_table)) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_test + 1]\nxgb_test_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_test_accuracy[i] <- sum(diag(xgb_test_table)) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\n# Calculate overall model accuracy\nxgb_holdout[i] <- mean(c(xgb_test_accuracy_mean))\nxgb_holdout_mean <- mean(xgb_holdout)\n\nxgb_end <- Sys.time()\nxgb_duration[i] <- xgb_end - xgb_start\nxgb_duration_mean <- mean(xgb_duration)\n\n# Build the ensemble of predictions\n\nensemble1 <- data.frame(\n  'Bag_rf' = bag_rf_test_pred,\n  'C50' = C50_test_pred,\n  'Ranger' = ranger_test_predict,\n  'SVM' = svm_test_pred,\n  'XGBoost' = xgb_preds\n)\n\nensemble_row_numbers <- as.numeric(row.names(ensemble1))\nensemble1$y <- df[ensemble_row_numbers, \"y\"]\n\nensemble_index <- sample(c(1:2), nrow(ensemble1), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble1[ensemble_index == 1, ]\nensemble_test <- ensemble1[ensemble_index == 2, ]\nensemble_y_train <- ensemble_train$y\nensemble_y_test <- ensemble_test$y\n\n\n# Ensemble bagged random forest\nensemble_bag_rf_start <- Sys.time()\n\nensemble_bag_train_rf <- randomForest::randomForest(ensemble_y_train ~ ., data = ensemble_train, mtry = ncol(ensemble_train) - 1)\nensemble_bag_rf_train_pred <- predict(ensemble_bag_train_rf, ensemble_train, type = \"class\")\nensemble_bag_rf_train_table <- table(ensemble_bag_rf_train_pred, ensemble_train$y)\nensemble_bag_rf_train_accuracy[i] <- sum(diag(ensemble_bag_rf_train_table)) / sum(ensemble_bag_rf_train_table)\nensemble_bag_rf_train_accuracy_mean <- mean(ensemble_bag_rf_train_accuracy)\n\nensemble_bag_rf_test_pred <- predict(ensemble_bag_train_rf, ensemble_test, type = \"class\")\nensemble_bag_rf_test_table <- table(ensemble_bag_rf_test_pred, ensemble_test$y)\nensemble_bag_rf_test_accuracy[i] <- sum(diag(ensemble_bag_rf_test_table)) / sum(ensemble_bag_rf_test_table)\nensemble_bag_rf_test_accuracy_mean <- mean(ensemble_bag_rf_test_accuracy)\n\nensemble_bag_rf_holdout[i] <- mean(c(ensemble_bag_rf_test_accuracy_mean))\nensemble_bag_rf_holdout_mean <- mean(ensemble_bag_rf_holdout)\n\nensemble_bag_rf_end <- Sys.time()\nensemble_bag_rf_duration[i] <- ensemble_bag_rf_end - ensemble_bag_rf_start\nensemble_bag_rf_duration_mean <- mean(ensemble_bag_rf_duration)\n\n# Ensemble C50\n\nensemble_C50_start <- Sys.time()\n\nensemble_C50_train_fit <- C50::C5.0(ensemble_y_train ~ ., data = ensemble_train)\nensemble_C50_train_pred <- predict(ensemble_C50_train_fit, ensemble_train)\nensemble_C50_train_table <- table(ensemble_C50_train_pred, ensemble_y_train)\nensemble_C50_train_accuracy[i] <- sum(diag(ensemble_C50_train_table)) / sum(ensemble_C50_train_table)\nensemble_C50_train_accuracy_mean <- mean(ensemble_C50_train_accuracy)\n\nensemble_C50_test_pred <- predict(ensemble_C50_train_fit, ensemble_test)\nensemble_C50_test_table <- table(ensemble_C50_test_pred, ensemble_y_test)\nensemble_C50_test_accuracy[i] <- sum(diag(ensemble_C50_test_table)) / sum(ensemble_C50_test_table)\nensemble_C50_test_accuracy_mean <- mean(ensemble_C50_test_accuracy)\n\nensemble_C50_holdout[i] <- mean(c(ensemble_C50_test_accuracy_mean))\nensemble_C50_holdout_mean <- mean(ensemble_C50_holdout)\n\nensemble_C50_end <- Sys.time()\nensemble_C50_duration[i] <- ensemble_C50_end - ensemble_C50_start\nensemble_C50_duration_mean <- mean(ensemble_C50_duration)\n\n# Ensemble using Ranger\n\nensemble_ranger_start <- Sys.time()\n\nensemble_ranger_train_fit <- MachineShop::fit(y ~ ., data = ensemble_train, model = \"RangerModel\")\nensemble_ranger_train_pred <- predict(ensemble_ranger_train_fit, newdata = ensemble_train)\nensemble_ranger_train_table <- table(ensemble_ranger_train_pred, ensemble_y_train)\nensemble_ranger_train_accuracy[i] <- sum(diag(ensemble_ranger_train_table)) / sum(ensemble_ranger_train_table)\nensemble_ranger_train_accuracy_mean <- mean(ensemble_ranger_train_accuracy)\n\nensemble_ranger_test_fit <- MachineShop::fit(y ~ ., data = ensemble_train, model = \"RangerModel\")\nensemble_ranger_test_pred <- predict(ensemble_ranger_test_fit, newdata = ensemble_test)\nensemble_ranger_test_table <- table(ensemble_ranger_test_pred, ensemble_y_test)\nensemble_ranger_test_accuracy[i] <- sum(diag(ensemble_ranger_test_table)) / sum(ensemble_ranger_test_table)\nensemble_ranger_test_accuracy_mean <- mean(ensemble_ranger_test_accuracy)\n\nensemble_ranger_holdout[i] <- mean(c(ensemble_ranger_test_accuracy_mean))\nensemble_ranger_holdout_mean <- mean(ensemble_ranger_holdout)\n\nensemble_ranger_end <- Sys.time()\nensemble_ranger_duration[i] <- ensemble_ranger_end - ensemble_ranger_start\nensemble_ranger_duration_mean <- mean(ensemble_ranger_duration)\n\n# Ensemble Random Forest\n\nensemble_rf_start <- Sys.time()\n\nensemble_train_rf_fit <- randomForest::randomForest(x = ensemble_train, y = ensemble_y_train)\nensemble_rf_train_pred <- predict(ensemble_train_rf_fit, ensemble_train, type = \"class\")\nensemble_rf_train_table <- table(ensemble_rf_train_pred, ensemble_y_train)\nensemble_rf_train_accuracy[i] <- sum(diag(ensemble_rf_train_table)) / sum(ensemble_rf_train_table)\nensemble_rf_train_accuracy_mean <- mean(ensemble_rf_train_accuracy)\n\nensemble_rf_test_pred <- predict(ensemble_train_rf_fit, ensemble_test, type = \"class\")\nensemble_rf_test_table <- table(ensemble_rf_test_pred, ensemble_y_test)\nensemble_rf_test_accuracy[i] <- sum(diag(ensemble_rf_test_table)) / sum(ensemble_rf_test_table)\nensemble_rf_test_accuracy_mean <- mean(ensemble_rf_test_accuracy)\n\nensemble_rf_holdout[i] <- mean(c(ensemble_rf_test_accuracy_mean))\nensemble_rf_holdout_mean <- mean(ensemble_rf_holdout)\n\nensemble_rf_end <- Sys.time()\nensemble_rf_duration[i] <- ensemble_rf_end -ensemble_rf_start\nensemble_rf_duration_mean <- mean(ensemble_rf_duration)\n\n\n# Ensemble Support Vector Machines\n\nensemble_svm_start <- Sys.time()\n\nensemble_svm_train_fit <- e1071::svm(ensemble_y_train ~ ., data = ensemble_train, kernel = \"radial\", gamma = 1, cost = 1)\nensemble_svm_train_pred <- predict(ensemble_svm_train_fit, ensemble_train, type = \"class\")\nensemble_svm_train_table <- table(ensemble_svm_train_pred, ensemble_y_train)\nensemble_svm_train_accuracy[i] <- sum(diag(ensemble_svm_train_table)) / sum(ensemble_svm_train_table)\nensemble_svm_train_accuracy_mean <- mean(ensemble_svm_train_accuracy)\n\nensemble_svm_test_fit <- e1071::svm(ensemble_y_train ~ ., data = ensemble_train, kernel = \"radial\", gamma = 1, cost = 1)\nensemble_svm_test_pred <- predict(ensemble_svm_test_fit, ensemble_test, type = \"class\")\nensemble_svm_test_table <- table(ensemble_svm_test_pred, ensemble_y_test)\nensemble_svm_test_accuracy[i] <- sum(diag(ensemble_svm_test_table)) / sum(ensemble_svm_test_table)\nensemble_svm_test_accuracy_mean <- mean(ensemble_svm_test_accuracy)\n\nensemble_svm_holdout[i] <- mean(c(ensemble_svm_test_accuracy_mean))\nensemble_svm_holdout_mean <- mean(ensemble_svm_holdout)\n\nensemble_svm_end <- Sys.time()\nensemble_svm_duration[i] <-  ensemble_svm_end - ensemble_svm_start\nensemble_svm_duration_mean <- mean(ensemble_svm_duration)\n\n# Return accuracy results\n\nresults <- data.frame(\n  'Model' = c('Bagged_Random_Forest', 'C50', 'Ranger', 'Support_Vector_Machines', 'XGBoost', 'Ensemble_Bag_RF', 'Ensemble_C50', 'Ensemble_Ranger', 'Ensemble_RF', 'Ensemble_SVM'),\n  'Accuracy' = c(bag_rf_holdout_mean, C50_holdout_mean, ranger_holdout_mean, svm_holdout_mean, xgb_holdout_mean, ensemble_bag_rf_holdout_mean, ensemble_C50_holdout_mean, ensemble_ranger_holdout_mean, ensemble_rf_holdout_mean, ensemble_svm_holdout_mean),\n  'Duration' = c(bag_rf_duration_mean, C50_duration_mean, ranger_duration_mean, svm_duration_mean, xgb_duration_mean, ensemble_bag_rf_duration_mean, ensemble_C50_duration_mean, ensemble_ranger_duration_mean, ensemble_rf_duration_mean, ensemble_svm_duration_mean)\n)\n\nresults <- results %>% arrange(desc(Accuracy))\n\nreturn(results)\n\n}# Closing braces for numresamples loop\n}# Closing brace for classification1 function\n\nclassification_1(data = ISLR::Carseats,colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#>                      Model  Accuracy    Duration\n#> 1          Ensemble_Bag_RF 1.0000000 0.129196167\n#> 2             Ensemble_C50 1.0000000 0.007623911\n#> 3              Ensemble_RF 0.9545455 0.018841982\n#> 4             Ensemble_SVM 0.8484848 0.005934000\n#> 5     Bagged_Random_Forest 0.6829268 0.090348959\n#> 6                  XGBoost 0.6768293 6.853636026\n#> 7                   Ranger 0.6646341 0.738637924\n#> 8                      C50 0.6463415 0.422610044\n#> 9  Support_Vector_Machines 0.5792683 0.013298988\n#> 10         Ensemble_Ranger 0.4696970 0.033684015\n\nwarnings()\n\ndf1 <- Ensembles::dry_beans_small\n#> Registered S3 method overwritten by 'tsibble':\n#>   method          from\n#>   format.interval inum\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n\nclassification_1(data = df1, colnum = 17, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#>                      Model  Accuracy    Duration\n#> 1          Ensemble_Bag_RF 1.0000000  0.12194419\n#> 2             Ensemble_C50 1.0000000  0.01354599\n#> 3     Bagged_Random_Forest 0.9081633  0.14628696\n#> 4                   Ranger 0.9047619  0.04827619\n#> 5                  XGBoost 0.8945578 21.21282005\n#> 6                      C50 0.8809524  0.02526808\n#> 7  Support_Vector_Machines 0.8571429  0.04564309\n#> 8             Ensemble_SVM 0.7520661  0.01254821\n#> 9              Ensemble_RF 0.6776860  0.16191602\n#> 10         Ensemble_Ranger 0.1239669  0.04927993\nwarnings()"},{"path":"individual-logistic-models.html","id":"individual-logistic-models","chapter":"7 Individual logistic models","heading":"7 Individual logistic models","text":"Logistic data sets extremely powerful. chapter ’ll use evaluate risk type 2 diabetes Pima Indian women, logistic ensembles chapter ’ll use make recommendations improve performance Lebron James.raises good question: can two fields far apart scientific research (Pima Indians) sports analytics (Lebron) connected? ’s structure data , ’s structure data makes easy use.Logistic regression rooted idea logical variable (hence name). variable logical variable specific number options, usually two options. many possible names come result. Names might include true false, presence absence condition (diabetes), success failure making basket.logistic modeling values converted 1 0 (converted already). Let’s start getting Pima Indians data set Kaggle web site:https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-databaseDownload data set, open system. example,can clearly see eight features used predict ninth feature, Outcome. final logistic model form : Outcome ~ ., data = df.far common way using Generalized Linear Models, begin . follow well established method building model, used numerical classification data:Load librarySet initial values 0Create functionSet random resamplingBreak data train testFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setFor set examples also going add results ROC curve, ROC curve printed automatically.results consistent similar results using Generalized Linear Models data set.","code":"\n\ndiabetes <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/EnsemblesData/main/diabetes.csv')\nhead(diabetes)\n#>   Pregnancies Glucose BloodPressure SkinThickness Insulin\n#> 1           6     148            72            35       0\n#> 2           1      85            66            29       0\n#> 3           8     183            64             0       0\n#> 4           1      89            66            23      94\n#> 5           0     137            40            35     168\n#> 6           5     116            74             0       0\n#>    BMI DiabetesPedigreeFunction Age Outcome\n#> 1 33.6                    0.627  50       1\n#> 2 26.6                    0.351  31       0\n#> 3 23.3                    0.672  32       1\n#> 4 28.1                    0.167  21       0\n#> 5 43.1                    2.288  33       1\n#> 6 25.6                    0.201  30       0\n\n# Load the library\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(pROC)\n#> Type 'citation(\"pROC\")' for a citation.\n#> \n#> Attaching package: 'pROC'\n#> \n#> The following objects are masked from 'package:stats':\n#> \n#>     cov, smooth, var\n\n# Set initial values to 0\n\nglm_train_accuracy <- 0\nglm_test_accuracy <- 0\nglm_holdout_accuracy <- 0\nglm_duration <- 0\nglm_table_total <- 0\n\n# Create the function\n\nglm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \ncolnames(data)[colnum] <- \"y\"\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n\ndf <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n# Set up random resampling\n\nfor (i in 1:numresamples) {\n  \nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ny_train <- train$y\ny_test <- test$y\n\n# Fit the model to the training data, make predictions on the holdout data\n\nglm_train_fit <- stats::glm(y ~ ., data = train, family = binomial(link = \"logit\"))\n\nglm_train_pred <- stats::predict(glm_train_fit, train, type = \"response\")\nglm_train_predictions <- ifelse(glm_train_pred > 0.5, 1, 0)\nglm_train_table <- table(glm_train_predictions, y_train)\nglm_train_accuracy[i] <- (glm_train_table[1, 1] + glm_train_table[2, 2]) / sum(glm_train_table)\nglm_train_accuracy_mean <- mean(glm_train_accuracy)\n\nglm_test_pred <- stats::predict(glm_train_fit, test, type = \"response\")\nglm_test_predictions <- ifelse(glm_test_pred > 0.5, 1, 0)\nglm_test_table <- table(glm_test_predictions, y_test)\nglm_test_accuracy[i] <- (glm_test_table[1, 1] + glm_test_table[2, 2]) / sum(glm_test_table)\nglm_test_accuracy_mean <- mean(glm_test_accuracy)\n\nglm_holdout_accuracy_mean <- mean(glm_test_accuracy)\n\nglm_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(glm_test_pred))\nglm_auc <- round((pROC::auc(c(test$y), as.numeric(c(glm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(glm_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Generalized Linear Models \", \"(AUC = \", glm_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n\nreturn(glm_holdout_accuracy_mean)\n\n} # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nglm_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7718631\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"how-to-use-non-glm-models-in-logistic-analysis","chapter":"7 Individual logistic models","heading":"7.0.1 How to use non-GLM models in logistic analysis","text":"authors excellent book, Introduction Statistical Learning, describe demonstrate non-GLM methods may used logistic analysis. investigated Linear Discriminant Analysis, Quadratic Discriminant Analysis K-Nearest Neighbors. look total ten methods, though many possible.","code":""},{"path":"individual-logistic-models.html","id":"eight-individual-models-for-logistic-data","chapter":"7 Individual logistic models","heading":"7.0.2 Eight individual models for logistic data","text":"","code":""},{"path":"individual-logistic-models.html","id":"adaboost","chapter":"7 Individual logistic models","heading":"7.0.3 Adaboost","text":"","code":"\n\n# Load the library\nlibrary(MachineShop)\n#> \n#> Attaching package: 'MachineShop'\n#> The following object is masked from 'package:pROC':\n#> \n#>     auc\n#> The following object is masked from 'package:purrr':\n#> \n#>     lift\n#> The following object is masked from 'package:stats':\n#> \n#>     ppr\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nadaboost_train_accuracy <- 0\nadaboost_test_accuracy <- 0\nadaboost_holdout_accuracy <- 0\nadaboost_duration <- 0\nadaboost_table_total <- 0\n\n# Create the function\n\nadaboost_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nadaboost_train_fit <- MachineShop::fit(formula = as.factor(y) ~ ., data = train, model = \"AdaBoostModel\")\n\nadaboost_train_pred <- stats::predict(adaboost_train_fit, train, type = \"prob\")\nadaboost_train_predictions <- ifelse(adaboost_train_pred > 0.5, 1, 0)\nadaboost_train_table <- table(adaboost_train_predictions, y_train)\nadaboost_train_accuracy[i] <- (adaboost_train_table[1, 1] + adaboost_train_table[2, 2]) / sum(adaboost_train_table)\nadaboost_train_accuracy_mean <- mean(adaboost_train_accuracy)\n    \nadaboost_test_pred <- stats::predict(adaboost_train_fit, test, type = \"prob\")\nadaboost_test_predictions <- ifelse(adaboost_test_pred > 0.5, 1, 0)\nadaboost_test_table <- table(adaboost_test_predictions, y_test)\nadaboost_test_accuracy[i] <- (adaboost_test_table[1, 1] + adaboost_test_table[2, 2]) / sum(adaboost_test_table)\nadaboost_test_accuracy_mean <- mean(adaboost_test_accuracy)\n\nadaboost_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(adaboost_test_pred))\nadaboost_auc <- round((pROC::auc(c(test$y), as.numeric(c(adaboost_test_pred)) - 1)), 4)\nprint(pROC::ggroc(adaboost_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"ADAboost Models \", \"(AUC = \", adaboost_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(adaboost_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nadaboost_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7484076\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"bayesglm-1","chapter":"7 Individual logistic models","heading":"7.0.4 BayesGLM","text":"","code":"\n\n# Load the library\nlibrary(arm)\n#> Loading required package: MASS\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russellconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nbayesglm_train_accuracy <- 0\nbayesglm_test_accuracy <- 0\nbayesglm_holdout_accuracy <- 0\nbayesglm_duration <- 0\nbayesglm_table_total <- 0\n\n# Create the function\n\nbayesglm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = binomial)\n    \nbayesglm_train_pred <- stats::predict(bayesglm_train_fit, train, type = \"response\")\nbayesglm_train_predictions <- ifelse(bayesglm_train_pred > 0.5, 1, 0)\nbayesglm_train_table <- table(bayesglm_train_predictions, y_train)\nbayesglm_train_accuracy[i] <- (bayesglm_train_table[1, 1] + bayesglm_train_table[2, 2]) / sum(bayesglm_train_table)\nbayesglm_train_accuracy_mean <- mean(bayesglm_train_accuracy)\n\nbayesglm_test_pred <- stats::predict(bayesglm_train_fit, test, type = \"response\")\nbayesglm_test_predictions <- ifelse(bayesglm_test_pred > 0.5, 1, 0)\nbayesglm_test_table <- table(bayesglm_test_predictions, y_test)\n\nbayesglm_test_accuracy[i] <- (bayesglm_test_table[1, 1] + bayesglm_test_table[2, 2]) / sum(bayesglm_test_table)\nbayesglm_test_accuracy_mean <- mean(bayesglm_test_accuracy)\n\nbayesglm_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(bayesglm_test_pred))\nbayesglm_auc <- round((pROC::auc(c(test$y), as.numeric(c(bayesglm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(bayesglm_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"Bayesglm Models \", \"(AUC = \", bayesglm_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(bayesglm_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nbayesglm_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7824675\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"c50","chapter":"7 Individual logistic models","heading":"7.0.5 C50","text":"","code":"\n\n# Load the library\nlibrary(C50)\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_holdout_accuracy <- 0\nC50_duration <- 0\nC50_table_total <- 0\n\n# Create the function\n\nC50_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\n\nC50_train_pred <- stats::predict(C50_train_fit, train, type = \"prob\")\nC50_train_predictions <- ifelse(C50_train_pred[, 2] > 0.5, 1, 0)\nC50_train_table <- table(C50_train_predictions, y_train)\nC50_train_accuracy[i] <- (C50_train_table[1, 1] + C50_train_table[2, 2]) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\n\nC50_test_pred <- stats::predict(C50_train_fit, test, type = \"prob\")\nC50_test_predictions <- ifelse(C50_test_pred[, 2] > 0.5, 1, 0)\nC50_test_table <- table(C50_test_predictions, y_test)\nC50_test_accuracy[i] <- (C50_test_table[1, 1] + C50_test_table[2, 2]) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\n\nC50_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(C50_test_predictions)))\nC50_auc <- round((pROC::auc(c(test$y), as.numeric(c(C50_test_predictions)) - 1)), 4)\nprint(pROC::ggroc(C50_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"C50 ROC curve \", \"(AUC = \", C50_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(C50_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nC50_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 1\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"cubist-1","chapter":"7 Individual logistic models","heading":"7.0.6 Cubist","text":"","code":"\n\n# Load the library\nlibrary(Cubist)\n#> Loading required package: lattice\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\ncubist_train_accuracy <- 0\ncubist_test_accuracy <- 0\ncubist_holdout_accuracy <- 0\ncubist_duration <- 0\ncubist_table_total <- 0\n\n# Create the function\n\ncubist_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\ncubist_train_fit <- Cubist::cubist(x = as.data.frame(train), y = train$y)\n    \ncubist_train_pred <- stats::predict(cubist_train_fit, train, type = \"prob\")\ncubist_train_table <- table(cubist_train_pred, y_train)\ncubist_train_accuracy[i] <- (cubist_train_table[1, 1] + cubist_train_table[2, 2]) / sum(cubist_train_table)\ncubist_train_accuracy_mean <- mean(cubist_train_accuracy)\n\ncubist_test_pred <- stats::predict(cubist_train_fit, test, type = \"prob\")\ncubist_test_table <- table(cubist_test_pred, y_test)\ncubist_test_accuracy[i] <- (cubist_test_table[1, 1] + cubist_test_table[2, 2]) / sum(cubist_test_table)\ncubist_test_accuracy_mean <- mean(cubist_test_accuracy)\n\n\ncubist_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(cubist_test_pred)))\ncubist_auc <- round((pROC::auc(c(test$y), as.numeric(c(cubist_test_pred)) - 1)), 4)\nprint(pROC::ggroc(cubist_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Cubist ROC curve \", \"(AUC = \", cubist_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(cubist_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\ncubist_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 1\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"gradient-boosted-1","chapter":"7 Individual logistic models","heading":"7.0.7 Gradient Boosted","text":"","code":"\n\n# Load the library\nlibrary(gbm)\n#> Loaded gbm 2.1.9\n#> This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\ngb_train_accuracy <- 0\ngb_test_accuracy <- 0\ngb_holdout_accuracy <- 0\ngb_duration <- 0\ngb_table_total <- 0\n\n# Create the function\n\ngb_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\ngb_train_fit <- gbm::gbm(train$y ~ ., data = train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\n    \ngb_train_pred <- stats::predict(gb_train_fit, train, type = \"response\")\ngb_train_predictions <- ifelse(gb_train_pred > 0.5, 1, 0)\ngb_train_table <- table(gb_train_predictions, y_train)\ngb_train_accuracy[i] <- (gb_train_table[1, 1] + gb_train_table[2, 2]) / sum(gb_train_table)\ngb_train_accuracy_mean <- mean(gb_train_accuracy)\n\ngb_test_pred <- stats::predict(gb_train_fit, test, type = \"response\")\ngb_test_predictions <- ifelse(gb_test_pred > 0.5, 1, 0)\ngb_test_table <- table(gb_test_predictions, y_test)\ngb_test_accuracy[i] <- (gb_test_table[1, 1] + gb_test_table[2, 2]) / sum(gb_test_table)\ngb_test_accuracy_mean <- mean(gb_test_accuracy)\n\ngb_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(gb_test_pred)))\ngb_auc <- round((pROC::auc(c(test$y), as.numeric(c(gb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(gb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Gradient Boosted ROC curve \", \"(AUC = \", gb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(gb_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\ngb_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Using 100 trees...\n#> Using 100 trees...\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7012987\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"random-forest-2","chapter":"7 Individual logistic models","heading":"7.0.8 Random Forest","text":"","code":"\n\n# Load the library\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nrf_train_accuracy <- 0\nrf_test_accuracy <- 0\nrf_holdout_accuracy <- 0\nrf_duration <- 0\nrf_table_total <- 0\n\n# Create the function\n\nrf_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nrf_train_fit <- randomForest(x = train, y = as.factor(y_train), data = df)\n    \nrf_train_pred <- stats::predict(rf_train_fit, train, type = \"prob\")\nrf_train_probabilities <- ifelse(rf_train_pred > 0.50, 1, 0)[, 2]\nrf_train_table <- table(rf_train_probabilities, y_train)\nrf_train_accuracy[i] <- (rf_train_table[1, 1] + rf_train_table[2, 2]) / sum(rf_train_table)\nrf_train_accuracy_mean <- mean(rf_train_accuracy)\n\nrf_test_pred <- stats::predict(rf_train_fit, test, type = \"prob\")\nrf_test_probabilities <- ifelse(rf_test_pred > 0.50, 1, 0)[, 2]\nrf_test_table <- table(rf_test_probabilities, y_test)\nrf_test_accuracy[i] <- (rf_test_table[1, 1] + rf_test_table[2, 2]) / sum(rf_test_table)\nrf_test_accuracy_mean <- mean(rf_test_accuracy)\n\nrf_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(rf_test_probabilities)))\nrf_auc <- round((pROC::auc(c(test$y), as.numeric(c(rf_test_probabilities)) - 1)), 4)\nprint(pROC::ggroc(rf_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Random Forest \", \"(AUC = \", rf_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(rf_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nrf_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 1\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"support-vector-machines-2","chapter":"7 Individual logistic models","heading":"7.0.9 Support Vector Machines","text":"","code":"\n\n# Load the library\nlibrary(e1071)\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nsvm_train_accuracy <- 0\nsvm_test_accuracy <- 0\nsvm_holdout_accuracy <- 0\nsvm_duration <- 0\nsvm_table_total <- 0\n\n# Create the function\n\nsvm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nsvm_train_fit <- e1071::svm(as.factor(y) ~ ., data = train)\n    \nsvm_train_pred <- stats::predict(svm_train_fit, train, type = \"prob\")\nsvm_train_table <- table(svm_train_pred, y_train)\nsvm_train_accuracy[i] <- (svm_train_table[1, 1] + svm_train_table[2, 2]) / sum(svm_train_table)\nsvm_train_accuracy_mean <- mean(svm_train_accuracy)\n\nsvm_test_pred <- stats::predict(svm_train_fit, test, type = \"prob\")\nsvm_test_table <- table(svm_test_pred, y_test)\nsvm_test_accuracy[i] <- (svm_test_table[1, 1] + svm_test_table[2, 2]) / sum(svm_test_table)\nsvm_test_accuracy_mean <- mean(svm_test_accuracy)\n\nsvm_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(svm_test_pred)))\nsvm_auc <- round((pROC::auc(c(test$y), as.numeric(c(svm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(svm_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Random Forest \", \"(AUC = \", svm_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(svm_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nsvm_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7317881\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"xgboost-2","chapter":"7 Individual logistic models","heading":"7.0.10 XGBoost","text":"","code":"\n\n# Load the library\nlibrary(xgboost)\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_holdout_accuracy <- 0\nxgb_duration <- 0\nxgb_table_total <- 0\n\n# Create the function\n\nxgb_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n# Fit the model to the training data, make predictions on the holdout data\ntrain_x <- data.matrix(train[, -ncol(train)])\ntrain_y <- train[, ncol(train)]\n    \n# define predictor and response variables in test set\ntest_x <- data.matrix(test[, -ncol(test)])\ntest_y <- test[, ncol(test)]\n    \n# define final train and test sets\nxgb_train <- xgboost::xgb.DMatrix(data = train_x, label = train_y)\nxgb_test <- xgboost::xgb.DMatrix(data = test_x, label = test_y)\n\n# define watchlist\nwatchlist <- list(train = xgb_train)\nwatchlist_test <- list(train = xgb_train, test = xgb_test)\n\nxgb_model <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_test, nrounds = 70)\n    \nxgb_min <- which.min(xgb_model$evaluation_log$validation_rmse)\n    \nxgb_train_pred <- stats::predict(object = xgb_model, newdata = train_x, type = \"prob\")\nxgb_train_predictions <- ifelse(xgb_train_pred > 0.5, 1, 0)\nxgb_train_table <- table(xgb_train_predictions, y_train)\nxgb_train_accuracy[i] <- (xgb_train_table[1, 1] + xgb_train_table[2, 2]) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\nxgb_test_pred <- stats::predict(object = xgb_model, newdata = test_x, type = \"prob\")\nxgb_test_predictions <- ifelse(xgb_test_pred > 0.5, 1, 0)\nxgb_test_table <- table(xgb_test_predictions, y_test)\nxgb_test_accuracy[i] <- (xgb_test_table[1, 1] + xgb_test_table[2, 2]) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\nxgb_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(xgb_test_pred)))\nxgb_auc <- round((pROC::auc(c(test$y), as.numeric(c(xgb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(xgb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"XGBoost \", \"(AUC = \", xgb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(xgb_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nxgb_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> [1]  train-rmse:0.443820 test-rmse:0.451114 \n#> [2]  train-rmse:0.410144 test-rmse:0.424842 \n#> [3]  train-rmse:0.389456 test-rmse:0.414245 \n#> [4]  train-rmse:0.375497 test-rmse:0.406926 \n#> [5]  train-rmse:0.361120 test-rmse:0.404512 \n#> [6]  train-rmse:0.351706 test-rmse:0.402855 \n#> [7]  train-rmse:0.344964 test-rmse:0.402907 \n#> [8]  train-rmse:0.338652 test-rmse:0.403506 \n#> [9]  train-rmse:0.332740 test-rmse:0.402499 \n#> [10] train-rmse:0.329630 test-rmse:0.404849 \n#> [11] train-rmse:0.326029 test-rmse:0.403572 \n#> [12] train-rmse:0.322103 test-rmse:0.403685 \n#> [13] train-rmse:0.316984 test-rmse:0.402375 \n#> [14] train-rmse:0.314421 test-rmse:0.402665 \n#> [15] train-rmse:0.312002 test-rmse:0.402012 \n#> [16] train-rmse:0.306025 test-rmse:0.401865 \n#> [17] train-rmse:0.300422 test-rmse:0.406396 \n#> [18] train-rmse:0.296890 test-rmse:0.407696 \n#> [19] train-rmse:0.294799 test-rmse:0.407525 \n#> [20] train-rmse:0.293248 test-rmse:0.409466 \n#> [21] train-rmse:0.290883 test-rmse:0.408838 \n#> [22] train-rmse:0.289344 test-rmse:0.407957 \n#> [23] train-rmse:0.286014 test-rmse:0.405951 \n#> [24] train-rmse:0.284558 test-rmse:0.408597 \n#> [25] train-rmse:0.282312 test-rmse:0.409822 \n#> [26] train-rmse:0.279900 test-rmse:0.409098 \n#> [27] train-rmse:0.279171 test-rmse:0.409240 \n#> [28] train-rmse:0.277008 test-rmse:0.409459 \n#> [29] train-rmse:0.275484 test-rmse:0.409977 \n#> [30] train-rmse:0.270146 test-rmse:0.409051 \n#> [31] train-rmse:0.268145 test-rmse:0.409530 \n#> [32] train-rmse:0.267432 test-rmse:0.408804 \n#> [33] train-rmse:0.266643 test-rmse:0.408577 \n#> [34] train-rmse:0.265945 test-rmse:0.408396 \n#> [35] train-rmse:0.262627 test-rmse:0.408979 \n#> [36] train-rmse:0.261944 test-rmse:0.409056 \n#> [37] train-rmse:0.261083 test-rmse:0.410124 \n#> [38] train-rmse:0.259975 test-rmse:0.410166 \n#> [39] train-rmse:0.257546 test-rmse:0.409997 \n#> [40] train-rmse:0.254742 test-rmse:0.406248 \n#> [41] train-rmse:0.253515 test-rmse:0.405945 \n#> [42] train-rmse:0.253009 test-rmse:0.405861 \n#> [43] train-rmse:0.248366 test-rmse:0.406695 \n#> [44] train-rmse:0.245926 test-rmse:0.405393 \n#> [45] train-rmse:0.245340 test-rmse:0.405159 \n#> [46] train-rmse:0.241983 test-rmse:0.404499 \n#> [47] train-rmse:0.238854 test-rmse:0.403837 \n#> [48] train-rmse:0.237425 test-rmse:0.402440 \n#> [49] train-rmse:0.234284 test-rmse:0.403601 \n#> [50] train-rmse:0.232766 test-rmse:0.403870 \n#> [51] train-rmse:0.230626 test-rmse:0.404637 \n#> [52] train-rmse:0.230082 test-rmse:0.404204 \n#> [53] train-rmse:0.229541 test-rmse:0.404657 \n#> [54] train-rmse:0.226442 test-rmse:0.403503 \n#> [55] train-rmse:0.222275 test-rmse:0.404375 \n#> [56] train-rmse:0.221854 test-rmse:0.404138 \n#> [57] train-rmse:0.219353 test-rmse:0.405571 \n#> [58] train-rmse:0.217785 test-rmse:0.405259 \n#> [59] train-rmse:0.215515 test-rmse:0.404085 \n#> [60] train-rmse:0.214636 test-rmse:0.403443 \n#> [61] train-rmse:0.213945 test-rmse:0.403807 \n#> [62] train-rmse:0.211350 test-rmse:0.404858 \n#> [63] train-rmse:0.209434 test-rmse:0.404904 \n#> [64] train-rmse:0.208350 test-rmse:0.404777 \n#> [65] train-rmse:0.206028 test-rmse:0.404528 \n#> [66] train-rmse:0.205355 test-rmse:0.404718 \n#> [67] train-rmse:0.203309 test-rmse:0.405660 \n#> [68] train-rmse:0.201764 test-rmse:0.404970 \n#> [69] train-rmse:0.199831 test-rmse:0.404663 \n#> [70] train-rmse:0.198291 test-rmse:0.405125\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7429577\n\n# Check for any errors\nwarnings()"},{"path":"advice-to-lebron-james-and-everyone-who-does-talent-analytics-logistic-ensembles.html","id":"advice-to-lebron-james-and-everyone-who-does-talent-analytics-logistic-ensembles","chapter":"8 Advice to Lebron James (and everyone who does talent analytics): Logistic ensembles","heading":"8 Advice to Lebron James (and everyone who does talent analytics): Logistic ensembles","text":"section ’re going take lessons previous chapter move making ensembles models. process extremely similar, follows steps:Load librarySet initial values 0Create functionSet random resamplingBreak data train testFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setLogistic ensembles can used extremely wide range fields. Previously modeled diabetes Pima Indian women. chapter’s example performance court Lebron James.’s image Lebron play.LOT data sports HR analytics (extremely similar ways). lot data set logistic data. example, data set performance Lebron James. main column interest “result”, either 1 0. Thus perfectly fits requirements logistic analysis.Let’s look structure data:see numbers. might easier “qtr” “opponent” changed factors, ’ll first.Now ’re ready create ensemble models make predictions Lebron’s future performance. new skill chapter saving trained models Environment. allow us look trained models, use make strongest evidence based recommendations.use following individual models ensemble models:Individual models:AdaBoostBayesGLMC50CubistGeneralized Linear Models (GLM)Random ForestXGBoostWe make ensemble predictions five models, use ensemble model predictions Lebron’s performance.also show ROC curves results, save trained models end.","code":"\n\nlibrary(Ensembles)\n#> Loading required package: arm\n#> Loading required package: MASS\n#> Loading required package: Matrix\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russellconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\n#> Loading required package: brnn\n#> Loading required package: Formula\n#> Loading required package: truncnorm\n#> Loading required package: broom\n#> Loading required package: C50\n#> Loading required package: caret\n#> Loading required package: ggplot2\n#> Loading required package: lattice\n#> Loading required package: class\n#> Loading required package: corrplot\n#> corrplot 0.92 loaded\n#> \n#> Attaching package: 'corrplot'\n#> The following object is masked from 'package:arm':\n#> \n#>     corrplot\n#> Loading required package: Cubist\n#> Loading required package: doParallel\n#> Loading required package: foreach\n#> Loading required package: iterators\n#> Loading required package: parallel\n#> Loading required package: dplyr\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:MASS':\n#> \n#>     select\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n#> Loading required package: e1071\n#> Loading required package: fable\n#> Loading required package: fabletools\n#> Registered S3 method overwritten by 'tsibble':\n#>   method          from\n#>   format.interval inum\n#> \n#> Attaching package: 'fabletools'\n#> The following object is masked from 'package:e1071':\n#> \n#>     interpolate\n#> The following objects are masked from 'package:caret':\n#> \n#>     MAE, RMSE\n#> The following object is masked from 'package:lme4':\n#> \n#>     refit\n#> Loading required package: fable.prophet\n#> Loading required package: Rcpp\n#> Loading required package: feasts\n#> Loading required package: gam\n#> Loading required package: splines\n#> Loaded gam 1.22-3\n#> Loading required package: gbm\n#> Loaded gbm 2.1.9\n#> This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n#> Loading required package: GGally\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n#> Loading required package: glmnet\n#> Loaded glmnet 4.1-8\n#> Loading required package: gridExtra\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> Loading required package: gt\n#> Loading required package: gtExtras\n#> \n#> Attaching package: 'gtExtras'\n#> The following object is masked from 'package:MASS':\n#> \n#>     select\n#> Loading required package: ipred\n#> Loading required package: kernlab\n#> \n#> Attaching package: 'kernlab'\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     alpha\n#> Loading required package: klaR\n#> Loading required package: leaps\n#> Loading required package: MachineShop\n#> \n#> Attaching package: 'MachineShop'\n#> The following objects are masked from 'package:fabletools':\n#> \n#>     accuracy, response\n#> The following objects are masked from 'package:caret':\n#> \n#>     calibration, lift, precision, recall, rfe,\n#>     sensitivity, specificity\n#> The following object is masked from 'package:stats':\n#> \n#>     ppr\n#> Loading required package: magrittr\n#> Loading required package: mda\n#> Loaded mda 0.5-4\n#> \n#> Attaching package: 'mda'\n#> The following object is masked from 'package:MachineShop':\n#> \n#>     confusion\n#> Loading required package: Metrics\n#> \n#> Attaching package: 'Metrics'\n#> The following objects are masked from 'package:MachineShop':\n#> \n#>     accuracy, auc, mae, mse, msle, precision, recall,\n#>     rmse, rmsle\n#> The following object is masked from 'package:fabletools':\n#> \n#>     accuracy\n#> The following objects are masked from 'package:caret':\n#> \n#>     precision, recall\n#> Loading required package: neuralnet\n#> \n#> Attaching package: 'neuralnet'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     compute\n#> Loading required package: pls\n#> \n#> Attaching package: 'pls'\n#> The following object is masked from 'package:corrplot':\n#> \n#>     corrplot\n#> The following object is masked from 'package:caret':\n#> \n#>     R2\n#> The following objects are masked from 'package:arm':\n#> \n#>     coefplot, corrplot\n#> The following object is masked from 'package:stats':\n#> \n#>     loadings\n#> Loading required package: pROC\n#> Type 'citation(\"pROC\")' for a citation.\n#> \n#> Attaching package: 'pROC'\n#> The following object is masked from 'package:Metrics':\n#> \n#>     auc\n#> The following object is masked from 'package:MachineShop':\n#> \n#>     auc\n#> The following objects are masked from 'package:stats':\n#> \n#>     cov, smooth, var\n#> Loading required package: purrr\n#> \n#> Attaching package: 'purrr'\n#> The following object is masked from 'package:magrittr':\n#> \n#>     set_names\n#> The following object is masked from 'package:MachineShop':\n#> \n#>     lift\n#> The following object is masked from 'package:kernlab':\n#> \n#>     cross\n#> The following objects are masked from 'package:foreach':\n#> \n#>     accumulate, when\n#> The following object is masked from 'package:caret':\n#> \n#>     lift\n#> Loading required package: randomForest\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:gridExtra':\n#> \n#>     combine\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\n#> Loading required package: reactable\n#> Loading required package: reactablefmtr\n#> \n#> Attaching package: 'reactablefmtr'\n#> The following object is masked from 'package:randomForest':\n#> \n#>     margin\n#> The following objects are masked from 'package:gt':\n#> \n#>     google_font, html\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\n#> Loading required package: readr\n#> Loading required package: rpart\n#> Loading required package: scales\n#> \n#> Attaching package: 'scales'\n#> The following object is masked from 'package:readr':\n#> \n#>     col_factor\n#> The following object is masked from 'package:purrr':\n#> \n#>     discard\n#> The following object is masked from 'package:kernlab':\n#> \n#>     alpha\n#> The following object is masked from 'package:arm':\n#> \n#>     rescale\n#> Loading required package: tibble\n#> Loading required package: tidyr\n#> \n#> Attaching package: 'tidyr'\n#> The following object is masked from 'package:magrittr':\n#> \n#>     extract\n#> The following objects are masked from 'package:Matrix':\n#> \n#>     expand, pack, unpack\n#> Loading required package: tree\n#> Loading required package: tsibble\n#> \n#> Attaching package: 'tsibble'\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, union\n#> Loading required package: xgboost\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\nhead(lebron, n = 20)\n#>    top left  date qtr time_remaining result shot_type\n#> 1  310  203 19283   2            566      0         3\n#> 2  213  259 19283   2            518      0         2\n#> 3  143  171 19283   2            490      0         2\n#> 4   68  215 19283   2            324      1         2\n#> 5   66  470 19283   2             62      0         3\n#> 6   63  239 19283   4            690      1         2\n#> 7  230   54 19283   4            630      0         3\n#> 8   53  224 19283   4            605      1         2\n#> 9  241   67 19283   4            570      0         3\n#> 10 273  113 19283   4            535      0         3\n#> 11  62  224 19283   4            426      0         2\n#> 12  63  249 19283   4            233      1         2\n#> 13 103  236 19283   4            154      0         2\n#> 14  54  249 19283   4            108      1         2\n#> 15  53  240 19283   4             58      0         2\n#> 16 230   71 19283   5            649      1         3\n#> 17 231  358 19283   5            540      0         2\n#> 18  61  240 19283   5            524      1         2\n#> 19  59  235 19283   5             71      1         2\n#> 20 299  188 19283   5              6      1         3\n#>    distance_ft lead lebron_team_score opponent_team_score\n#> 1           26    0                 2                   2\n#> 2           16    0                 4                   5\n#> 3           11    0                 4                   7\n#> 4            3    0                12                  19\n#> 5           23    0                22                  23\n#> 6            1    0                24                  25\n#> 7           26    0                24                  27\n#> 8            2    0                26                  27\n#> 9           26    0                26                  29\n#> 10          25    0                26                  32\n#> 11           2    0                31                  39\n#> 12           1    0                39                  49\n#> 13           5    0                39                  51\n#> 14           1    0                44                  53\n#> 15           0    0                46                  55\n#> 16          25    0                58                  63\n#> 17          21    0                60                  70\n#> 18           1    0                62                  70\n#> 19           1    0                68                  91\n#> 20          25    0                71                  91\n#>    opponent\n#> 1         9\n#> 2         9\n#> 3         9\n#> 4         9\n#> 5         9\n#> 6         9\n#> 7         9\n#> 8         9\n#> 9         9\n#> 10        9\n#> 11        9\n#> 12        9\n#> 13        9\n#> 14        9\n#> 15        9\n#> 16        9\n#> 17        9\n#> 18        9\n#> 19        9\n#> 20        9\nlebron <- Ensembles::lebron\nstr(Ensembles::lebron)\n#> 'data.frame':    1533 obs. of  12 variables:\n#>  $ top                : int  310 213 143 68 66 63 230 53 241 273 ...\n#>  $ left               : int  203 259 171 215 470 239 54 224 67 113 ...\n#>  $ date               : num  19283 19283 19283 19283 19283 ...\n#>  $ qtr                : num  2 2 2 2 2 4 4 4 4 4 ...\n#>  $ time_remaining     : num  566 518 490 324 62 690 630 605 570 535 ...\n#>  $ result             : num  0 0 0 1 0 1 0 1 0 0 ...\n#>  $ shot_type          : int  3 2 2 2 3 2 3 2 3 3 ...\n#>  $ distance_ft        : int  26 16 11 3 23 1 26 2 26 25 ...\n#>  $ lead               : num  0 0 0 0 0 0 0 0 0 0 ...\n#>  $ lebron_team_score  : int  2 4 4 12 22 24 24 26 26 26 ...\n#>  $ opponent_team_score: int  2 5 7 19 23 25 27 27 29 32 ...\n#>  $ opponent           : num  9 9 9 9 9 9 9 9 9 9 ...\n\nlebron$qtr <- as.factor(lebron$qtr)\nlebron$opponent <- as.factor(lebron$opponent)\n\n# Load libraries - note these will work with individual and ensemble models\nlibrary(arm) # to use with BayesGLM\nlibrary(C50) # To use with C50\nlibrary(Cubist) # To use with Cubist modeling\nlibrary(MachineShop)# To use with ADABoost\nlibrary(randomForest) # Random Forest models\nlibrary(tidyverse) # My favorite tool for data science!\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ lubridate 1.9.3     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ purrr::accumulate()     masks foreach::accumulate()\n#> ✖ scales::alpha()         masks kernlab::alpha(), ggplot2::alpha()\n#> ✖ scales::col_factor()    masks readr::col_factor()\n#> ✖ randomForest::combine() masks gridExtra::combine(), dplyr::combine()\n#> ✖ neuralnet::compute()    masks dplyr::compute()\n#> ✖ purrr::cross()          masks kernlab::cross()\n#> ✖ scales::discard()       masks purrr::discard()\n#> ✖ tidyr::expand()         masks Matrix::expand()\n#> ✖ tidyr::extract()        masks magrittr::extract()\n#> ✖ dplyr::filter()         masks stats::filter()\n#> ✖ lubridate::interval()   masks tsibble::interval()\n#> ✖ dplyr::lag()            masks stats::lag()\n#> ✖ purrr::lift()           masks MachineShop::lift(), caret::lift()\n#> ✖ reactablefmtr::margin() masks randomForest::margin(), ggplot2::margin()\n#> ✖ tidyr::pack()           masks Matrix::pack()\n#> ✖ gtExtras::select()      masks dplyr::select(), MASS::select()\n#> ✖ purrr::set_names()      masks magrittr::set_names()\n#> ✖ xgboost::slice()        masks dplyr::slice()\n#> ✖ tidyr::unpack()         masks Matrix::unpack()\n#> ✖ purrr::when()           masks foreach::when()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(pROC) # To print ROC curves\n\n# Set initial values to 0\nadaboost_train_accuracy <- 0\nadaboost_test_accuracy <- 0\nadaboost_holdout_accuracy <- 0\nadaboost_duration <- 0\nadaboost_table_total <- 0\n\nbayesglm_train_accuracy <- 0\nbayesglm_test_accuracy <- 0\nbayesglm_holdout_accuracy <- 0\nbayesglm_duration <- 0\nbayesglm_table_total <- 0\n\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_holdout_accuracy <- 0\nC50_duration <- 0\nC50_table_total <- 0\n\ncubist_train_accuracy <- 0\ncubist_test_accuracy <- 0\ncubist_holdout_accuracy <- 0\ncubist_duration <- 0\ncubist_table_total <- 0\n\nrf_train_accuracy <- 0\nrf_test_accuracy <- 0\nrf_holdout_accuracy <- 0\nrf_duration <- 0\nrf_table_total <- 0\n\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_holdout_accuracy <- 0\nxgb_duration <- 0\nxgb_table_total <- 0\n\n\nensemble_adaboost_train_accuracy <- 0\nensemble_adaboost_test_accuracy <- 0\nensemble_adaboost_holdout_accuracy <- 0\nensemble_adaboost_duration <- 0\nensemble_adaboost_table_total <- 0\nensemble_adaboost_train_pred <- 0\n\nensemble_bayesglm_train_accuracy <- 0\nensemble_bayesglm_test_accuracy <- 0\nensemble_bayesglm_holdout_accuracy <- 0\nensemble_bayesglm_duration <- 0\nensemble_bayesglm_table_total <- 0\n\nensemble_C50_train_accuracy <- 0\nensemble_C50_test_accuracy <- 0\nensemble_C50_holdout_accuracy <- 0\nensemble_C50_duration <- 0\nensemble_C50_table_total <- 0\n\nensemble_rf_train_accuracy <- 0\nensemble_rf_test_accuracy <- 0\nensemble_rf_holdout_accuracy <- 0\nensemble_rf_duration <- 0\nensemble_rf_table_total <- 0\n\nensemble_xgb_train_accuracy <- 0\nensemble_xgb_test_accuracy <- 0\nensemble_xgb_holdout_accuracy <- 0\nensemble_xgb_duration <- 0\nensemble_xgb_table_total <- 0\n\n# Create the function\n\nlogistic_1 <- function(data, colnum, numresamples, train_amount, test_amount){\ncolnames(data)[colnum] <- \"y\"\n  \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \ndf <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n# Set up random resampling\n  \nfor (i in 1:numresamples) {\n    \nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n    \ny_train <- train$y\ny_test <- test$y\n  \n  \n# ADABoost model\nadaboost_train_fit <- MachineShop::fit(formula = as.factor(y) ~ ., data = train, model = \"AdaBoostModel\")\n\nadaboost_train_pred <- stats::predict(adaboost_train_fit, train, type = \"prob\")\nadaboost_train_predictions <- ifelse(adaboost_train_pred > 0.5, 1, 0)\nadaboost_train_table <- table(adaboost_train_predictions, y_train)\nadaboost_train_accuracy[i] <- (adaboost_train_table[1, 1] + adaboost_train_table[2, 2]) / sum(adaboost_train_table)\nadaboost_train_accuracy_mean <- mean(adaboost_train_accuracy)\n\nadaboost_test_pred <- stats::predict(adaboost_train_fit, test, type = \"prob\")\nadaboost_test_predictions <- ifelse(adaboost_test_pred > 0.5, 1, 0)\nadaboost_test_table <- table(adaboost_test_predictions, y_test)\nadaboost_test_accuracy[i] <- (adaboost_test_table[1, 1] + adaboost_test_table[2, 2]) / sum(adaboost_test_table)\nadaboost_test_accuracy_mean <- mean(adaboost_test_accuracy)\n\nadaboost_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(adaboost_test_pred))\nadaboost_auc <- round((pROC::auc(c(test$y), as.numeric(c(adaboost_test_pred)) - 1)), 4)\nprint(pROC::ggroc(adaboost_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"ADAboost Models \", \"(AUC = \", adaboost_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n\n# BayesGLM\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = binomial)\n    \nbayesglm_train_pred <- stats::predict(bayesglm_train_fit, train, type = \"response\")\nbayesglm_train_predictions <- ifelse(bayesglm_train_pred > 0.5, 1, 0)\nbayesglm_train_table <- table(bayesglm_train_predictions, y_train)\nbayesglm_train_accuracy[i] <- (bayesglm_train_table[1, 1] + bayesglm_train_table[2, 2]) / sum(bayesglm_train_table)\nbayesglm_train_accuracy_mean <- mean(bayesglm_train_accuracy)\n\nbayesglm_test_pred <- stats::predict(bayesglm_train_fit, test, type = \"response\")\nbayesglm_test_predictions <- ifelse(bayesglm_test_pred > 0.5, 1, 0)\nbayesglm_test_table <- table(bayesglm_test_predictions, y_test)\n\nbayesglm_test_accuracy[i] <- (bayesglm_test_table[1, 1] + bayesglm_test_table[2, 2]) / sum(bayesglm_test_table)\nbayesglm_test_accuracy_mean <- mean(bayesglm_test_accuracy)\n\nbayesglm_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(bayesglm_test_pred))\nbayesglm_auc <- round((pROC::auc(c(test$y), as.numeric(c(bayesglm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(bayesglm_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"Bayesglm Models \", \"(AUC = \", bayesglm_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# C50 model\n\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\n\nC50_train_pred <- stats::predict(C50_train_fit, train, type = \"prob\")\nC50_train_predictions <- ifelse(C50_train_pred[, 2] > 0.5, 1, 0)\nC50_train_table <- table(C50_train_predictions, y_train)\nC50_train_accuracy[i] <- (C50_train_table[1, 1] + C50_train_table[2, 2]) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\n\nC50_test_pred <- stats::predict(C50_train_fit, test, type = \"prob\")\nC50_test_predictions <- ifelse(C50_test_pred[, 2] > 0.5, 1, 0)\nC50_test_table <- table(C50_test_predictions, y_test)\nC50_test_accuracy[i] <- (C50_test_table[1, 1] + C50_test_table[2, 2]) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\n\nC50_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(C50_test_predictions)))\nC50_auc <- round((pROC::auc(c(test$y), as.numeric(c(C50_test_predictions)) - 1)), 4)\nprint(pROC::ggroc(C50_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"C50 ROC curve \", \"(AUC = \", C50_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# Cubist\ncubist_train_fit <- Cubist::cubist(x = as.data.frame(train), y = train$y)\n    \ncubist_train_pred <- stats::predict(cubist_train_fit, train, type = \"prob\")\ncubist_train_table <- table(cubist_train_pred, y_train)\ncubist_train_accuracy[i] <- (cubist_train_table[1, 1] + cubist_train_table[2, 2]) / sum(cubist_train_table)\ncubist_train_accuracy_mean <- mean(cubist_train_accuracy)\n\ncubist_test_pred <- stats::predict(cubist_train_fit, test, type = \"prob\")\ncubist_test_table <- table(cubist_test_pred, y_test)\ncubist_test_accuracy[i] <- (cubist_test_table[1, 1] + cubist_test_table[2, 2]) / sum(cubist_test_table)\ncubist_test_accuracy_mean <- mean(cubist_test_accuracy)\n\n\ncubist_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(cubist_test_pred)))\ncubist_auc <- round((pROC::auc(c(test$y), as.numeric(c(cubist_test_pred)) - 1)), 4)\nprint(pROC::ggroc(cubist_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Cubist ROC curve \", \"(AUC = \", cubist_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# Random Forest\nrf_train_fit <- randomForest(x = train, y = as.factor(y_train), data = df)\n    \nrf_train_pred <- stats::predict(rf_train_fit, train, type = \"prob\")\nrf_train_probabilities <- ifelse(rf_train_pred > 0.50, 1, 0)[, 2]\nrf_train_table <- table(rf_train_probabilities, y_train)\nrf_train_accuracy[i] <- (rf_train_table[1, 1] + rf_train_table[2, 2]) / sum(rf_train_table)\nrf_train_accuracy_mean <- mean(rf_train_accuracy)\n\nrf_test_pred <- stats::predict(rf_train_fit, test, type = \"prob\")\nrf_test_probabilities <- ifelse(rf_test_pred > 0.50, 1, 0)[, 2]\nrf_test_table <- table(rf_test_probabilities, y_test)\nrf_test_accuracy[i] <- (rf_test_table[1, 1] + rf_test_table[2, 2]) / sum(rf_test_table)\nrf_test_accuracy_mean <- mean(rf_test_accuracy)\n\nrf_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(rf_test_probabilities)))\nrf_auc <- round((pROC::auc(c(test$y), as.numeric(c(rf_test_probabilities)) - 1)), 4)\nprint(pROC::ggroc(rf_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Random Forest \", \"(AUC = \", rf_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# XGBoost\ntrain_x <- data.matrix(train[, -ncol(train)])\ntrain_y <- train[, ncol(train)]\n    \n# define predictor and response variables in test set\ntest_x <- data.matrix(test[, -ncol(test)])\ntest_y <- test[, ncol(test)]\n    \n# define final train and test sets\nxgb_train <- xgboost::xgb.DMatrix(data = train_x, label = train_y)\nxgb_test <- xgboost::xgb.DMatrix(data = test_x, label = test_y)\n\n# define watchlist\nwatchlist <- list(train = xgb_train)\nwatchlist_test <- list(train = xgb_train, test = xgb_test)\n\nxgb_model <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_test, nrounds = 70)\n    \nxgb_min <- which.min(xgb_model$evaluation_log$validation_rmse)\n    \nxgb_train_pred <- stats::predict(object = xgb_model, newdata = train_x, type = \"prob\")\nxgb_train_predictions <- ifelse(xgb_train_pred > 0.5, 1, 0)\nxgb_train_table <- table(xgb_train_predictions, y_train)\nxgb_train_accuracy[i] <- (xgb_train_table[1, 1] + xgb_train_table[2, 2]) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\nxgb_test_pred <- stats::predict(object = xgb_model, newdata = test_x, type = \"prob\")\nxgb_test_predictions <- ifelse(xgb_test_pred > 0.5, 1, 0)\nxgb_test_table <- table(xgb_test_predictions, y_test)\nxgb_test_accuracy[i] <- (xgb_test_table[1, 1] + xgb_test_table[2, 2]) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\nxgb_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(xgb_test_pred)))\nxgb_auc <- round((pROC::auc(c(test$y), as.numeric(c(xgb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(xgb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"XGBoost \", \"(AUC = \", xgb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# Ensemble\n\nensemble1 <- data.frame(\n  'ADABoost' = adaboost_test_predictions,\n  'BayesGLM'= bayesglm_test_predictions,\n  'C50' = C50_test_predictions,\n  'Cubist' = cubist_test_pred,\n  'Random_Forest' = rf_test_pred,\n  'XGBoost' = xgb_test_predictions,\n  'y' = test$y\n)\n\nensemble_index <- sample(c(1:2), nrow(ensemble1), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble1[ensemble_index == 1, ]\nensemble_test <- ensemble1[ensemble_index == 2, ]\nensemble_y_train <- ensemble_train$y\nensemble_y_test <- ensemble_test$y\n\n# Ensemble ADABoost\nensemble_adaboost_train_fit <- MachineShop::fit(as.factor(y) ~ ., data = ensemble_train, model = \"AdaBoostModel\")\n    \nensemble_adaboost_train_pred <- stats::predict(ensemble_adaboost_train_fit, ensemble_train, type = \"prob\")\nensemble_adaboost_train_probabilities <- ifelse(ensemble_adaboost_train_pred > 0.5, 1, 0)\nensemble_adaboost_train_table <- table(ensemble_adaboost_train_probabilities, ensemble_y_train)\nensemble_adaboost_train_accuracy[i] <- (ensemble_adaboost_train_table[1, 1] + ensemble_adaboost_train_table[2, 2]) / sum(ensemble_adaboost_train_table)\nensemble_adaboost_train_accuracy_mean <- mean(ensemble_adaboost_train_accuracy)\n    \nensemble_adaboost_test_pred <- stats::predict(ensemble_adaboost_train_fit, ensemble_test, type = \"prob\")\nensemble_adaboost_test_probabilities <- ifelse(ensemble_adaboost_test_pred > 0.5, 1, 0)\nensemble_adaboost_test_table <- table(ensemble_adaboost_test_probabilities, ensemble_y_test)\nensemble_adaboost_test_accuracy[i] <- (ensemble_adaboost_test_table[1, 1] + ensemble_adaboost_test_table[2, 2]) / sum(ensemble_adaboost_test_table)\nensemble_adaboost_test_accuracy_mean <- mean(ensemble_adaboost_test_accuracy)\n    \nensemble_adaboost_holdout_accuracy_mean <- mean(ensemble_adaboost_test_accuracy)\n    \nensemble_adaboost_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_adaboost_test_pred)))\nensemble_adaboost_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_adaboost_test_pred)) - 1)), 4)\nprint(pROC::ggroc(ensemble_adaboost_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"Ensemble Adaboostoost \", \"(AUC = \", ensemble_adaboost_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n\n# Ensembles using C50\nensemble_C50_train_fit <- C50::C5.0(as.factor(ensemble_y_train) ~ ., data = ensemble_train)\n\nensemble_C50_train_pred <- stats::predict(ensemble_C50_train_fit, ensemble_train, type = \"prob\")\nensemble_C50_train_probabilities <- ifelse(ensemble_C50_train_pred[, 2] > 0.5, 1, 0)\nensemble_C50_train_table <- table(ensemble_C50_train_probabilities, ensemble_y_train)\nensemble_C50_train_accuracy[i] <- (ensemble_C50_train_table[1, 1] + ensemble_C50_train_table[2, 2]) / sum(ensemble_C50_train_table)\nensemble_C50_train_accuracy_mean <- mean(ensemble_C50_train_accuracy)\n\nensemble_C50_test_pred <- stats::predict(ensemble_C50_train_fit, ensemble_test, type = \"prob\")\nensemble_C50_test_probabilities <- ifelse(ensemble_C50_test_pred[, 2] > 0.5, 1, 0)\nensemble_C50_test_table <- table(ensemble_C50_test_probabilities, ensemble_y_test)\nensemble_C50_test_accuracy[i] <- (ensemble_C50_test_table[1, 1] + ensemble_C50_test_table[2, 2]) / sum(ensemble_C50_test_table)\nensemble_C50_test_accuracy_mean <- mean(ensemble_C50_test_accuracy)\n\nensemble_C50_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_C50_test_pred[, 2])))\nensemble_C50_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_C50_test_pred[, 2])) - 1)), 4)\nprint(pROC::ggroc(ensemble_C50_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Ensemble_C50 \", \"(AUC = \", ensemble_C50_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n\n# Ensemble Random Forest\n\nensemble_rf_train_fit <- randomForest(x = ensemble_train, y = as.factor(ensemble_y_train), data = ensemble1)\n\nensemble_rf_train_pred <- stats::predict(ensemble_rf_train_fit, ensemble_train, type = \"prob\")\nensemble_rf_train_predictions <- ifelse(ensemble_rf_train_pred > 0.50, 1, 0)[, 2]\nensemble_rf_train_table <- table(ensemble_rf_train_predictions, ensemble_y_train)\nensemble_rf_train_accuracy[i] <- (ensemble_rf_train_table[1, 1] + ensemble_rf_train_table[2, 2]) / sum(ensemble_rf_train_table)\nensemble_rf_train_accuracy_mean <- mean(ensemble_rf_train_accuracy)\n\nensemble_rf_test_pred <- stats::predict(ensemble_rf_train_fit, ensemble_test, type = \"prob\")\nensemble_rf_test_predictions <- ifelse(ensemble_rf_test_pred > 0.50, 1, 0)[, 2]\nensemble_rf_test_table <- table(ensemble_rf_test_predictions, ensemble_y_test)\nensemble_rf_test_accuracy[i] <- (ensemble_rf_test_table[1, 1] + ensemble_rf_test_table[2, 2]) / sum(ensemble_rf_test_table)\nensemble_rf_test_accuracy_mean <- mean(ensemble_rf_test_accuracy)\n\nensemble_rf_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_rf_test_predictions)))\nensemble_rf_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_rf_test_predictions)) - 1)), 4)\nprint(pROC::ggroc(ensemble_rf_roc_obj, color = \"steelblue\", size = 2) +\n        ggplot2::ggtitle(paste0(\"Ensemble_rf \", \"(AUC = \", ensemble_rf_auc, \")\")) +\n        ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n        ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n\n# Ensemble XGBoost\n\nensemble_train_x <- data.matrix(ensemble_train[, -ncol(ensemble_train)])\nensemble_train_y <- ensemble_train[, ncol(ensemble_train)]\n\n  # define predictor and response variables in test set\nensemble_test_x <- data.matrix(ensemble_test[, -ncol(ensemble_test)])\nensemble_test_y <- ensemble_test[, ncol(ensemble_test)]\n\n# define final train and test sets\nensemble_xgb_train <- xgboost::xgb.DMatrix(data = ensemble_train_x, label = ensemble_train_y)\nensemble_xgb_test <- xgboost::xgb.DMatrix(data = ensemble_test_x, label = ensemble_test_y)\n\n# define watchlist\nensemble_watchlist <- list(train = ensemble_xgb_train)\nensemble_watchlist_test <- list(train = ensemble_xgb_train, test = ensemble_xgb_test)\n\nensemble_xgb_model <- xgboost::xgb.train(data = ensemble_xgb_train, max.depth = 3, watchlist = ensemble_watchlist_test, nrounds = 70)\n\nensemble_xgboost_min <- which.min(ensemble_xgb_model$evaluation_log$validation_rmse)\n\nensemble_xgb_train_pred <- predict(object = ensemble_xgb_model, newdata = ensemble_train_x, type = \"response\")\nensemble_xgb_train_probabilities <- ifelse(ensemble_xgb_train_pred > 0.5, 1, 0)\nensemble_xgb_train_table <- table(ensemble_xgb_train_probabilities, ensemble_y_train)\nensemble_xgb_train_accuracy[i] <- (ensemble_xgb_train_table[1, 1] + ensemble_xgb_train_table[2, 2]) / sum(ensemble_xgb_train_table)\nensemble_xgb_train_accuracy_mean <- mean(ensemble_xgb_train_accuracy)\n\nensemble_xgb_test_pred <- predict(object = ensemble_xgb_model, newdata = ensemble_test_x, type = \"response\")\nensemble_xgb_test_probabilities <- ifelse(ensemble_xgb_test_pred > 0.5, 1, 0)\nensemble_xgb_test_table <- table(ensemble_xgb_test_probabilities, ensemble_y_test)\nensemble_xgb_test_accuracy[i] <- (ensemble_xgb_test_table[1, 1] + ensemble_xgb_test_table[2, 2]) / sum(ensemble_xgb_test_table)\nensemble_xgb_test_accuracy_mean <- mean(ensemble_xgb_test_accuracy)\n\nensemble_xgb_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_xgb_test_pred)))\nensemble_xgb_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_xgb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(ensemble_xgb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Ensemble XGBoost \", \"(AUC = \", ensemble_xgb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n  \n  \n# Save all trained models to the Environment\nadaboost_train_fit <<- adaboost_train_fit\nbayesglm_train_fit <<- bayesglm_train_fit\nC50_train_fit<<- C50_train_fit\ncubist_train_fit <<- cubist_train_fit\nrf_train_fit <<- rf_train_fit\nxgb_model <<- xgb_model\nensemble_adaboost_train_fit <<- ensemble_adaboost_train_fit\nensemble_C50_train_fit <<- ensemble_C50_train_fit\nensemble_rf_train_fit <<- ensemble_rf_train_fit\nensemble_xgb_model <<- ensemble_xgb_model\n\nresults <- data.frame(\n  'Model'= c('ADABoost', 'BayesGLM', 'C50', 'Cubist', 'Random_Forest', 'XGBoost', 'Ensemble_ADABoost', 'Ensemble_C50', 'Ensemble_Random_Forest', 'Ensemble_XGBoost'),\n  'Accuracy' = c(adaboost_test_accuracy_mean, bayesglm_test_accuracy_mean, C50_test_accuracy_mean, cubist_test_accuracy_mean, rf_test_accuracy_mean, xgb_test_accuracy_mean, ensemble_adaboost_holdout_accuracy_mean, ensemble_C50_test_accuracy_mean, ensemble_rf_test_accuracy_mean, ensemble_xgb_test_accuracy_mean)\n)\n\nresults <- results %>% arrange(desc(Accuracy), Model)\n\n} # Closing loop for numresamples\nreturn(results)\n\n} # Closing loop for the function\n\nlogistic_1(data = lebron, colnum = 6, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.473260 test-rmse:0.480928 \n#> [2]  train-rmse:0.456710 test-rmse:0.470992 \n#> [3]  train-rmse:0.447663 test-rmse:0.467005 \n#> [4]  train-rmse:0.440178 test-rmse:0.465661 \n#> [5]  train-rmse:0.434177 test-rmse:0.466090 \n#> [6]  train-rmse:0.428811 test-rmse:0.467722 \n#> [7]  train-rmse:0.423727 test-rmse:0.466571 \n#> [8]  train-rmse:0.421580 test-rmse:0.466593 \n#> [9]  train-rmse:0.417858 test-rmse:0.467885 \n#> [10] train-rmse:0.413965 test-rmse:0.467015 \n#> [11] train-rmse:0.410669 test-rmse:0.466459 \n#> [12] train-rmse:0.405800 test-rmse:0.467934 \n#> [13] train-rmse:0.402774 test-rmse:0.468156 \n#> [14] train-rmse:0.399552 test-rmse:0.468364 \n#> [15] train-rmse:0.396837 test-rmse:0.469123 \n#> [16] train-rmse:0.395868 test-rmse:0.468896 \n#> [17] train-rmse:0.393769 test-rmse:0.469476 \n#> [18] train-rmse:0.392662 test-rmse:0.469459 \n#> [19] train-rmse:0.388343 test-rmse:0.471196 \n#> [20] train-rmse:0.385616 test-rmse:0.473240 \n#> [21] train-rmse:0.382860 test-rmse:0.472697 \n#> [22] train-rmse:0.379975 test-rmse:0.472335 \n#> [23] train-rmse:0.376858 test-rmse:0.472360 \n#> [24] train-rmse:0.375869 test-rmse:0.472726 \n#> [25] train-rmse:0.373040 test-rmse:0.472583 \n#> [26] train-rmse:0.371312 test-rmse:0.472935 \n#> [27] train-rmse:0.370105 test-rmse:0.473267 \n#> [28] train-rmse:0.367494 test-rmse:0.473389 \n#> [29] train-rmse:0.364031 test-rmse:0.474244 \n#> [30] train-rmse:0.363426 test-rmse:0.474258 \n#> [31] train-rmse:0.361620 test-rmse:0.475409 \n#> [32] train-rmse:0.360143 test-rmse:0.477001 \n#> [33] train-rmse:0.357859 test-rmse:0.477171 \n#> [34] train-rmse:0.355857 test-rmse:0.476392 \n#> [35] train-rmse:0.353645 test-rmse:0.476856 \n#> [36] train-rmse:0.350245 test-rmse:0.477487 \n#> [37] train-rmse:0.349426 test-rmse:0.478119 \n#> [38] train-rmse:0.346989 test-rmse:0.479051 \n#> [39] train-rmse:0.344126 test-rmse:0.478877 \n#> [40] train-rmse:0.342363 test-rmse:0.479401 \n#> [41] train-rmse:0.340939 test-rmse:0.480583 \n#> [42] train-rmse:0.339069 test-rmse:0.481435 \n#> [43] train-rmse:0.338761 test-rmse:0.481577 \n#> [44] train-rmse:0.336075 test-rmse:0.482866 \n#> [45] train-rmse:0.334078 test-rmse:0.485063 \n#> [46] train-rmse:0.332140 test-rmse:0.485620 \n#> [47] train-rmse:0.329294 test-rmse:0.486408 \n#> [48] train-rmse:0.326892 test-rmse:0.487116 \n#> [49] train-rmse:0.325316 test-rmse:0.487400 \n#> [50] train-rmse:0.322989 test-rmse:0.487219 \n#> [51] train-rmse:0.322548 test-rmse:0.487497 \n#> [52] train-rmse:0.320879 test-rmse:0.486324 \n#> [53] train-rmse:0.319337 test-rmse:0.485726 \n#> [54] train-rmse:0.317468 test-rmse:0.485902 \n#> [55] train-rmse:0.316826 test-rmse:0.485982 \n#> [56] train-rmse:0.314655 test-rmse:0.486821 \n#> [57] train-rmse:0.312849 test-rmse:0.487355 \n#> [58] train-rmse:0.312045 test-rmse:0.487324 \n#> [59] train-rmse:0.310744 test-rmse:0.487221 \n#> [60] train-rmse:0.308556 test-rmse:0.488889 \n#> [61] train-rmse:0.306704 test-rmse:0.488557 \n#> [62] train-rmse:0.305717 test-rmse:0.488832 \n#> [63] train-rmse:0.302922 test-rmse:0.489763 \n#> [64] train-rmse:0.300995 test-rmse:0.490850 \n#> [65] train-rmse:0.299050 test-rmse:0.491371 \n#> [66] train-rmse:0.297149 test-rmse:0.492290 \n#> [67] train-rmse:0.295700 test-rmse:0.492574 \n#> [68] train-rmse:0.295242 test-rmse:0.492833 \n#> [69] train-rmse:0.294881 test-rmse:0.492807 \n#> [70] train-rmse:0.293163 test-rmse:0.493658\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350802 test-rmse:0.350804 \n#> [2]  train-rmse:0.246124 test-rmse:0.246126 \n#> [3]  train-rmse:0.172682 test-rmse:0.172684 \n#> [4]  train-rmse:0.121154 test-rmse:0.121156 \n#> [5]  train-rmse:0.085002 test-rmse:0.085004 \n#> [6]  train-rmse:0.059638 test-rmse:0.059640 \n#> [7]  train-rmse:0.041842 test-rmse:0.041844 \n#> [8]  train-rmse:0.029357 test-rmse:0.029358 \n#> [9]  train-rmse:0.020597 test-rmse:0.020598 \n#> [10] train-rmse:0.014451 test-rmse:0.014451 \n#> [11] train-rmse:0.010139 test-rmse:0.010139 \n#> [12] train-rmse:0.007113 test-rmse:0.007114 \n#> [13] train-rmse:0.004991 test-rmse:0.004991 \n#> [14] train-rmse:0.003502 test-rmse:0.003502 \n#> [15] train-rmse:0.002457 test-rmse:0.002457 \n#> [16] train-rmse:0.001724 test-rmse:0.001724 \n#> [17] train-rmse:0.001209 test-rmse:0.001209 \n#> [18] train-rmse:0.000848 test-rmse:0.000849 \n#> [19] train-rmse:0.000595 test-rmse:0.000595 \n#> [20] train-rmse:0.000418 test-rmse:0.000418 \n#> [21] train-rmse:0.000293 test-rmse:0.000293 \n#> [22] train-rmse:0.000206 test-rmse:0.000206 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.475826 test-rmse:0.481500 \n#> [2]  train-rmse:0.459657 test-rmse:0.472443 \n#> [3]  train-rmse:0.450397 test-rmse:0.465385 \n#> [4]  train-rmse:0.442450 test-rmse:0.462061 \n#> [5]  train-rmse:0.434957 test-rmse:0.462729 \n#> [6]  train-rmse:0.429407 test-rmse:0.461732 \n#> [7]  train-rmse:0.424977 test-rmse:0.462985 \n#> [8]  train-rmse:0.420224 test-rmse:0.462965 \n#> [9]  train-rmse:0.415484 test-rmse:0.462735 \n#> [10] train-rmse:0.413699 test-rmse:0.462835 \n#> [11] train-rmse:0.408541 test-rmse:0.464830 \n#> [12] train-rmse:0.405177 test-rmse:0.462881 \n#> [13] train-rmse:0.403092 test-rmse:0.464161 \n#> [14] train-rmse:0.400489 test-rmse:0.465113 \n#> [15] train-rmse:0.399295 test-rmse:0.465720 \n#> [16] train-rmse:0.396391 test-rmse:0.467182 \n#> [17] train-rmse:0.393456 test-rmse:0.469140 \n#> [18] train-rmse:0.390611 test-rmse:0.468779 \n#> [19] train-rmse:0.388942 test-rmse:0.468704 \n#> [20] train-rmse:0.384047 test-rmse:0.470124 \n#> [21] train-rmse:0.381096 test-rmse:0.470402 \n#> [22] train-rmse:0.380055 test-rmse:0.470720 \n#> [23] train-rmse:0.377648 test-rmse:0.470391 \n#> [24] train-rmse:0.374757 test-rmse:0.470524 \n#> [25] train-rmse:0.372815 test-rmse:0.470675 \n#> [26] train-rmse:0.372197 test-rmse:0.471392 \n#> [27] train-rmse:0.370336 test-rmse:0.472008 \n#> [28] train-rmse:0.369157 test-rmse:0.472623 \n#> [29] train-rmse:0.367782 test-rmse:0.473329 \n#> [30] train-rmse:0.365917 test-rmse:0.473106 \n#> [31] train-rmse:0.363801 test-rmse:0.474664 \n#> [32] train-rmse:0.361328 test-rmse:0.474565 \n#> [33] train-rmse:0.359178 test-rmse:0.474916 \n#> [34] train-rmse:0.357302 test-rmse:0.475243 \n#> [35] train-rmse:0.354064 test-rmse:0.476413 \n#> [36] train-rmse:0.351931 test-rmse:0.477776 \n#> [37] train-rmse:0.350072 test-rmse:0.478116 \n#> [38] train-rmse:0.349207 test-rmse:0.478193 \n#> [39] train-rmse:0.346550 test-rmse:0.479690 \n#> [40] train-rmse:0.343628 test-rmse:0.478700 \n#> [41] train-rmse:0.343019 test-rmse:0.478809 \n#> [42] train-rmse:0.340757 test-rmse:0.480682 \n#> [43] train-rmse:0.338739 test-rmse:0.481500 \n#> [44] train-rmse:0.337990 test-rmse:0.481779 \n#> [45] train-rmse:0.335377 test-rmse:0.482366 \n#> [46] train-rmse:0.334332 test-rmse:0.482275 \n#> [47] train-rmse:0.332337 test-rmse:0.484199 \n#> [48] train-rmse:0.331546 test-rmse:0.484268 \n#> [49] train-rmse:0.330369 test-rmse:0.484690 \n#> [50] train-rmse:0.329295 test-rmse:0.484644 \n#> [51] train-rmse:0.326743 test-rmse:0.486167 \n#> [52] train-rmse:0.324712 test-rmse:0.486878 \n#> [53] train-rmse:0.322148 test-rmse:0.487101 \n#> [54] train-rmse:0.320578 test-rmse:0.486838 \n#> [55] train-rmse:0.319698 test-rmse:0.487108 \n#> [56] train-rmse:0.318283 test-rmse:0.487043 \n#> [57] train-rmse:0.315816 test-rmse:0.486793 \n#> [58] train-rmse:0.314033 test-rmse:0.487445 \n#> [59] train-rmse:0.312157 test-rmse:0.487611 \n#> [60] train-rmse:0.311287 test-rmse:0.487606 \n#> [61] train-rmse:0.310028 test-rmse:0.487976 \n#> [62] train-rmse:0.308745 test-rmse:0.489266 \n#> [63] train-rmse:0.307336 test-rmse:0.489580 \n#> [64] train-rmse:0.306699 test-rmse:0.490017 \n#> [65] train-rmse:0.306391 test-rmse:0.490565 \n#> [66] train-rmse:0.305921 test-rmse:0.490665 \n#> [67] train-rmse:0.303994 test-rmse:0.490936 \n#> [68] train-rmse:0.302247 test-rmse:0.490894 \n#> [69] train-rmse:0.301744 test-rmse:0.491388 \n#> [70] train-rmse:0.299744 test-rmse:0.492478\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350798 test-rmse:0.350798 \n#> [2]  train-rmse:0.246118 test-rmse:0.246119 \n#> [3]  train-rmse:0.172676 test-rmse:0.172676 \n#> [4]  train-rmse:0.121148 test-rmse:0.121149 \n#> [5]  train-rmse:0.084997 test-rmse:0.084998 \n#> [6]  train-rmse:0.059634 test-rmse:0.059634 \n#> [7]  train-rmse:0.041839 test-rmse:0.041839 \n#> [8]  train-rmse:0.029354 test-rmse:0.029354 \n#> [9]  train-rmse:0.020595 test-rmse:0.020595 \n#> [10] train-rmse:0.014449 test-rmse:0.014449 \n#> [11] train-rmse:0.010137 test-rmse:0.010138 \n#> [12] train-rmse:0.007112 test-rmse:0.007112 \n#> [13] train-rmse:0.004990 test-rmse:0.004990 \n#> [14] train-rmse:0.003501 test-rmse:0.003501 \n#> [15] train-rmse:0.002456 test-rmse:0.002456 \n#> [16] train-rmse:0.001723 test-rmse:0.001723 \n#> [17] train-rmse:0.001209 test-rmse:0.001209 \n#> [18] train-rmse:0.000848 test-rmse:0.000848 \n#> [19] train-rmse:0.000595 test-rmse:0.000595 \n#> [20] train-rmse:0.000418 test-rmse:0.000418 \n#> [21] train-rmse:0.000293 test-rmse:0.000293 \n#> [22] train-rmse:0.000206 test-rmse:0.000206 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.472769 test-rmse:0.482654 \n#> [2]  train-rmse:0.455927 test-rmse:0.474974 \n#> [3]  train-rmse:0.445050 test-rmse:0.471813 \n#> [4]  train-rmse:0.437652 test-rmse:0.470455 \n#> [5]  train-rmse:0.432534 test-rmse:0.472091 \n#> [6]  train-rmse:0.427086 test-rmse:0.471085 \n#> [7]  train-rmse:0.424693 test-rmse:0.470329 \n#> [8]  train-rmse:0.423074 test-rmse:0.469975 \n#> [9]  train-rmse:0.419267 test-rmse:0.468250 \n#> [10] train-rmse:0.416963 test-rmse:0.468207 \n#> [11] train-rmse:0.413444 test-rmse:0.467577 \n#> [12] train-rmse:0.410447 test-rmse:0.468505 \n#> [13] train-rmse:0.406754 test-rmse:0.468206 \n#> [14] train-rmse:0.403631 test-rmse:0.469170 \n#> [15] train-rmse:0.402640 test-rmse:0.469300 \n#> [16] train-rmse:0.401049 test-rmse:0.469462 \n#> [17] train-rmse:0.397761 test-rmse:0.469707 \n#> [18] train-rmse:0.395251 test-rmse:0.469630 \n#> [19] train-rmse:0.392317 test-rmse:0.468573 \n#> [20] train-rmse:0.389907 test-rmse:0.468384 \n#> [21] train-rmse:0.386263 test-rmse:0.467207 \n#> [22] train-rmse:0.385092 test-rmse:0.466582 \n#> [23] train-rmse:0.382947 test-rmse:0.468251 \n#> [24] train-rmse:0.379480 test-rmse:0.469186 \n#> [25] train-rmse:0.377447 test-rmse:0.469316 \n#> [26] train-rmse:0.374702 test-rmse:0.469646 \n#> [27] train-rmse:0.371630 test-rmse:0.469389 \n#> [28] train-rmse:0.369563 test-rmse:0.469242 \n#> [29] train-rmse:0.367447 test-rmse:0.469919 \n#> [30] train-rmse:0.364762 test-rmse:0.468792 \n#> [31] train-rmse:0.362708 test-rmse:0.469501 \n#> [32] train-rmse:0.359730 test-rmse:0.469661 \n#> [33] train-rmse:0.357905 test-rmse:0.469505 \n#> [34] train-rmse:0.356490 test-rmse:0.470354 \n#> [35] train-rmse:0.355746 test-rmse:0.470424 \n#> [36] train-rmse:0.353715 test-rmse:0.470998 \n#> [37] train-rmse:0.352081 test-rmse:0.471469 \n#> [38] train-rmse:0.348582 test-rmse:0.472291 \n#> [39] train-rmse:0.347687 test-rmse:0.472313 \n#> [40] train-rmse:0.345912 test-rmse:0.472007 \n#> [41] train-rmse:0.345686 test-rmse:0.471909 \n#> [42] train-rmse:0.343115 test-rmse:0.470923 \n#> [43] train-rmse:0.341945 test-rmse:0.470430 \n#> [44] train-rmse:0.340025 test-rmse:0.470453 \n#> [45] train-rmse:0.338328 test-rmse:0.470725 \n#> [46] train-rmse:0.337134 test-rmse:0.471166 \n#> [47] train-rmse:0.335030 test-rmse:0.471723 \n#> [48] train-rmse:0.333215 test-rmse:0.472744 \n#> [49] train-rmse:0.332176 test-rmse:0.472636 \n#> [50] train-rmse:0.330275 test-rmse:0.472985 \n#> [51] train-rmse:0.328481 test-rmse:0.473944 \n#> [52] train-rmse:0.328104 test-rmse:0.474638 \n#> [53] train-rmse:0.326592 test-rmse:0.474324 \n#> [54] train-rmse:0.325568 test-rmse:0.474741 \n#> [55] train-rmse:0.323967 test-rmse:0.474859 \n#> [56] train-rmse:0.322216 test-rmse:0.475386 \n#> [57] train-rmse:0.320249 test-rmse:0.477011 \n#> [58] train-rmse:0.318805 test-rmse:0.477852 \n#> [59] train-rmse:0.318007 test-rmse:0.478533 \n#> [60] train-rmse:0.314959 test-rmse:0.478792 \n#> [61] train-rmse:0.313552 test-rmse:0.478426 \n#> [62] train-rmse:0.311992 test-rmse:0.477958 \n#> [63] train-rmse:0.309770 test-rmse:0.477936 \n#> [64] train-rmse:0.308589 test-rmse:0.477971 \n#> [65] train-rmse:0.307905 test-rmse:0.478684 \n#> [66] train-rmse:0.305894 test-rmse:0.479162 \n#> [67] train-rmse:0.304314 test-rmse:0.479473 \n#> [68] train-rmse:0.302703 test-rmse:0.479354 \n#> [69] train-rmse:0.301543 test-rmse:0.479396 \n#> [70] train-rmse:0.301132 test-rmse:0.479597\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350787 test-rmse:0.350788 \n#> [2]  train-rmse:0.246104 test-rmse:0.246105 \n#> [3]  train-rmse:0.172660 test-rmse:0.172661 \n#> [4]  train-rmse:0.121134 test-rmse:0.121135 \n#> [5]  train-rmse:0.084985 test-rmse:0.084986 \n#> [6]  train-rmse:0.059623 test-rmse:0.059624 \n#> [7]  train-rmse:0.041830 test-rmse:0.041831 \n#> [8]  train-rmse:0.029347 test-rmse:0.029347 \n#> [9]  train-rmse:0.020589 test-rmse:0.020590 \n#> [10] train-rmse:0.014445 test-rmse:0.014445 \n#> [11] train-rmse:0.010134 test-rmse:0.010134 \n#> [12] train-rmse:0.007110 test-rmse:0.007110 \n#> [13] train-rmse:0.004988 test-rmse:0.004988 \n#> [14] train-rmse:0.003500 test-rmse:0.003500 \n#> [15] train-rmse:0.002455 test-rmse:0.002455 \n#> [16] train-rmse:0.001722 test-rmse:0.001723 \n#> [17] train-rmse:0.001208 test-rmse:0.001209 \n#> [18] train-rmse:0.000848 test-rmse:0.000848 \n#> [19] train-rmse:0.000595 test-rmse:0.000595 \n#> [20] train-rmse:0.000417 test-rmse:0.000417 \n#> [21] train-rmse:0.000293 test-rmse:0.000293 \n#> [22] train-rmse:0.000205 test-rmse:0.000205 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.475532 test-rmse:0.479722 \n#> [2]  train-rmse:0.461715 test-rmse:0.469507 \n#> [3]  train-rmse:0.450855 test-rmse:0.461037 \n#> [4]  train-rmse:0.443802 test-rmse:0.459167 \n#> [5]  train-rmse:0.438438 test-rmse:0.457998 \n#> [6]  train-rmse:0.434583 test-rmse:0.458561 \n#> [7]  train-rmse:0.431620 test-rmse:0.455979 \n#> [8]  train-rmse:0.428429 test-rmse:0.455833 \n#> [9]  train-rmse:0.426257 test-rmse:0.456038 \n#> [10] train-rmse:0.422599 test-rmse:0.456239 \n#> [11] train-rmse:0.418804 test-rmse:0.455760 \n#> [12] train-rmse:0.417226 test-rmse:0.455697 \n#> [13] train-rmse:0.414559 test-rmse:0.456316 \n#> [14] train-rmse:0.411412 test-rmse:0.455462 \n#> [15] train-rmse:0.408283 test-rmse:0.455988 \n#> [16] train-rmse:0.407567 test-rmse:0.455761 \n#> [17] train-rmse:0.405276 test-rmse:0.456471 \n#> [18] train-rmse:0.403403 test-rmse:0.457584 \n#> [19] train-rmse:0.400764 test-rmse:0.457745 \n#> [20] train-rmse:0.399792 test-rmse:0.458031 \n#> [21] train-rmse:0.396750 test-rmse:0.458245 \n#> [22] train-rmse:0.393334 test-rmse:0.459049 \n#> [23] train-rmse:0.391641 test-rmse:0.460172 \n#> [24] train-rmse:0.390145 test-rmse:0.459686 \n#> [25] train-rmse:0.386848 test-rmse:0.461748 \n#> [26] train-rmse:0.383815 test-rmse:0.462019 \n#> [27] train-rmse:0.382860 test-rmse:0.461958 \n#> [28] train-rmse:0.380752 test-rmse:0.462361 \n#> [29] train-rmse:0.379025 test-rmse:0.463454 \n#> [30] train-rmse:0.376613 test-rmse:0.463279 \n#> [31] train-rmse:0.375179 test-rmse:0.462394 \n#> [32] train-rmse:0.372078 test-rmse:0.462297 \n#> [33] train-rmse:0.370390 test-rmse:0.461997 \n#> [34] train-rmse:0.367370 test-rmse:0.461258 \n#> [35] train-rmse:0.365171 test-rmse:0.461642 \n#> [36] train-rmse:0.364556 test-rmse:0.462097 \n#> [37] train-rmse:0.362250 test-rmse:0.463210 \n#> [38] train-rmse:0.360822 test-rmse:0.463464 \n#> [39] train-rmse:0.359736 test-rmse:0.464817 \n#> [40] train-rmse:0.357757 test-rmse:0.465533 \n#> [41] train-rmse:0.355711 test-rmse:0.464467 \n#> [42] train-rmse:0.353179 test-rmse:0.464570 \n#> [43] train-rmse:0.350284 test-rmse:0.465779 \n#> [44] train-rmse:0.348136 test-rmse:0.466365 \n#> [45] train-rmse:0.346818 test-rmse:0.466977 \n#> [46] train-rmse:0.344784 test-rmse:0.466445 \n#> [47] train-rmse:0.343106 test-rmse:0.466392 \n#> [48] train-rmse:0.340745 test-rmse:0.466662 \n#> [49] train-rmse:0.338533 test-rmse:0.466695 \n#> [50] train-rmse:0.337674 test-rmse:0.466918 \n#> [51] train-rmse:0.335752 test-rmse:0.467161 \n#> [52] train-rmse:0.333461 test-rmse:0.467704 \n#> [53] train-rmse:0.331401 test-rmse:0.467842 \n#> [54] train-rmse:0.329978 test-rmse:0.468242 \n#> [55] train-rmse:0.328074 test-rmse:0.469040 \n#> [56] train-rmse:0.327299 test-rmse:0.469125 \n#> [57] train-rmse:0.326539 test-rmse:0.469162 \n#> [58] train-rmse:0.325743 test-rmse:0.469178 \n#> [59] train-rmse:0.323197 test-rmse:0.468921 \n#> [60] train-rmse:0.321051 test-rmse:0.469180 \n#> [61] train-rmse:0.318822 test-rmse:0.468901 \n#> [62] train-rmse:0.317071 test-rmse:0.468493 \n#> [63] train-rmse:0.315341 test-rmse:0.469553 \n#> [64] train-rmse:0.313901 test-rmse:0.470131 \n#> [65] train-rmse:0.312277 test-rmse:0.470533 \n#> [66] train-rmse:0.310928 test-rmse:0.470303 \n#> [67] train-rmse:0.309997 test-rmse:0.470076 \n#> [68] train-rmse:0.309665 test-rmse:0.470048 \n#> [69] train-rmse:0.308328 test-rmse:0.470354 \n#> [70] train-rmse:0.306956 test-rmse:0.470402\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350813 test-rmse:0.350813 \n#> [2]  train-rmse:0.246140 test-rmse:0.246140 \n#> [3]  train-rmse:0.172698 test-rmse:0.172698 \n#> [4]  train-rmse:0.121169 test-rmse:0.121170 \n#> [5]  train-rmse:0.085016 test-rmse:0.085016 \n#> [6]  train-rmse:0.059649 test-rmse:0.059649 \n#> [7]  train-rmse:0.041851 test-rmse:0.041851 \n#> [8]  train-rmse:0.029364 test-rmse:0.029364 \n#> [9]  train-rmse:0.020603 test-rmse:0.020603 \n#> [10] train-rmse:0.014455 test-rmse:0.014455 \n#> [11] train-rmse:0.010142 test-rmse:0.010142 \n#> [12] train-rmse:0.007116 test-rmse:0.007116 \n#> [13] train-rmse:0.004993 test-rmse:0.004993 \n#> [14] train-rmse:0.003503 test-rmse:0.003503 \n#> [15] train-rmse:0.002458 test-rmse:0.002458 \n#> [16] train-rmse:0.001724 test-rmse:0.001724 \n#> [17] train-rmse:0.001210 test-rmse:0.001210 \n#> [18] train-rmse:0.000849 test-rmse:0.000849 \n#> [19] train-rmse:0.000596 test-rmse:0.000596 \n#> [20] train-rmse:0.000418 test-rmse:0.000418 \n#> [21] train-rmse:0.000293 test-rmse:0.000293 \n#> [22] train-rmse:0.000206 test-rmse:0.000206 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.473850 test-rmse:0.479414 \n#> [2]  train-rmse:0.459094 test-rmse:0.469794 \n#> [3]  train-rmse:0.449354 test-rmse:0.461877 \n#> [4]  train-rmse:0.443149 test-rmse:0.459174 \n#> [5]  train-rmse:0.437627 test-rmse:0.459811 \n#> [6]  train-rmse:0.432413 test-rmse:0.460721 \n#> [7]  train-rmse:0.428558 test-rmse:0.462380 \n#> [8]  train-rmse:0.425258 test-rmse:0.462570 \n#> [9]  train-rmse:0.423077 test-rmse:0.461556 \n#> [10] train-rmse:0.418896 test-rmse:0.462989 \n#> [11] train-rmse:0.416189 test-rmse:0.464328 \n#> [12] train-rmse:0.414683 test-rmse:0.463995 \n#> [13] train-rmse:0.411538 test-rmse:0.462447 \n#> [14] train-rmse:0.410403 test-rmse:0.462092 \n#> [15] train-rmse:0.407792 test-rmse:0.463728 \n#> [16] train-rmse:0.404779 test-rmse:0.464408 \n#> [17] train-rmse:0.402084 test-rmse:0.464279 \n#> [18] train-rmse:0.398376 test-rmse:0.464539 \n#> [19] train-rmse:0.395787 test-rmse:0.464975 \n#> [20] train-rmse:0.393919 test-rmse:0.466032 \n#> [21] train-rmse:0.391891 test-rmse:0.465225 \n#> [22] train-rmse:0.390346 test-rmse:0.464729 \n#> [23] train-rmse:0.387206 test-rmse:0.465898 \n#> [24] train-rmse:0.384101 test-rmse:0.465737 \n#> [25] train-rmse:0.383148 test-rmse:0.465977 \n#> [26] train-rmse:0.382018 test-rmse:0.465669 \n#> [27] train-rmse:0.378659 test-rmse:0.467162 \n#> [28] train-rmse:0.376657 test-rmse:0.469619 \n#> [29] train-rmse:0.373677 test-rmse:0.468774 \n#> [30] train-rmse:0.371718 test-rmse:0.469019 \n#> [31] train-rmse:0.368742 test-rmse:0.469019 \n#> [32] train-rmse:0.365627 test-rmse:0.470020 \n#> [33] train-rmse:0.363830 test-rmse:0.472418 \n#> [34] train-rmse:0.360158 test-rmse:0.473165 \n#> [35] train-rmse:0.357578 test-rmse:0.472734 \n#> [36] train-rmse:0.356505 test-rmse:0.473035 \n#> [37] train-rmse:0.353059 test-rmse:0.472369 \n#> [38] train-rmse:0.351706 test-rmse:0.472498 \n#> [39] train-rmse:0.351316 test-rmse:0.472638 \n#> [40] train-rmse:0.350547 test-rmse:0.473132 \n#> [41] train-rmse:0.347650 test-rmse:0.473525 \n#> [42] train-rmse:0.345427 test-rmse:0.475680 \n#> [43] train-rmse:0.343986 test-rmse:0.476514 \n#> [44] train-rmse:0.342900 test-rmse:0.477205 \n#> [45] train-rmse:0.341841 test-rmse:0.478580 \n#> [46] train-rmse:0.340976 test-rmse:0.478855 \n#> [47] train-rmse:0.338885 test-rmse:0.478731 \n#> [48] train-rmse:0.336912 test-rmse:0.478227 \n#> [49] train-rmse:0.333370 test-rmse:0.479379 \n#> [50] train-rmse:0.331511 test-rmse:0.480517 \n#> [51] train-rmse:0.328791 test-rmse:0.480654 \n#> [52] train-rmse:0.327809 test-rmse:0.480366 \n#> [53] train-rmse:0.325429 test-rmse:0.481282 \n#> [54] train-rmse:0.324589 test-rmse:0.481796 \n#> [55] train-rmse:0.323163 test-rmse:0.482088 \n#> [56] train-rmse:0.322723 test-rmse:0.482131 \n#> [57] train-rmse:0.320544 test-rmse:0.482739 \n#> [58] train-rmse:0.318653 test-rmse:0.484235 \n#> [59] train-rmse:0.316577 test-rmse:0.484282 \n#> [60] train-rmse:0.315825 test-rmse:0.484865 \n#> [61] train-rmse:0.315498 test-rmse:0.484841 \n#> [62] train-rmse:0.313831 test-rmse:0.485849 \n#> [63] train-rmse:0.313020 test-rmse:0.486368 \n#> [64] train-rmse:0.311073 test-rmse:0.487467 \n#> [65] train-rmse:0.309329 test-rmse:0.488293 \n#> [66] train-rmse:0.308715 test-rmse:0.488110 \n#> [67] train-rmse:0.307195 test-rmse:0.487954 \n#> [68] train-rmse:0.304213 test-rmse:0.488320 \n#> [69] train-rmse:0.301896 test-rmse:0.487969 \n#> [70] train-rmse:0.299934 test-rmse:0.489176\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350824 test-rmse:0.350826 \n#> [2]  train-rmse:0.246155 test-rmse:0.246158 \n#> [3]  train-rmse:0.172714 test-rmse:0.172718 \n#> [4]  train-rmse:0.121185 test-rmse:0.121188 \n#> [5]  train-rmse:0.085029 test-rmse:0.085032 \n#> [6]  train-rmse:0.059660 test-rmse:0.059663 \n#> [7]  train-rmse:0.041861 test-rmse:0.041863 \n#> [8]  train-rmse:0.029371 test-rmse:0.029373 \n#> [9]  train-rmse:0.020608 test-rmse:0.020610 \n#> [10] train-rmse:0.014460 test-rmse:0.014461 \n#> [11] train-rmse:0.010146 test-rmse:0.010146 \n#> [12] train-rmse:0.007119 test-rmse:0.007119 \n#> [13] train-rmse:0.004995 test-rmse:0.004995 \n#> [14] train-rmse:0.003505 test-rmse:0.003505 \n#> [15] train-rmse:0.002459 test-rmse:0.002459 \n#> [16] train-rmse:0.001725 test-rmse:0.001726 \n#> [17] train-rmse:0.001211 test-rmse:0.001211 \n#> [18] train-rmse:0.000849 test-rmse:0.000850 \n#> [19] train-rmse:0.000596 test-rmse:0.000596 \n#> [20] train-rmse:0.000418 test-rmse:0.000418 \n#> [21] train-rmse:0.000293 test-rmse:0.000293 \n#> [22] train-rmse:0.000206 test-rmse:0.000206 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#>                     Model  Accuracy\n#> 1                     C50 1.0000000\n#> 2                  Cubist 1.0000000\n#> 3       Ensemble_ADABoost 1.0000000\n#> 4            Ensemble_C50 1.0000000\n#> 5  Ensemble_Random_Forest 1.0000000\n#> 6        Ensemble_XGBoost 1.0000000\n#> 7           Random_Forest 1.0000000\n#> 8                BayesGLM 0.6479583\n#> 9                 XGBoost 0.6437120\n#> 10               ADABoost 0.6191110\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"how-to-make-27-individual-forecasting-models","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9 How to Make 27 Individual Forecasting Models","text":"chapter builds time series models. also known forecasting, professional organization named International Institute Forecasters, website https://forecasters.org. strongly recommend checking IIF, ’ve found good source skills knowledge comes forecasting.chapter going build 16 forecasting models. large groups models, variations within groups. example, use (use) seasonality model making process.’ll follow pattern/process ’ve following previous sections:Load librarySet initial values 0Create functionBreak data train test setsSet random resamplingFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setThe first step load library case time series forecasting, library excellent FPP3 library. excellent book available guides learner time series process. book Forecasting Principles Practice. currently third edition, recommend highly. website book :https://otexts.com/fpp3/time series data use important data published regular bases United States federal government: monthly labor report. large set time series data sets Bureau Labor Statistics website:https://www.bls.govThe top picks time series data :https://data.bls.gov/cgi-bin/surveymost?ceFor work looking one data set, ’s far watched result: Total nonfarm employment. data can found :https://data.bls.gov/timeseries/CES0000000001I data stored Github repository, w accessing data, ways data may retrieved. plan use lot, consider registering Application Program Interface (API) time series data. information API directions register available :https://www.bls.gov/developers/","code":""},{"path":"how-to-make-27-individual-forecasting-models.html","id":"individual-time-series-models","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1 12 Individual Time Series Models","text":"","code":""},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima-1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.1 Arima 1","text":"","code":"\n\nlibrary(fpp3)\n#> ── Attaching packages ────────────────────────── fpp3 0.5 ──\n#> ✔ tibble      3.2.1     ✔ tsibble     1.1.4\n#> ✔ dplyr       1.1.4     ✔ tsibbledata 0.4.1\n#> ✔ tidyr       1.3.1     ✔ feasts      0.3.2\n#> ✔ lubridate   1.9.3     ✔ fable       0.3.4\n#> ✔ ggplot2     3.5.1     ✔ fabletools  0.4.2\n#> ── Conflicts ───────────────────────────── fpp3_conflicts ──\n#> ✖ lubridate::date()    masks base::date()\n#> ✖ dplyr::filter()      masks stats::filter()\n#> ✖ tsibble::intersect() masks base::intersect()\n#> ✖ tsibble::interval()  masks lubridate::interval()\n#> ✖ dplyr::lag()         masks stats::lag()\n#> ✖ tsibble::setdiff()   masks base::setdiff()\n#> ✖ tsibble::union()     masks base::union()\n\n# Set initial values to 0\n\n# Set up function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\n# Build the model\nArima1_model = fable::ARIMA(Difference ~ season() + trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n\n# Calculate error rate\nArima1_test_error <- time_series_train %>%\n    fabletools::model(Arima1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n\n# Make predictions on the holdout/test data\nArima1_predictions <- time_series_test %>%\n    fabletools::model(\n      Arima1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n\n# Report the predictions\nArima1_prediction_model <- Arima1_predictions[1]\nArima1_prediction_date<- Arima1_predictions[2]\nArima1_prediction_range <- Arima1_predictions[3]\nArima1_prediction_mean <-Arima1_predictions[4]\n\nresults <- data.frame(\n  'Model' = Arima1_predictions[1],\n  'Error' = Arima1_test_error$RMSE,\n  'Date' = Arima1_predictions[2],\n  'Forecast' = Arima1_predictions[4]\n)\n\nreturn(results)\n\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date     .mean\n#> 1 Arima1_model 58.39161 2024 May  740.0623\n#> 2 Arima1_model 58.39161 2024 Jun 1029.3480\n#> 3 Arima1_model 58.39161 2024 Jul  586.4908\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima-2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.2 Arima 2","text":"","code":"\n\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\nArima2_model <- fable::ARIMA(Difference ~ season(), stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n\n# Calculate error rate\nArima2_test_error <- time_series_train %>%\n  fabletools::model(Arima2_model) %>%\n  fabletools::forecast(h = number) %>%\n  fabletools::accuracy(time_series_test)\n\n# Make predictions on the holdout/test data\nArima2_predictions <- time_series_test %>%\n  fabletools::model(\n    Arima2_model,\n  ) %>%\n  fabletools::forecast(h = number)\n\n# Report the predictions\nArima2_prediction_model <- Arima2_predictions[1]\nArima2_prediction_date<- Arima2_predictions[2]\nArima2_prediction_range <- Arima2_predictions[3]\nArima2_prediction_mean <-Arima2_predictions[4]\n\nresults <- data.frame(\n  'Model' = Arima2_predictions[1],\n  'Error' = Arima2_test_error$RMSE,\n  'Date' = Arima2_predictions[2],\n  'Forecast' = Arima2_predictions[4]\n)\n\nreturn(results)\n\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date    .mean\n#> 1 Arima2_model 54.98322 2024 May 623.5714\n#> 2 Arima2_model 54.98322 2024 Jun 912.8571\n#> 3 Arima2_model 54.98322 2024 Jul 470.0000\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.3 Arima3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\n# Create the model:\nArima3_model <- fable::ARIMA(Difference ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n\n# Calculate the error:\nArima3_test_error <- time_series_train %>%\n    fabletools::model(Arima3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n\n# Calculate the forecast:\nArima3_predictions <- time_series_test %>%\n    fabletools::model(\n      Arima3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n\n# Report the predictions:\nresults <- data.frame(\n  'Model' = Arima3_predictions[1],\n  'Error' = Arima3_test_error$RMSE,\n  'Date' = Arima3_predictions[2],\n  'Forecast' = Arima3_predictions[4]\n)\n\nreturn(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date    .mean\n#> 1 Arima3_model 46.96308 2024 May 196.6640\n#> 2 Arima3_model 46.96308 2024 Jun 197.5579\n#> 3 Arima3_model 46.96308 2024 Jul 198.4518\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.4 Arima4","text":"","code":"\n\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nArima4_model <- fable::ARIMA(Difference ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n  \n  # Calculate the error:\n  Arima4_test_error <- time_series_train %>%\n    fabletools::model(Arima4_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Arima4_predictions <- time_series_test %>%\n    fabletools::model(\n      Arima4_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Arima4_predictions[1],\n    'Error' = Arima4_test_error$RMSE,\n    'Date' = Arima4_predictions[2],\n    'Forecast' = Arima4_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date    .mean\n#> 1 Arima4_model 46.96308 2024 May 196.6640\n#> 2 Arima4_model 46.96308 2024 Jun 197.5579\n#> 3 Arima4_model 46.96308 2024 Jul 198.4518\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"deterministic","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.5 Deterministic","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\nDeterministic_model <- fable::ARIMA(Difference ~  1 + pdq(d = 0))\n  \n# Calculate the error:\nDeterministic_test_error <- time_series_train %>%\n    fabletools::model(Deterministic_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nDeterministic_predictions <- time_series_test %>%\n    fabletools::model(\n      Deterministic_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Deterministic_predictions[1],\n    'Error' = Deterministic_test_error$RMSE,\n    'Date' = Deterministic_predictions[2],\n    'Forecast' = Deterministic_predictions[4]\n  )\n  \nreturn(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                .model    Error     Date    .mean\n#> 1 Deterministic_model 42.86143 2024 May 146.3409\n#> 2 Deterministic_model 42.86143 2024 Jun 146.3409\n#> 3 Deterministic_model 42.86143 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"drift","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.6 Drift","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nDrift_model <- fable::SNAIVE(Difference ~ drift())\n\n# Calculate the error:\nDrift_test_error <- time_series_train %>%\n    fabletools::model(Drift_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nDrift_predictions <- time_series_test %>%\n    fabletools::model(\n      Drift_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Drift_predictions[1],\n    'Error' = Drift_test_error$RMSE,\n    'Date' = Drift_predictions[2],\n    'Range' = Drift_predictions[3],\n    'Value' = Drift_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>        .model    Error     Date            Difference\n#> 1 Drift_model 99.09185 2024 May N(287.3684, 12520161)\n#> 2 Drift_model 99.09185 2024 Jun N(191.3684, 12520161)\n#> 3 Drift_model 99.09185 2024 Jul N(193.3684, 12520161)\n#>      .mean\n#> 1 287.3684\n#> 2 191.3684\n#> 3 193.3684\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.7 ETS1","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS1_model <-   fable::ETS(Difference ~ season() + trend())\n\n# Calculate the error:\nETS1_test_error <- time_series_train %>%\n    fabletools::model(ETS1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS1_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS1_predictions[1],\n    'Error' = ETS1_test_error$RMSE,\n    'Date' = ETS1_predictions[2],\n    'Value' = ETS1_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS1_model 42.98491 2024 May 189.8662\n#> 2 ETS1_model 42.98491 2024 Jun 189.8662\n#> 3 ETS1_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.8 ETS2","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS2_model <- fable::ETS(Difference ~ trend())\n\n# Calculate the error:\nETS2_test_error <- time_series_train %>%\n    fabletools::model(ETS2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS2_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS2_predictions[1],\n    'Error' = ETS2_test_error$RMSE,\n    'Date' = ETS2_predictions[2],\n    'Value' = ETS2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS2_model 42.98491 2024 May 189.8662\n#> 2 ETS2_model 42.98491 2024 Jun 189.8662\n#> 3 ETS2_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.9 ETS3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS3_model <- fable::ETS(Difference ~ season())\n\n# Calculate the error:\nETS3_test_error <- time_series_train %>%\n    fabletools::model(ETS3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS3_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS3_predictions[1],\n    'Error' = ETS3_test_error$RMSE,\n    'Date' = ETS3_predictions[2],\n    'Value' = ETS3_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS3_model 42.98491 2024 May 189.8662\n#> 2 ETS3_model 42.98491 2024 Jun 189.8662\n#> 3 ETS3_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.10 ETS4","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS4_model <- fable::ETS(Difference)\n\n# Calculate the error:\nETS4_test_error <- time_series_train %>%\n    fabletools::model(ETS4_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS4_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS4_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS4_predictions[1],\n    'Error' = ETS4_test_error$RMSE,\n    'Date' = ETS4_predictions[2],\n    'Value' = ETS4_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS4_model 42.98491 2024 May 189.8662\n#> 2 ETS4_model 42.98491 2024 Jun 189.8662\n#> 3 ETS4_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"holt-winters-additive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.11 Holt-Winters Additive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nHolt_Winters_Additive_model <- fable::ETS(Difference ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n\n# Calculate the error:\nHolt_Winters_Additive_test_error <- time_series_train %>%\n    fabletools::model(Holt_Winters_Additive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nHolt_Winters_Additive_predictions <- time_series_test %>%\n    fabletools::model(\n      Holt_Winters_Additive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Holt_Winters_Additive_predictions[1],\n    'Error' = Holt_Winters_Additive_test_error$RMSE,\n    'Date' = Holt_Winters_Additive_predictions[2],\n    'Value' = Holt_Winters_Additive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                        .model    Error     Date    .mean\n#> 1 Holt_Winters_Additive_model 52.74689 2024 May 467.2980\n#> 2 Holt_Winters_Additive_model 52.74689 2024 Jun 802.6268\n#> 3 Holt_Winters_Additive_model 52.74689 2024 Jul 229.5501\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"holt-winters-damped","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.12 Holt-Winters Damped","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nHolt_Winters_Damped_model <- fable::ETS(Difference ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n\n# Calculate the error:\nHolt_Winters_Damped_test_error <- time_series_train %>%\n    fabletools::model(Holt_Winters_Damped_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nHolt_Winters_Damped_predictions <- time_series_test %>%\n    fabletools::model(\n      Holt_Winters_Damped_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Holt_Winters_Damped_predictions[1],\n    'Error' = Holt_Winters_Damped_test_error$RMSE,\n    'Date' = Holt_Winters_Damped_predictions[2],\n    'Value' = Holt_Winters_Damped_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                      .model    Error     Date     .mean\n#> 1 Holt_Winters_Damped_model 481.2281 2024 May 137.36755\n#> 2 Holt_Winters_Damped_model 481.2281 2024 Jun 116.40494\n#> 3 Holt_Winters_Damped_model 481.2281 2024 Jul  97.76673\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"holt-winters-multiplicative","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.13 Holt-Winters Multiplicative","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nHolt_Winters_Multiplicative_model <- fable::ETS(Difference ~ error(\"M\") + trend(\"A\") + season(\"M\"))\n\n# Calculate the error:\nHolt_Winters_Multiplicative_test_error <- time_series_train %>%\n    fabletools::model(Holt_Winters_Multiplicative_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nHolt_Winters_Multiplicative_predictions <- time_series_test %>%\n    fabletools::model(\n      Holt_Winters_Multiplicative_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Holt_Winters_Multiplicative_predictions[1],\n    'Error' = Holt_Winters_Multiplicative_test_error$RMSE,\n    'Date' = Holt_Winters_Multiplicative_predictions[2],\n    'Value' = Holt_Winters_Multiplicative_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                              .model    Error     Date\n#> 1 Holt_Winters_Multiplicative_model 470.7401 2024 May\n#> 2 Holt_Winters_Multiplicative_model 470.7401 2024 Jun\n#> 3 Holt_Winters_Multiplicative_model 470.7401 2024 Jul\n#>      .mean\n#> 1 122.6052\n#> 2 128.6342\n#> 3 108.2321\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.14 Linear 1","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nLinear1_model <- fable::ETS(Difference ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n\n# Calculate the error:\nLinear1_test_error <- time_series_train %>%\n    fabletools::model(Linear1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nLinear1_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Linear1_predictions[1],\n    'Error' = Linear1_test_error$RMSE,\n    'Date' = Linear1_predictions[2],\n    'Value' = Linear1_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date     .mean\n#> 1 Linear1_model 481.2281 2024 May 137.36755\n#> 2 Linear1_model 481.2281 2024 Jun 116.40494\n#> 3 Linear1_model 481.2281 2024 Jul  97.76673\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.15 Linear 2","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Linear2_model <- fable::TSLM(Difference)\n  \n  # Calculate the error:\n  Linear2_test_error <- time_series_train %>%\n    fabletools::model(Linear2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Linear2_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Linear2_predictions[1],\n    'Error' = Linear2_test_error$RMSE,\n    'Date' = Linear2_predictions[2],\n    'Value' = Linear2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date    .mean\n#> 1 Linear2_model 120.6944 2024 May 146.3409\n#> 2 Linear2_model 120.6944 2024 Jun 146.3409\n#> 3 Linear2_model 120.6944 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.16 Linear 3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Linear2_model <- fable::TSLM(Difference)\n  \n  # Calculate the error:\n  Linear2_test_error <- time_series_train %>%\n    fabletools::model(Linear2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Linear2_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Linear2_predictions[1],\n    'Error' = Linear2_test_error$RMSE,\n    'Date' = Linear2_predictions[2],\n    'Value' = Linear2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date    .mean\n#> 1 Linear2_model 120.6944 2024 May 146.3409\n#> 2 Linear2_model 120.6944 2024 Jun 146.3409\n#> 3 Linear2_model 120.6944 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.17 Linear 4","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Linear4_model <- fable::TSLM(Difference ~ trend())\n  \n  # Calculate the error:\n  Linear4_test_error <- time_series_train %>%\n    fabletools::model(Linear4_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Linear4_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear4_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Linear4_predictions[1],\n    'Error' = Linear4_test_error$RMSE,\n    'Date' = Linear4_predictions[2],\n    'Value' = Linear4_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date    .mean\n#> 1 Linear4_model 87.83978 2024 May 313.7312\n#> 2 Linear4_model 87.83978 2024 Jun 317.4928\n#> 3 Linear4_model 87.83978 2024 Jul 321.2543\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"mean","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.18 Mean","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Mean_model <- fable::MEAN(Difference)\n  \n  # Calculate the error:\n  Mean_test_error <- time_series_train %>%\n    fabletools::model(Mean_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Mean_predictions <- time_series_test %>%\n    fabletools::model(\n      Mean_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Mean_predictions[1],\n    'Error' = Mean_test_error$RMSE,\n    'Date' = Mean_predictions[2],\n    'Value' = Mean_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 Mean_model 120.6944 2024 May 146.3409\n#> 2 Mean_model 120.6944 2024 Jun 146.3409\n#> 3 Mean_model 120.6944 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"naive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.19 Naive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Naive_model <- fable::NAIVE(Difference)\n  \n  # Calculate the error:\n  Naive_test_error <- time_series_train %>%\n    fabletools::model(Naive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Naive_predictions <- time_series_test %>%\n    fabletools::model(\n      Naive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Naive_predictions[1],\n    'Error' = Naive_test_error$RMSE,\n    'Date' = Naive_predictions[2],\n    'Value' = Naive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>        .model    Error     Date .mean\n#> 1 Naive_model 52.38957 2024 May   175\n#> 2 Naive_model 52.38957 2024 Jun   175\n#> 3 Naive_model 52.38957 2024 Jul   175\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.20 Neuralnet 1","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet1_model <- fable::NNETAR(Difference ~ season() + trend())\n  \n  # Calculate the error:\n  Neuralnet1_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet1_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet1_predictions[1],\n    'Error' = Neuralnet1_test_error$RMSE,\n    'Date' = Neuralnet1_predictions[2],\n    'Value' = Neuralnet1_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date       .mean\n#> 1 Neuralnet1_model 33.67247 2024 May    35.34217\n#> 2 Neuralnet1_model 33.67247 2024 Jun  -667.09962\n#> 3 Neuralnet1_model 33.67247 2024 Jul -3670.34751\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.21 Neuralnet 2","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet2_model <- fable::NNETAR(Difference ~ trend())\n  \n  # Calculate the error:\n  Neuralnet2_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet2_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet2_predictions[1],\n    'Error' = Neuralnet2_test_error$RMSE,\n    'Date' = Neuralnet2_predictions[2],\n    'Value' = Neuralnet2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date      .mean\n#> 1 Neuralnet2_model 50.22354 2024 May   224.2327\n#> 2 Neuralnet2_model 50.22354 2024 Jun  -787.1454\n#> 3 Neuralnet2_model 50.22354 2024 Jul -5740.6597\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.22 Neuralnet 3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet3_model <- fable::NNETAR(Difference ~ season())\n  \n  # Calculate the error:\n  Neuralnet3_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet3_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet3_predictions[1],\n    'Error' = Neuralnet3_test_error$RMSE,\n    'Date' = Neuralnet3_predictions[2],\n    'Value' = Neuralnet3_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date    .mean\n#> 1 Neuralnet3_model 37.26703 2024 May 287.3900\n#> 2 Neuralnet3_model 37.26703 2024 Jun 359.2465\n#> 3 Neuralnet3_model 37.26703 2024 Jul 175.8352\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.23 Neuralnet 4","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet3_model <- fable::NNETAR(Difference ~ season())\n  \n  # Calculate the error:\n  Neuralnet3_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet3_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet3_predictions[1],\n    'Error' = Neuralnet3_test_error$RMSE,\n    'Date' = Neuralnet3_predictions[2],\n    'Value' = Neuralnet3_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date    .mean\n#> 1 Neuralnet3_model 32.37168 2024 May 257.0148\n#> 2 Neuralnet3_model 32.37168 2024 Jun 312.9690\n#> 3 Neuralnet3_model 32.37168 2024 Jul  86.5750\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"prophet-additive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.24 Prophet Additive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Prophet_Additive_model <- fable.prophet::prophet(Difference ~ season(period = 12, type = \"additive\"))\n  \n  # Calculate the error:\n  Prophet_Additive_test_error <- time_series_train %>%\n    fabletools::model(Prophet_Additive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Prophet_Additive_predictions <- time_series_test %>%\n    fabletools::model(\n      Prophet_Additive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Prophet_Additive_predictions[1],\n    'Error' = Prophet_Additive_test_error$RMSE,\n    'Date' = Prophet_Additive_predictions[2],\n    'Value' = Prophet_Additive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                   .model    Error     Date    .mean\n#> 1 Prophet_Additive_model 97.56159 2024 May 2633.286\n#> 2 Prophet_Additive_model 97.56159 2024 Jun 3193.283\n#> 3 Prophet_Additive_model 97.56159 2024 Jul 1001.410\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"prophet-multiplicative","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.25 Prophet Multiplicative","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Prophet_Multiplicative_model <- fable.prophet::prophet(Difference ~ season(period = 12, type = \"multiplicative\"))\n  \n  # Calculate the error:\n  Prophet_Multiplicative_test_error <- time_series_train %>%\n    fabletools::model(Prophet_Multiplicative_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Prophet_Multiplicative_predictions <- time_series_test %>%\n    fabletools::model(\n      Prophet_Multiplicative_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Prophet_Multiplicative_predictions[1],\n    'Error' = Prophet_Multiplicative_test_error$RMSE,\n    'Date' = Prophet_Multiplicative_predictions[2],\n    'Value' = Prophet_Multiplicative_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                         .model    Error     Date     .mean\n#> 1 Prophet_Multiplicative_model 75.40479 2024 May -34.78062\n#> 2 Prophet_Multiplicative_model 75.40479 2024 Jun -65.08612\n#> 3 Prophet_Multiplicative_model 75.40479 2024 Jul -32.17572\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"seasonal-naive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.26 Seasonal Naive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  SNaive_model <- fable::SNAIVE(Difference)\n  \n  # Calculate the error:\n  SNaive_test_error <- time_series_train %>%\n    fabletools::model(SNaive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  SNaive_predictions <- time_series_test %>%\n    fabletools::model(\n      SNaive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = SNaive_predictions[1],\n    'Error' = SNaive_test_error$RMSE,\n    'Date' = SNaive_predictions[2],\n    'Value' = SNaive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date .mean\n#> 1 SNaive_model 98.94106 2024 May   281\n#> 2 SNaive_model 98.94106 2024 Jun   185\n#> 3 SNaive_model 98.94106 2024 Jul   187\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"stochastic","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.27 Stochastic","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Stochastic_model <- fable::ARIMA(Difference ~ pdq(d = 1), stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n  \n  # Calculate the error:\n  Stochastic_test_error <- time_series_train %>%\n    fabletools::model(Stochastic_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Stochastic_predictions <- time_series_test %>%\n    fabletools::model(\n      Stochastic_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Stochastic_predictions[1],\n    'Error' = Stochastic_test_error$RMSE,\n    'Date' = Stochastic_predictions[2],\n    'Value' = Stochastic_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date    .mean\n#> 1 Stochastic_model 42.98025 2024 May 239.7312\n#> 2 Stochastic_model 42.98025 2024 Jun 256.0506\n#> 3 Stochastic_model 42.98025 2024 Jul 241.0237\nwarnings()\nsummary_table <- data.frame()"},{"path":"ensembles-of-26-forecasting-models.html","id":"ensembles-of-26-forecasting-models","chapter":"10 Ensembles of 26 Forecasting Models","heading":"10 Ensembles of 26 Forecasting Models","text":"know make 27 individual time series forecasting models, ensemble simply puts 27 models together.","code":"\n\nlibrary(fpp3)\n#> ── Attaching packages ────────────────────────── fpp3 0.5 ──\n#> ✔ tibble      3.2.1     ✔ tsibble     1.1.4\n#> ✔ dplyr       1.1.4     ✔ tsibbledata 0.4.1\n#> ✔ tidyr       1.3.1     ✔ feasts      0.3.2\n#> ✔ lubridate   1.9.3     ✔ fable       0.3.4\n#> ✔ ggplot2     3.5.1     ✔ fabletools  0.4.2\n#> ── Conflicts ───────────────────────────── fpp3_conflicts ──\n#> ✖ lubridate::date()    masks base::date()\n#> ✖ dplyr::filter()      masks stats::filter()\n#> ✖ tsibble::intersect() masks base::intersect()\n#> ✖ tsibble::interval()  masks lubridate::interval()\n#> ✖ dplyr::lag()         masks stats::lag()\n#> ✖ tsibble::setdiff()   masks base::setdiff()\n#> ✖ tsibble::union()     masks base::union()\n\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\n# Fit the ensemble model on the training data\nEnsembles_model <- time_series_train %>%\n    fabletools::model(\n      Ensemble = (\n      fable::TSLM(Value ~ season() + trend()) +\n      fable::TSLM(Value) + fable::TSLM(Value ~ season()) +\n      fable::TSLM(Value ~ trend()) +\n      fable::ARIMA(Value ~ season() + trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Value ~ season(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Value ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +         fable::ARIMA(Value) + fable::ARIMA(Value ~ pdq(d = 1), stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ETS(Value ~ season() + trend()) + fable::ETS(Value ~ trend()) + fable::ETS(Value ~ season()) +\n      fable::ETS(Value) +\n      fable::ETS(Value ~ error(\"A\") + trend(\"A\") + season(\"A\")) + fable::ETS(Value ~ error(\"M\") + trend(\"A\") + season(\"M\")) +\n      fable::ETS(Value ~ error(\"M\") + trend(\"Ad\") + season(\"M\")) +\n      fable::MEAN(Value) +\n      fable::NAIVE(Value) +\n      fable::SNAIVE(Value) +\n      fable::SNAIVE(Value ~ drift()) +\n      fable.prophet::prophet(Value ~ season(period = 12, type = \"multiplicative\")) +\n      fable.prophet::prophet(Value ~ season(period = 12, type = \"additive\")) +\n      fable::NNETAR(Value ~ season() + trend()) +\n      fable::NNETAR(Value ~ trend()) +\n      fable::NNETAR(Value ~ season()) +\n      fable::NNETAR(Value))/26\n    )\n\n# # Make predicitons:\n# Ensemble_predictions <- time_series_test %>% \n#   model(Ensemble_model) %>%\n#     fabletools::forecast(h = number)\n\nEnsemble_predictions <- time_series_test %>%\n  fabletools::model(\n    Ensemble = (\n      fable::TSLM(Difference ~ season() + trend()) +\n      fable::TSLM(Difference) +\n      fable::TSLM(Difference ~ season()) +\n      fable::TSLM(Difference ~ trend()) +\n      fable::ARIMA(Difference ~ season() + trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Difference ~ season(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Difference ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Difference) +\n      fable::ARIMA(Difference ~ pdq(d = 1), stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ETS(Difference ~ season() + trend()) +\n      fable::ETS(Difference ~ trend()) +\n      fable::ETS(Difference ~ season()) +\n      fable::ETS(Difference) +\n      fable::ETS(Difference ~ error(\"A\") + trend(\"A\") + season(\"A\")) +\n      fable::ETS(Difference ~ error(\"M\") + trend(\"A\") + season(\"M\")) +\n      fable::ETS(Difference ~ error(\"M\") + trend(\"Ad\") + season(\"M\")) +\n      fable::MEAN(Difference) +\n      fable::NAIVE(Difference) +\n      fable::SNAIVE(Difference) +\n      fable::SNAIVE(Difference ~ drift()) +\n      fable.prophet::prophet(Difference ~ season(period = 12, type = \"multiplicative\")) +\n      fable.prophet::prophet(Difference ~ season(period = 12, type = \"additive\")) +\n      fable::NNETAR(Difference ~ season() + trend()) +\n      fable::NNETAR(Difference ~ trend()) +\n      fable::NNETAR(Difference ~ season()) +\n      fable::NNETAR(Difference)/26\n    )\n  ) %>%\n  fabletools::forecast(h = number)\n\nresults <- data.frame(\n  'Model' = Ensemble_predictions[1],\n  'Date' = Ensemble_predictions[2],\n  'Forecast' = Ensemble_predictions[4]\n)\n\nreturn(results)\n\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#> Warning in sqrt(diag(best$var.coef)): NaNs produced\n#>     .model     Date     .mean\n#> 1 Ensemble 2024 May  9786.223\n#> 2 Ensemble 2024 Jun  8727.396\n#> 3 Ensemble 2024 Jul -1303.570\nwarnings()"},{"path":"predicting-on-totally-new-data-with-individual-models-and-ensembles.html","id":"predicting-on-totally-new-data-with-individual-models-and-ensembles","chapter":"11 Predicting on totally new data with individual models and ensembles","heading":"11 Predicting on totally new data with individual models and ensembles","text":"Let’s start simple ensemble cubist, gam linear models:","code":"\nlibrary(tree) # Allows us to use tree models\nlibrary(MASS) # For the Boston Housing data set library(Metrics)\nlibrary(reactable) # For the final report - looks amazing!\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ✖ dplyr::select() masks MASS::select()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Set initial values to 0\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_RMSE <- 0\nlinear_test_predict_value <- 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_RMSE <- 0\ntree_holdout_RMSE <- 0\ntree_test_predict_value <- 0\n\nensemble_linear_RMSE <- 0\nensemble_linear_RMSE_mean <- 0\nensemble_tree_RMSE <- 0\nensemble_tree_RMSE_mean <- 0\n\nnumerical_1 <- function(data, colnum, train_amount, test_amount, numresamples, do_you_have_new_data = c(\"Y\", \"N\")){\n\n# Move target column to far right\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Set up resampling\nfor (i in 1:numresamples) {\n  idx <- sample(seq(1, 2), size = nrow(data), replace = TRUE, prob = c(train_amount, test_amount))\n  train <- data[idx == 1, ]\n  test <- data[idx == 2, ]\n\n# Fit linear model on the training data, make predictions on the test data\nlinear_train_fit <- lm(y ~ ., data = train)\nlinear_predictions <- predict(object = linear_train_fit, newdata = test)\nlinear_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = linear_predictions)\nlinear_RMSE_mean <- mean(linear_RMSE)\n\n# Fit tree model on the training data, make predictions on the test data\ntree_train_fit <- tree(y ~ ., data = train)\ntree_predictions <- predict(object = tree_train_fit, newdata = test)\ntree_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = tree_predictions)\ntree_RMSE_mean <- mean(tree_RMSE)\n\n# Make the weighted ensemble\nensemble <- data.frame(\n  'linear' = linear_predictions / linear_RMSE_mean,\n  'tree' = tree_predictions / tree_RMSE_mean,\n  'y_ensemble' = test$y)\n\n# Split ensemble between train and test\nensemble_idx <- sample(seq(1, 2), size = nrow(ensemble), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble[ensemble_idx == 1, ]\nensemble_test <- ensemble[ensemble_idx == 2, ]\n\n# Fit the ensemble data on the ensemble training data, predict on ensemble test data\nensemble_linear_train_fit <- lm(y_ensemble ~ ., data = ensemble_train)\n\nensemble_linear_predictions <- predict(object = ensemble_linear_train_fit, newdata = ensemble_test)\n\nensemble_linear_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_linear_predictions)\n\nensemble_linear_RMSE_mean <- mean(ensemble_linear_RMSE)\n\n# Fit the tree model on the ensemble training data, predict on ensemble test data\nensemble_tree_train_fit <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree_train_fit, newdata = ensemble_test) \n\nensemble_tree_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_tree_predictions)\n\nensemble_tree_RMSE_mean <- mean(ensemble_tree_RMSE)\n\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble_Linear', 'Ensemble_tree'),\n  'Error_Rate' = c(linear_RMSE_mean, tree_RMSE_mean, ensemble_linear_RMSE_mean, ensemble_tree_RMSE_mean)\n)\n\nresults <- results %>% arrange(Error_Rate)\n\n} # Closing brace for numresamples\n\nif (do_you_have_new_data == \"Y\") {\n  new_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/EnsemblesData/main/NewBoston.csv', header = TRUE, sep = ',')\n\n  y <- 0\n  colnames(new_data)[colnum] <- \"y\"\n\n  new_data <- new_data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n}\n  \n  new_linear <- predict(object = linear_train_fit, newdata = new_data)\n  new_tree <- predict(object = tree_train_fit, newdata = new_data)\n\n  new_ensemble <- data.frame(\n    \"linear\" = new_linear / linear_RMSE_mean,\n    \"tree\" = new_tree / tree_RMSE_mean\n    )\n\n  new_ensemble$Row_mean <- rowMeans(new_ensemble)\n  new_ensemble$y_ensemble <- new_data$y\n\n  new_ensemble_linear <- predict(object = ensemble_linear_train_fit, newdata = new_ensemble)\n  new_ensemble_tree <- predict(object = ensemble_tree_train_fit, newdata = new_ensemble)\n\n  new_data_results <-\n    data.frame(\n      \"True_Value\" = new_ensemble$y_ensemble,\n      \"Linear\" = round(new_linear, 4),\n      \"Tree\" = round(new_tree, 4),\n      \"Ensemble_Linear\" = round(new_ensemble_linear, 4),\n      \"Ensemble_Tree\" = round(new_ensemble_tree, 4)\n    )\n\n  df1 <- t(new_data_results)\n\n  predictions_of_new_data <- reactable::reactable(\n    data = df1, searchable = TRUE, pagination = FALSE, wrap = TRUE, rownames = TRUE, fullWidth = TRUE, filterable = TRUE, bordered = TRUE,\n    striped = TRUE, highlight = TRUE, resizable = TRUE\n  ) %>%\n    \n    reactablefmtr::add_title(\"Predictions of new data\")\n  \n  results <- reactable::reactable(\n    data = results, searchable = TRUE, pagination = FALSE, wrap = TRUE, rownames = TRUE, fullWidth = TRUE, filterable = TRUE, bordered = TRUE, striped = TRUE, highlight = TRUE, resizable = TRUE\n  ) %>% \n    reactablefmtr::add_title(\"Model and error rates\")\n\nreturn(list(results, predictions_of_new_data))\n\n} # Closing brace for the function\n\nnumerical_1(data = read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/EnsemblesData/main/Boston_Housing.csv'), colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25, do_you_have_new_data = \"Y\")\n#> [[1]]\n#> \n#> [[2]]\n\n# Note these results show up in the Viewer."},{"path":"how-to-communicate-your-results.html","id":"how-to-communicate-your-results","chapter":"12 How to communicate your results","heading":"12 How to communicate your results","text":"chapter, going discuss, presenting results people various levels organization. include people ranging management, analysts, people C-Suite. elements common presenting results, elements going specific, depending person’s responsibilities include.’ve opportunity manage multi-million dollar accounts fortune 1000 company, ’ve also run two volunteer nonprofits, chapter Amnesty International, Chicago Apple User Group.work also includes several nonprofit social service organizations. ’ve also run vacation rental business, ’ve done lot volunteer work. Therefore, can speak wide range experience business needs.","code":""},{"path":"how-to-communicate-your-results.html","id":"a-very-basic-introduction-to-financial-reporting","chapter":"12 How to communicate your results","heading":"12.1 A very basic introduction to financial reporting","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"always-give-the-best-service-you-can-possibly-do-the-ritz-carlton-method","chapter":"12 How to communicate your results","heading":"12.1.1 Always give the best service you can possibly do: The Ritz Carlton method","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"help-your-manager-make-the-best-possible-decisions","chapter":"12 How to communicate your results","heading":"12.2 Help your manager make the best possible decisions","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"have-empathy-for-your-managersand-customers-situations","chapter":"12 How to communicate your results","heading":"12.3 Have empathy for your manager’s—and customer’s— situations","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"dont-need-them-in-any-way","chapter":"12 How to communicate your results","heading":"12.3.1 don’t need them in any way","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"have-a-great-story-to-tellhow-to-create-a-great-story","chapter":"12 How to communicate your results","heading":"12.3.2 Have a great story to tell—How to create a great story","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"br-prepared-with-strong-counterexamples","chapter":"12 How to communicate your results","heading":"12.3.3 Br prepared with strong counterexamples","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"include-uncertainty","chapter":"12 How to communicate your results","heading":"12.3.4 include uncertainty","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"tell-the-hard-truth-prepare-them-for-criticism","chapter":"12 How to communicate your results","heading":"12.3.5 tell the (hard) truth, prepare them for criticism","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"communicating-results-with-staff-who-have-profit-and-loss-responsibility","chapter":"12 How to communicate your results","heading":"12.4 Communicating results with staff who have Profit and Loss responsibility","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"vision-mission-values---what-is-the-central-question","chapter":"12 How to communicate your results","heading":"12.4.1 Vision, mission, values - what is the central question?","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"financials","chapter":"12 How to communicate your results","heading":"12.4.2 Financials","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"the-higher-up-the-org-chart-the-more-you-will-be-working-with-vision-stragety-financials","chapter":"12 How to communicate your results","heading":"12.4.3 The higher up the org chart, the more you will be working with vision, stragety, financials","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"communicating-with-customers-and-vendors","chapter":"12 How to communicate your results","heading":"12.5 Communicating with customers and vendors","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"communicating-roi-and-other-results-of-data-science","chapter":"12 How to communicate your results","heading":"12.6 Communicating ROI and other results of data science","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"social-media-and-more","chapter":"12 How to communicate your results","heading":"12.7 Social media and more","text":"","code":""},{"path":"final-comprhensive-project.html","id":"final-comprhensive-project","chapter":"13 Final Comprhensive Project","heading":"13 Final Comprhensive Project","text":"text.","code":""},{"path":"building-your-own-no-code-solutions.html","id":"building-your-own-no-code-solutions","chapter":"14 Building your own no-code solutions","heading":"14 Building your own no-code solutions","text":"text","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":" Introduction\nPhoto City Chicago snow plow stuck snow: Victorgrigas English Wikipedia - (t3xt (talk)) created work entirely ., CC0, Link","code":""}]

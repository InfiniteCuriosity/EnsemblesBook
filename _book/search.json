[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"Welcome Ensembles! book guide entire process building ensemble models beginning end. also give full access Ensembles package automates entire process.’ve done best make book interesting, fun, practical. lots examples using real world data steps included.able wonderful things complete skills book. book show, ensembles much accurate method help us understand model nature. done level accuracy achieved previously. can .phrase “wonderful things” intentional. Howard Carter archaeology, one point November, 1922, quite sure found something important. Carter made small hole see . Lord Carnarvon (paying !) asked Howard Carter, “Can see anything?. Howard Carter’s famous reply,”Yes, wonderful things!“. opened everything , found intact tomb Tutankhamun. contained 5,000 items, enriched knowledge ancient Africa beyond find.tiny taste one 5,000 “wonderful things” found Howard Carter, Lord Carnarvon, team archaeologists.best share many “wonderful things” entire book explore world ensembles.Ensembles package ’ve made entire analysis process automatically. put power ensembles hands, give strongest foundation work, highest degree accuracy.examples book come real data. example (many examples book):• HR Analytics• Predicting winning time London Marathon• World’s accurate score difficult classification problem• Beat best score student Kaggle competitionsWe many practical examples wide range fields enjoy.book show ensembles improve understanding nature, can use ensembles work. results using ensembles much accurate ever possible , demonstrated book. able use ensembles understand world, build models data, level accuracy achieved .","code":""},{"path":"index.html","id":"ensembles-the-new-ai-from-beginner-to-expert","chapter":"1 Welcome!","heading":"1.1 Ensembles: The New AI, from beginner to expert","text":"see, Ensembles new AI. Science gone calculus Newton Leibnetz, differential equations, modern world creating models, many points -. Ensembles powerful way put models together achieve best possible results. book guide process, show can build ensembles pass testing.new AI. Welcome path, ’s extremely fun, look forward sharing !","code":""},{"path":"index.html","id":"what-you-will-be-able-to-do-by-the-end-of-the-book","chapter":"1 Welcome!","heading":"1.2 What you will be able to do by the end of the book","text":"• Make customized ensembles models numerical, classification, logistic time series data.• Use Ensembles package entire process automatically (little customization possible).• Make ensemble solutions packages can shared users.• Make ensemble solutions totally self-contained solutions can shared anyone.• Learn ensembles models can help make wisest possible decision based data.• Learn present results different levels, regular user CEO board directors.• present results social media friendly.• Find data create ensemble solution beginning end (called One chapter exercises)• Solve real world examples book ensembles achieve results :• Beat top score student data science competition 90% (numerical ensembles).• Correctly predict winning time 2024 Men’s London Marathon (time series ensembles).• Produce 100% accurate solution dry beans classification problem (first world data set, done using classification ensembles).• Make recommendations Lebron James can improve performance basketball court (logistic ensembles).• Complete comprehensive Final Project put new skills ensembles together. result can shared employers, advisors, social media, job interviews, anywhere else like share work.","code":""},{"path":"index.html","id":"how-this-book-is-organized-so-you-learn-the-material-as-easily-as-possible","chapter":"1 Welcome!","heading":"1.3 How this book is organized so you learn the material as easily as possible","text":"book begins foundations making ensembles models. look :• Individual numerical models• Ensembles numerical models• Individual classification models• Ensembles classification models• Individual logistic models• Ensembles logistic models• Individual forecasting models• Ensembles forecasting models• Advanced data visualizations• Multiple ways communicate results. range people field, customers, C-Suite (CEO, CTO, board directors, etc.)• look treat data science business. particular pay close attention showing return investment (ROI) data science, using ensembles models.• book conclude showing four examples final comprehensive project. one example numerical data, classification data, logistic data forecasting data. example professionally formatted. source files eight files available github repository.","code":""},{"path":"index.html","id":"how-you-can-learn-the-skills-as-fast-as-possible-how-the-exercises-are-organized","chapter":"1 Welcome!","heading":"1.4 How you can learn the skills as fast as possible: How the exercises are organized","text":"young child, learned much better retention system always called delayed repetition. means learn best fastest see worked example, several practice examples, repeat delay time. delay can range hour days.example, exercises Individual Classification Models chapter ask build models using techniques classification models prior chapters. exercises logistic ensembles ask build models content logistic models chapter, previous chapters. experience repeating fastest way learn new content, retain longest period time.time get Final Comprehensive Project, skills sharp modeling techniques.","code":""},{"path":"index.html","id":"going-from-student-to-teacher-you-are-required-to-post-on-social-media-and-help-others-understand-the-results","chapter":"1 Welcome!","heading":"1.5 Going from student to teacher: You are required to post on social media and help others understand the results","text":"One important parts role data science communicating findings. present many examples summaries reports adapt use projects. also required post results social media. may use appropriate choice social media, needs publicly available. number important benefits :• build body work shows skill level• results demonstrate ability communicate way works wide variety people• work demonstrate good skills video /audio production• Use hashtag #AIEnsembles post social media","code":""},{"path":"index.html","id":"helping-you-use-the-power-of-pre-trained-ensembles-and-individual-models","chapter":"1 Welcome!","heading":"1.6 Helping you use the power of pre-trained ensembles and individual models","text":"Another important part skills learn includes building pre-trained ensembles models. book walk process building pre-trained models ensembles four types data (numerical, classification, logicial, time series).","code":""},{"path":"index.html","id":"helping-you-master-the-material-one-of-your-own-exercises","chapter":"1 Welcome!","heading":"1.7 Helping you master the material: One of your own exercises","text":"One differences exercises Ensembles inclusion One exercises. set exercises include one asks find data (many hints given help find data), define problem, make ensemble, report results.","code":""},{"path":"index.html","id":"keeping-it-real-actual-business-data-and-problems-as-the-source-of-all-the-data-sets","chapter":"1 Welcome!","heading":"1.8 Keeping it real: Actual business data and problems as the source of all the data sets","text":"data sets book use real data. exceptions, synthetic data. sources data cited, real world implications can found simple search. data absolutely real.","code":""},{"path":"index.html","id":"check-your-biases-test-your-model-on-a-neutral-data-set","chapter":"1 Welcome!","heading":"1.9 Check your biases: Test your model on a neutral data set","text":"set exercises ask check one trained models neutral data set. model biases, reveal . ’ll knowledge go back address biases models.","code":""},{"path":"index.html","id":"helping-you-check-your-workand-verifying-that-your-results-beat-previously-published-results","chapter":"1 Welcome!","heading":"1.10 Helping you check your work—and verifying that your results beat previously published results","text":"Many data sets solved previous investigators (competitions), results can easily compared published results.example, look Boston Housing data set look numerical data sets. data set used many times Kaggle competitions, published papers, Github repositories, among many sources.Ensembles package automatically solve data set, return RMSE less 0.20 (slight variation depending parameters set, explained chapters). comparison, Boston Housing data set used Kaggle student competition: https://www.kaggle.com/competitions/uou-g03784-2022-spring/leaderboard?tab=public, best score 2.09684. Ensembles package beat best result Kaggle student competition 90%. Ensembles package requires one line code.","code":""},{"path":"index.html","id":"helping-you-work-as-a-team-with-fully-reproducible-ensembles-and-individual-models","chapter":"1 Welcome!","heading":"1.11 Helping you work as a team with fully reproducible ensembles and individual models","text":"large part skills learn include make results reproducible. include:• Multiple random resamplings data• Learning test totally unseen data individual ensemble models• repeat results (example, 25 times), report accuracy resamplingFor example, make ensembles models, use trained models make predictions totally unseen data.","code":""},{"path":"index.html","id":"the-final-comprehensive-project-will-put-everything-together-for-you","chapter":"1 Welcome!","heading":"1.12 The Final Comprehensive Project will put everything together for you","text":"studying data science, one professors said papers turned “good enough show CEO Board Directors” Fortune 1000 company worked . chapter Final Comprehensive Project share highest level skills following:• Truly understanding business problem• able convey high value data science brings table• able back 100% claims rock solid evidence, facts, clear reasoning• make truly professional quality presentation worthy C-SuiteI’ve incredible pleasure learning many different skills. include able play 20 musical instruments, communicate three languages professional level, manage multi-million dollar division Fortune 1000 company, run two non-profit volunteer groups, snowboard three mile run Colorado, work professional counselor, much . book reading recent project. None skills acquired overnight. huge part success able make slow (usually) steady progress. next chapter reveal big secret getting results, now best plan regular time work contents book.Always remember test everything, save ton problems road.","code":""},{"path":"index.html","id":"exercises-to-help-improve-your-skills","chapter":"1 Welcome!","heading":"1.13 Exercises to help improve your skills","text":"Exercise 1: Schedule regular time work bookYou gain much progress work steady pace. Take everything small pieces. ’s OK go slow, long keep going. Schedule regular time work book, get largest possible reward efforts.Exercise 2: Read chapter least twice begin working material.Reading chapter twice begin working actually speed progress results. actually take less time complete chapter. might believe right now, ’s totally true.Exercise 2a: Read chapter ahead able .Exercise 3: Read chapter ","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"introduction-and-your-first-ensembles","chapter":"2 Introduction and your first ensembles","heading":"2 Introduction and your first ensembles","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"how-a-chicago-blizzard-led-to-the-very-unlikely-story-of-the-best-solutions-to-supervised-data","chapter":"2 Introduction and your first ensembles","heading":"2.1 How a Chicago blizzard led to the very unlikely story of the best solutions to supervised data","text":"journey advanced AI world started actual\nblizzard Chicago. might seem like Chicago never get \nblizzard, 2011, incredibly intense, \nvideo shows:https://www.youtube.com/watch?v=cPiFn52ztd8What Chicago 2011\nSnomageddon\ncreation advanced AI? Everything. ’s\nstory.time 2011 Blizzard worked Recruiter Kelly\nServices, \nworked since 1996. agreed work Kelly Services office \nFrankfort, Illinois time, though worked nearly every\nKelly Services office one time another. trip Frankfort\ninvolved daily commute office, able make best\nuse time road.manager time let know several days advance \nlarge amount snow forecast, might want \nprepared. recent forecasts large amounts snow \nChicago area amounted nothing. perfectly normal days \nChicago area, predicted storm also nothing, based\nrecent results. great example prior\nprediction transferring well current situation.morning went work normal, even look \nweather forecast. Around 2:45 pm manager came office \nsaid “Russ, need come look weather radar!”. \nwalked office, saw map winter storm \nincredibly huge. image zoomed , possible see\nseveral states. tell, massive snow storm \nbarreling Chicago, 15 minutes away \nlocation.told candidate interviewing leaving immediately,\nallowed stay. get home fast \npossible safety.storm started dropping snow trip north back home. commute\ntook around 50% longer normal due rapidly falling snow.later learned, storm forecast start Chicago area\naround 3:00 pm, finish 11:00 - 1:00 pm two days later, \nleave 17 - 19 inches snow.bad ? Even City Chicago snow plows stopped \nsnow:see forecasts looked like, check news report \nday:https://www.nbcchicago.com/news/local/blizzard-unleashes-winter-fury/2096753/turns three predictions blizzard accurate \nlevel almost seemed uncanny : Start time, accumulation, \nend time spot . first time recall ever seeing \nprediction level accuracy. idea type \npredictive accuracy even possible. level accuracy \npredicting results totally blew away. never seen anything \nlevel accuracy, now wanted know done.searched searched accuracy high \nforecast.power method—whatever —obvious . realized\nwork weather, solution method work \nincredibly broad range situations. many areas\ninclude business forecasts, production work, modeling prices, much,\nmuch . point idea accurate prediction\ndone.months later person wrote Tom\nSkilling, chief\nmeteorologist WGN TV Chicago. Tom posted answer opened \nsolution . relevant part Tom Skilling’s\nanswer \n2011 storm forecast accurate:Weather Service developed interesting “SNOWFALL ENSEMBLE\nFORECAST PROBABILITY SYSTEM” draws upon wide range snow\naccumulation forecasts whole set different computer models.\n“blending” model projections, probability snowfalls\nfalling within certain ranges becomes possible. Also, “blending”\nmultiple forecasts “smooths” sometimes huge model disparities\namounts predicted. resulting probabilities therefore\nrepresent “best case” forecast.first step. Ensembles way achieved \nextraordinary prediction accuracy.next goal figure ensembles made. looked \ninformation, became obvious ensembles used ,\nwinning entry Netflix Prize Competition:Netflix Prize\nCompetition sponsored\nNetflix create method accurately predict user ratings \nfilms. minimum winning score needed beat Netflix method\n(named Cinematch) least 10%. Several years work went \nsolving problem, results even included several published\npapers. winning solution ensemble methods beat \nCinematch results 10.09%.now clear ensembles path forward. However,\nidea make ensembles.went graduate school study data science predictive\nanalytics. degree completed 2017, Northwestern\nUniversity. However, still sure ensembles models \nbuilt, find clear methods build (except \npre-made methods, random forests). true \npackages work, nothing found \nlooking : build ensembles models general. Despite\nplaying idea looking online, able build \nensembles wanted build.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"saturday-october-15-2022-at-458-pm.-the-exact-birth-of-the-ensembles-system","chapter":"2 Introduction and your first ensembles","heading":"2.2 Saturday, October 15, 2022 at 4:58 pm. The exact birth of the Ensembles system","text":"Everything changed Saturday, October 15, 2022 4:58 pm. \nplaying various methods make ensemble, got ensemble\nworked first time. results extremely\nmodest standards, clear foundation \nbuild general solution can work extremely wide\nrange areas. journal entry:might asking know day time. \nreasonable question. ’ve keeping journal since 19 years\nold, thousands entries. soon realized \ncorrectly build ensembles, made entry, contains key\nelements make ensemble, steps just \nmoment. Notice subject line journal matches text\n.One ways improve skills keep journal, ’ll\nlooking depth chapter future chapters.\njournal use MacJournal, though \nlarge number options available market.Birth ensembles, Saturday, October 15, 2022 4:58 pm","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"here-is-what-an-ensemble-of-models-looks-like-at-the-most-basic-level-using-the-boston-housing-data-set-as-an-example","chapter":"2 Introduction and your first ensembles","heading":"2.3 Here is what an ensemble of models looks like at the most basic level, using the Boston Housing data set as an example:","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"head-of-boston-housing-data-set","chapter":"2 Introduction and your first ensembles","heading":"2.3.1 Head of Boston Housing data set","text":"start first ensemble data set numerical\nvalues. first example use Boston Housing data set, \nMASS package. Boston Housing data set controversial (\ndiscuss controversies example making\nprofessional quality reports C-Suite), now works \nwell known data set begin journey ensembles.Overview basic steps make ensemble:using Boston Housing data set, let’s look \nBoston images:","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"the-steps-to-build-your-first-ensemble-from-scratch","chapter":"2 Introduction and your first ensembles","heading":"2.4 The steps to build your first ensemble from scratch","text":"Load packages need (MASS, tree)Load packages need (MASS, tree)Load Boston Housing data set, split train (60%) \ntest (40%) sections.Load Boston Housing data set, split train (60%) \ntest (40%) sections.Create linear model fitting linear model training\ndata, make predictions Boston Housing test data. Measure\naccuracy predictions actual values.Create linear model fitting linear model training\ndata, make predictions Boston Housing test data. Measure\naccuracy predictions actual values.Create model using trees fitting tree model training\ndata, making predictions Boston Housing test data.\nMeasure accuracy predictions actual values.Create model using trees fitting tree model training\ndata, making predictions Boston Housing test data.\nMeasure accuracy predictions actual values.Make new data frame. ensemble model\npredictions. One column linear predictions, one \ntree predictions.Make new data frame. ensemble model\npredictions. One column linear predictions, one \ntree predictions.Make new column true values—true values \nBoston Housing test data setMake new column true values—true values \nBoston Housing test data setOnce new ensemble data set, ’s simply another data\nset. different many ways data set (except \nmade).new ensemble data set, ’s simply another data\nset. different many ways data set (except \nmade).Break ensemble data set train (60%) test (40%)\nsections.Break ensemble data set train (60%) test (40%)\nsections.Fit linear model ensemble training data. Make predictions\nusing testing data, measure accuracy predictions\ntest data.Fit linear model ensemble training data. Make predictions\nusing testing data, measure accuracy predictions\ntest data.Summarize results.Summarize results.suggest reading basic steps make ensemble\ncouple times, make sure familiar steps.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"building-the-first-actual-ensemble","chapter":"2 Introduction and your first ensembles","heading":"2.5 Building the first actual ensemble","text":"Load packages need (MASS, tree):Load Boston Housing data set, split train (60%) test\n(40%) sections.Create linear model fitting linear model training data,\nmake predictions Boston Housing test data. Measure \naccuracy predictions actual values.Calculate error modelThe error rate linear model 6.108005. Let’s using\ntree method.Create model using trees fitting tree model training\ndata, making predictions Boston Housing test data. Measure\naccuracy predictions actual values.Calculate error rate tree model:error rate tree model lower (better). error\nrate tree model 5.478017.","code":"\nlibrary(MASS) # for the Boston Housing data set\nlibrary(tree) # To make models using trees\nlibrary(Metrics) # To calculate error rate (root mean squared error)\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ✖ dplyr::select() masks MASS::select()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\ndf <- MASS::Boston\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\n# Let's have a quick look at the train and test sets\nhead(train)\n#>      crim zn indus chas   nox    rm  age    dis rad tax\n#> 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296\n#> 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242\n#> 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242\n#> 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222\n#> 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222\n#> 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222\n#>   ptratio  black lstat medv\n#> 1    15.3 396.90  4.98 24.0\n#> 2    17.8 396.90  9.14 21.6\n#> 3    17.8 392.83  4.03 34.7\n#> 4    18.7 394.63  2.94 33.4\n#> 5    18.7 396.90  5.33 36.2\n#> 6    18.7 394.12  5.21 28.7\nhead(test)\n#>         crim zn indus chas   nox    rm   age    dis rad tax\n#> 401 25.04610  0  18.1    0 0.693 5.987 100.0 1.5888  24 666\n#> 402 14.23620  0  18.1    0 0.693 6.343 100.0 1.5741  24 666\n#> 403  9.59571  0  18.1    0 0.693 6.404 100.0 1.6390  24 666\n#> 404 24.80170  0  18.1    0 0.693 5.349  96.0 1.7028  24 666\n#> 405 41.52920  0  18.1    0 0.693 5.531  85.4 1.6074  24 666\n#> 406 67.92080  0  18.1    0 0.693 5.683 100.0 1.4254  24 666\n#>     ptratio  black lstat medv\n#> 401    20.2 396.90 26.77  5.6\n#> 402    20.2 396.90 20.32  7.2\n#> 403    20.2 376.11 20.31 12.1\n#> 404    20.2 396.90 19.77  8.3\n#> 405    20.2 329.46 27.38  8.5\n#> 406    20.2 384.97 22.98  5.0\nBoston_lm <- lm(medv ~ ., data = train) # Fit the model to the training data\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the model predictions\nhead(Boston_lm_predictions)\n#>       401       402       403       404       405       406 \n#> 12.618507 19.785728 20.919370 13.014507  6.946392  5.123039\nBoston_linear_RMSE <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions)\nBoston_linear_RMSE\n#> [1] 6.108005\nBoston_tree <- tree(medv ~ ., data = train) # Fit the model to the training data\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\n\n# Let's have a quick look at the predictions:\nhead(Boston_tree_predictions)\n#>      401      402      403      404      405      406 \n#> 13.30769 13.30769 13.30769 13.30769 13.30769 13.30769\nBoston_tree_RMSE <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions)\nBoston_tree_RMSE\n#> [1] 5.478017"},{"path":"introduction-and-your-first-ensembles.html","id":"were-ready-to-make-our-first-ensemble","chapter":"2 Introduction and your first ensembles","heading":"2.6 We’re ready to make our first ensemble!!","text":"Make new data frame. ensemble model predictions,\none column true values. One column linear\npredictions, one tree predictions. ’ll make third\ncolumn, true values.Make new column true values—true values \nBoston Housing test data setOnce new ensemble data set, ’s simply another data set. \ndifferent many ways data set (except made).Break ensemble data set train (60%) test (40%) sections.\nnothing special 60/40 split , may use \nnumbers wish.Fit linear model ensemble training data. Make predictions using\ntesting data, measure accuracy predictions \ntest data. Notice similar linear tree models.Summarize results.Clearly ensemble lowest error rate three models. \nensemble easily best three models \nlowest error rate models.","code":"\nensemble <- data.frame(\n  'linear' = Boston_lm_predictions,\n  'tree' = Boston_tree_predictions,\n  'y' = test$medv\n)\n\n# Let's have a look at the ensemble:\nhead(ensemble)\n#>        linear     tree    y\n#> 401 12.618507 13.30769  5.6\n#> 402 19.785728 13.30769  7.2\n#> 403 20.919370 13.30769 12.1\n#> 404 13.014507 13.30769  8.3\n#> 405  6.946392 13.30769  8.5\n#> 406  5.123039 13.30769  5.0\ndim(ensemble)\n#> [1] 105   3\nensemble_train <- ensemble[1:60, ]\nensemble_test <- ensemble[61:105, ]\n\nhead(ensemble_train)\n#>        linear     tree    y\n#> 401 12.618507 13.30769  5.6\n#> 402 19.785728 13.30769  7.2\n#> 403 20.919370 13.30769 12.1\n#> 404 13.014507 13.30769  8.3\n#> 405  6.946392 13.30769  8.5\n#> 406  5.123039 13.30769  5.0\nhead(ensemble_test)\n#>       linear     tree    y\n#> 461 23.88984 13.30769 16.4\n#> 462 23.29129 13.30769 17.7\n#> 463 22.54055 21.84327 19.5\n#> 464 25.50940 21.84327 20.2\n#> 465 22.71231 21.84327 21.4\n#> 466 20.83810 21.84327 19.9\n# Fit the model to the training data\nensemble_lm <- lm(y ~ ., data = ensemble_train)\n\n# Make predictions using the model on the test data\nensemble_lm_predictions <- predict(object = ensemble_lm, newdata = ensemble_test)\n\n# Calculate error rate for the ensemble predictions\nensemble_lm_rmse <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_lm_predictions)\n\n# Report the error rate for the ensemble\nensemble_lm_rmse\n#> [1] 4.826962\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble'),\n  'Error' = c(Boston_linear_RMSE, Boston_tree_RMSE, ensemble_lm_rmse)\n)\n\nresults\n#>      Model    Error\n#> 1   Linear 6.108005\n#> 2     Tree 5.478017\n#> 3 Ensemble 4.826962"},{"path":"introduction-and-your-first-ensembles.html","id":"try-it-yourself-make-an-ensemble-where-the-ensemble-is-made-using-trees-instead-of-linear-models.","chapter":"2 Introduction and your first ensembles","heading":"2.6.1 Try it yourself: Make an ensemble where the ensemble is made using trees instead of linear models.","text":"compare three results? Let’s update \nresults table","code":"\n# Fit the model to the training data\nensemble_tree <- tree(y ~ ., data = ensemble_train)\n\n# Make predictions using the model on the test data\nensemble_tree_predict <- predict(object = ensemble_tree, newdata = ensemble_test)\n\n# Let's look at the predictions\nhead(ensemble_tree_predict)\n#>      461      462      463      464      465      466 \n#> 14.80000 14.80000 18.94286 18.94286 18.94286 18.94286\n\n# Calculate the error rate\nensemble_tree_rmse <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_tree_predict)\n\nensemble_tree_rmse\n#> [1] 5.322011\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble_Linear', 'Ensemble_Tree'),\n  'Error' = c(Boston_linear_RMSE, Boston_tree_RMSE, ensemble_lm_rmse, ensemble_tree_rmse)\n)\n\nresults <- results %>% arrange(Error)\n\nresults\n#>             Model    Error\n#> 1 Ensemble_Linear 4.826962\n#> 2   Ensemble_Tree 5.322011\n#> 3            Tree 5.478017\n#> 4          Linear 6.108005"},{"path":"introduction-and-your-first-ensembles.html","id":"both-of-the-ensemble-models-beat-both-of-the-individual-models-in-this-example","chapter":"2 Introduction and your first ensembles","heading":"2.6.2 Both of the ensemble models beat both of the individual models in this example","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"principle-what-is-one-improvement-that-can-be-made-use-a-diverse-set-of-models-and-ensembles-to-get-the-best-possible-result","chapter":"2 Introduction and your first ensembles","heading":"2.7 Principle: What is one improvement that can be made? Use a diverse set of models and ensembles to get the best possible result","text":"shall see go learn build ensembles, \nnumerical method use build 27 individual models 13\nensembles total 40 results. goal get best\npossible results, diverse set models ensembles, 40\nresults numerical data, produce much better results \nlimited number models ensembles.principal looking classification\ndata, logistic, data, time series forecasting data. use \nlarge number individual models ensembles goal \nachieving best possible result.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"principle-randomizing-the-data-before-the-analysis-will-make-the-results-more-general-and-is-very-easy-to-do","chapter":"2 Introduction and your first ensembles","heading":"2.8 Principle: Randomizing the data before the analysis will make the results more general (and is very easy to do!)","text":"","code":"\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis"},{"path":"introduction-and-your-first-ensembles.html","id":"try-it-yourself-repeat-the-previous-analysis-but-randomize-the-rows-before-the-analysis.-otherwise-keep-the-process-the-same.-share-your-results-on-social-media.","chapter":"2 Introduction and your first ensembles","heading":"2.9 Try it yourself: Repeat the previous analysis, but randomize the rows before the analysis. Otherwise keep the process the same. Share your results on social media.","text":"’ll follow exact steps, except randomizing rows\nfirst.• Randomize rows• Break data train test sets• Fit model training set• Make predictions calculate error model test setThe fact results bit different first ensemble \nuseful. gives us another solid principle use analysis\nmethods:","code":"\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis\n\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\n# Fit the model to the training data\nBoston_lm <- lm(medv ~ ., data = train)\n\n# Make predictions using the model on the test data\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the linear model predictions:\n\nhead(Boston_lm_predictions)\n#>      240      332      137      454      334      270 \n#> 28.61158 19.88467 15.96199 22.47862 22.24022 24.87986\nBoston_linear_rmse <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions)\n\nBoston_tree <- tree(medv ~ ., data = train)\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\nBoston_tree_rmse <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions)\n\n# Let's have a quick look at the tree model predictions:\n\nhead(Boston_tree_predictions)\n#>      240      332      137      454      334      270 \n#> 27.91087 21.99000 16.96486 22.52000 21.99000 21.99000\nensemble <- data.frame( 'linear' = Boston_lm_predictions, 'tree' = Boston_tree_predictions, 'y_ensemble' = test$medv )\n\nensemble <- ensemble[sample(nrow(ensemble)), ] # Randomizes the rows of the ensemble\n\nensemble_train <- ensemble[1:60, ]\nensemble_test <- ensemble[61:105, ]\nensemble_lm <- lm(y_ensemble ~ ., data = ensemble_train)\n\n# Predictions for the ensemble linear model\n\nensemble_prediction <- predict(ensemble_lm, newdata = ensemble_test)\n\n# Root mean squared error for the ensemble linear model\n\nensemble_lm_rmse <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_prediction)\n\n# Same for tree models\n\nensemble_tree <- tree(y_ensemble ~ ., data = ensemble_train)\nensemble_tree_predictions <- predict(object = ensemble_tree, newdata = ensemble_test)\nensemble_tree_rmse <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_tree_predictions)\n\nresults <- list( 'Linear' = Boston_linear_rmse, 'Trees' = Boston_tree_rmse, 'Ensembles_Linear' = ensemble_lm_rmse, 'Ensemble_Tree' = ensemble_tree_rmse )\n\nresults\n#> $Linear\n#> [1] 5.773528\n#> \n#> $Trees\n#> [1] 5.217239\n#> \n#> $Ensembles_Linear\n#> [1] 3.496981\n#> \n#> $Ensemble_Tree\n#> [1] 5.648841"},{"path":"introduction-and-your-first-ensembles.html","id":"the-more-we-can-randomize-the-data-the-more-our-results-will-match-nature","chapter":"2 Introduction and your first ensembles","heading":"2.10 The more we can randomize the data, the more our results will match nature","text":"Just watch: Repeat results 100 times, return mean results\n(hint: ’s two small changes)","code":"\nfor (i in 1:100) {\n\n# First the linear model with randomized data\n\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis\n\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\nBoston_lm <- lm(medv ~ ., data = train)\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the linear model predictions:\n\nhead(Boston_lm_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_linear_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions)\n\nBoston_linear_rmse_mean <- mean(Boston_linear_rmse)\n\n# Let's use tree models\n\nBoston_tree <- tree(medv ~ ., data = train)\n\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\n\n# Let's have a quick look at the tree model predictions:\n\nhead(Boston_tree_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_tree_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions) \nBoston_tree_rmse_mean <- mean(Boston_tree_rmse)\n\nensemble <- data.frame('linear' = Boston_lm_predictions, 'tree' = Boston_tree_predictions, 'y_ensemble' = test$medv )\n\nensemble <- ensemble[sample(nrow(ensemble)), ] # Randomizes the rows of the ensemble\n\nensemble_train <- ensemble[1:60, ]\nensemble_test <- ensemble[61:105, ]\n\n# Ensemble linear modeling\n\nensemble_lm <- lm(y_ensemble ~ ., data = ensemble_train)\n\n# Predictions for the ensemble linear model\n\nensemble_prediction <- predict(ensemble_lm, newdata = ensemble_test)\n\n# Root mean squared error for the ensemble linear model\n\nensemble_lm_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_prediction)\n\nensemble_lm_rmse_mean <- mean(ensemble_lm_rmse)\n\nensemble_tree <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree, newdata = ensemble_test) \n\nensemble_tree_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = \nensemble_tree_predictions)\n\nensemble_tree_rmse_mean <- mean(ensemble_tree_rmse)\n\nresults <- data.frame(\n  'Linear' = Boston_linear_rmse_mean,\n  'Trees' = Boston_tree_rmse_mean,\n  'Ensembles_Linear' = ensemble_lm_rmse_mean,\n  'Ensemble_Tree' = ensemble_tree_rmse_mean )\n\n}\n\nresults\n#>     Linear    Trees Ensembles_Linear Ensemble_Tree\n#> 1 4.882039 4.644311         4.165575      5.139567\nwarnings() # No warnings!"},{"path":"introduction-and-your-first-ensembles.html","id":"principle-is-this-my-very-best-work","chapter":"2 Introduction and your first ensembles","heading":"2.11 Principle: “Is this my very best work?”","text":"best work build ensembles stage skills.\ngoing make number improvements solutions see\n, final result much stronger \nfar. Always strive best work, without excuses.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"where-do-i-get-help-with-errors-or-warnings","chapter":"2 Introduction and your first ensembles","heading":"2.12 “Where do I get help with errors or warnings?”","text":"extremely useful check code returns errors \nwarnings, fix fast possible. numerous sites \nhelp address errors code:https://stackoverflow.comhttps://forum.posit.cohttps://www.r-project.org/help.html","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"is-there-an-easy-way-to-save-all-trained-models","chapter":"2 Introduction and your first ensembles","heading":"2.13 Is there an easy way to save all trained models?","text":"Absolutely! simply add code end section \nsaves four trained models (linear, tree, ensemble_linear \nensemble_tree), follows:","code":"\nlibrary(MASS)\nlibrary(Metrics)\nlibrary(tree)\n\nensemble_lm_rmse <- 0\nensemble_tree_rmse <- 0\n\nfor (i in 1:100) {\n\n# Fit the linear model with randomized data\n\ndf <- df[sample(nrow(df)),] # Randomize the rows before the analysis\n\ntrain <- df[1:400, ]\ntest <- df[401:505, ]\n\nBoston_lm <- lm(medv ~ ., data = train)\n\nBoston_lm_predictions <- predict(object = Boston_lm, newdata = test)\n\n# Let's have a quick look at the linear model predictions:\n\nhead(Boston_lm_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_linear_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_lm_predictions) \nBoston_linear_rmse_mean <- mean(Boston_linear_rmse)\n\n# Let's use tree models\n\nBoston_tree <- tree(medv ~ ., data = train)\n\nBoston_tree_predictions <- predict(object = Boston_tree, newdata = test)\n\n# Let's have a quick look at the tree model predictions:\n\nhead(Boston_tree_predictions)\n\n# Let's calculate the root mean squared error rate of the predictions:\n\nBoston_tree_rmse[i] <- Metrics::rmse(actual = test$medv, predicted = Boston_tree_predictions) \nBoston_tree_rmse_mean <- mean(Boston_tree_rmse)\n\nensemble <- data.frame( 'linear' = Boston_lm_predictions, 'tree' = Boston_tree_predictions, 'y_ensemble' = test$medv )\n\nensemble <- ensemble[sample(nrow(ensemble)), ] # Randomizes the rows of the ensemble\n\nensemble_train <- ensemble[1:60, ]\n\nensemble_test <- ensemble[61:105, ]\n\n# Ensemble linear modeling\n\nensemble_lm <- lm(y_ensemble ~ ., data = ensemble_train)\n\n# Predictions for the ensemble linear model\n\nensemble_prediction <- predict(ensemble_lm, newdata = ensemble_test)\n\n# Root mean squared error for the ensemble linear model\n\nensemble_lm_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_prediction)\n\nensemble_lm_rmse_mean <- mean(ensemble_lm_rmse)\n\nensemble_tree <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree, newdata = ensemble_test) \n\nensemble_tree_rmse[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = ensemble_tree_predictions)\n\nensemble_tree_rmse_mean <- mean(ensemble_tree_rmse)\n\nresults <- list( 'Linear' = Boston_linear_rmse_mean, 'Trees' = Boston_tree_rmse_mean, 'Ensembles_Linear' = ensemble_lm_rmse_mean, 'Ensemble_Tree' = ensemble_tree_rmse_mean )\n\n}\n\nresults\n#> $Linear\n#> [1] 4.921173\n#> \n#> $Trees\n#> [1] 4.755993\n#> \n#> $Ensembles_Linear\n#> [1] 4.284075\n#> \n#> $Ensemble_Tree\n#> [1] 5.211208\nwarnings()\n\nBoston_lm <- Boston_lm\nBoston_tree <- Boston_tree\nensemble_lm <- ensemble_lm\nensemble_tree <- ensemble_tree"},{"path":"introduction-and-your-first-ensembles.html","id":"what-about-classification-logistic-and-time-series-data","chapter":"2 Introduction and your first ensembles","heading":"2.13.1 What about classification, logistic and time series data?","text":"subsequent chapters similar processes classification,\nlogistic time series data. ’s possible build ensembles \ntypes data. results extremely similar results\n’ve seen numerical data: ensembles won’t always\nbest results, best diverse set models \nensembles get best possible results.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"principle-ensembles-can-work-with-many-types-of-data-and-we-will-do-that-in-this-book","chapter":"2 Introduction and your first ensembles","heading":"2.13.2 Principle: Ensembles can work with many types of data, and we will do that in this book","text":"","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"can-it-make-predictions-on-totally-new-data-from-the-trained-modelsincluding-the-ensembles","chapter":"2 Introduction and your first ensembles","heading":"2.13.3 Can it make predictions on totally new data from the trained models—including the ensembles?","text":"solutions book independent use data. \nlook everything housing prices business analysis HR\nanalytics research medicine. One later examples \nexactly question asking—build individual ensemble\nmodels data, use pre-trained models make predictions\ntotally unseen data. develop set skills later \nbook, ’s minor extension ’re already seen \ncompleted.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"the-way-i-was-taught-how-to-write-code-was-totally-wrong-for-me-the-best-way-for-me-is-to-start-at-the-end-and-work-backward-from-there.-do-not-start-coding-looking-for-a-solution-instead-start-with-the-ending-and-work-backwards-from-there.","chapter":"2 Introduction and your first ensembles","heading":"2.13.4 The way I was taught how to write code was totally wrong for me: The best way for me is to start at the end and work backward from there. Do not start coding looking for a solution, instead, start with the ending and work backwards from there.","text":"Start end work backwards \nthereThe biggest lesson work make ensembles.\n’ve already seen steps, results \ncome. second biggest lesson everything taught \ndata science AI backwards actually works \nreal life. ’ve learned learn, applied skill\n(learning learn) wide range skills, including:• Running multi-million dollar division Fortune 1000 company,\nincluding full profit loss responsibility• Performing professional level many musical instruments• Able communicate English, Spanish sign language \nprofessional setting• Earning #1 place annual undergradate university mathematics\ncompetition—twice• Completing Master’s degree Guidance Counseling, allowing \nhelp many people path toward healthier life• Leader Oak Park, Illinois chapter Amnesty International \nten years, helping release several Prisoners Conscience• President Chicago Apple User Group ten years, helping many\npeople extremely good work hardware software• Leg press 1,000 pounds ten times row• Climbed mountain Colorado• Completed multiple skydives (looking forward )point learned learn, ’ve applied \nskill many areas. started learning data science/AI/coding, \ndifferent way creative whole life.\nway works start end, work backward \n, never give . Maybe best evidence success \nmethod fact:started write code led Ensembles package, \nfollowed steps: Start end, work backward , \nnever give . wound writing average 1,000 lines clean,\nerror free code per month 15 months. Ensembles package around\n15,000 lines clean, error free code.found attitude much important skill set, long\nshot.","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"how-i-stuck-with-it-all-the-way-to-the-end-the-best-career-advice-i-ever-received-was-from-a-homeless-man-i-never-met-and-answers-the-question-of-what-most-strongly-predicts-success.","chapter":"2 Introduction and your first ensembles","heading":"2.13.5 How I stuck with it all the way to the end: The best career advice I ever received was from a homeless man I never met, and answers the question of what most strongly predicts success.","text":"Ashford SimpsonLearning building ensembles help make accurate\npredictions. ’s extrdmely good skill setting. \nfound important thing predict success. \nstudied, quite good works subject, \nacademic general population.favorite career advice—listened nearly every day \nworked Ensembles project—man homeless \ntime came words.Nick Ashford Willow Run, Michigan. moved New York, hoping\nget entertainment world dancer. Unfortunately ended\nhomeless streets New York. slept park benches, \ngot food soup kitchens.heard people White Rock Baptist Church feed (\nhomeless man) normal meal, Nick went one Sunday morning. \nmet people, especially choir members, started working \npiano player choir. name Valerie Simpson.Soon Nick Valerie writing songs church choir. Nick\nmentioned homeless, realized New York wasn’t\ngoing “”. determined. words put say:Ain’t mountain high enoughAin’t valley low enoughAin’t river wide enoughValerie took words, set music. sent song \nMotown, released Marvin Gaye Tammy Terrell covering \nvocals. later re-done Ashford Simpson Paul Riser, \nDiana Ross singing lead.short video summarizes experience, concludes\nfinale 1970 version song. attitude \nAshford Simpson expressed song extremely highly predictive \nsuccess, matter field endeavor. found extremely\nmotivating, used overcome obstacles challenges \njourney.skill knowing learn (continue \nshare book), attitude working matter \nhigh mountain long valley wide river, gives \nkeep moving toward success, success fully\nachieved.Later look make presentations, consider \nexample level quality can done:https://www.icloud.com/iclouddrive/002bNfVreagRYCYHAZ9GyQ02w#Ain’t%5FNo%5FMountain%5FHigh%5FEnough","code":""},{"path":"introduction-and-your-first-ensembles.html","id":"exercises","chapter":"2 Introduction and your first ensembles","heading":"2.13.6 Exercises:","text":"Find data science Genesis. data science idea totally\nexcites gets bed every day. idea leads\ncreation many ideas. biggest boldest dreams\ncan possibly . idea strong \n. , benefit use \nreceive good create.Keep journal progress. ’s much easier see results\ntime record. Set journal today (\nweek). use Github journal. journal crazy\nideas, contradictory evidence, writing frustrations \nsuccesses, inspiration, one next thing worked , \nrock solid record path success. Seeing path \ntraversed huge motivation finishing project.best add journal entries regular schedule.Make ensemble using Boston Housing data set. Model \n13 columns data, median value home (14th\ncolumn) working chapter.Start planning comprehensive project. types data\ninterested ? patterns like \ndiscover? Begin looking online now possible data sets, \nlittle basic research. examples provided get\ncloser section book.","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3 Numerical data: How to make 23 individual models, and basic skills with functions","text":"begin building skills make ensembles \nmodels numerical data. However, going much easier \nmight appear first. Let’s see can make easy \npossible.work backwards make function need: Start endWe going start ending, beginning, work\nbackwards . method much, much easier working\nforward, see throughout book. might \nlittle uncomfortable first, skill allow complete\nwork faster rate work forward.’ll use Boston Housing data set, ’ll start Bagged\nRandom Forest function. now ’re going work one\nfunction, keep everything simple. essence, going run\nlike assembly line.want ending error rate model. Virtually customer\nwork going want know, “accurate ?” ’s \nstarting point.determine model accuracy? already previous\nchapter, finding root mean squared error individual models\nensemble models. ’re going steps , \nprocess familiar .get error rate model holdout data sets (test \nvalidation), ’re going need model (Bagged Random Forest \nfirst example), fit training data, use model make\npredictions test data. can measure error \npredictions, just . steps familiar \n. , please re-read previous chapter.need complete steps? ’re going go\nbackward (little) make function allow us work \ndata set.function need? Let’s make list:data (Boston housing)data (Boston housing)Column number (14, median value property)Column number (14, median value property)Train amountTrain amountTest amountTest amountValidation amountValidation amountNumber times resampleNumber times resampleOne key steps change name target variable\ny. initial name nearly anything, method changes\nname target variable y. allows us make one small\nchange allow easiest possible solution:","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"all-our-models-will-be-structured-the-same-way-y-.-data-train","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.1 All our models will be structured the same way: y ~ ., data = train","text":"means y (target value) function \nfeatures, data set training data set. \nvariations 27 models, basic structure \n.","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"having-the-same-structure-for-all-the-models-makes-it-much-easier-to-build-debug-and-deploy-the-completed-models.","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.2 Having the same structure for all the models makes it much easier to build, debug, and deploy the completed models.","text":"need start initial values, run.One extremely nice part creating models way enormous\nefficiency gives us. Bagged Random Forest model\nworking, able use similar (identical many\ncases!) processes models (Support Vector Machines).rock solid foundation lay beginning allow us \nsmooth easy experience foundation solid use \nbuild models. models mainly almost exact\nduplicates fist example.’steps follow:Load libraryLoad librarySet initial values 0Set initial values 0Create functionCreate functionSet random resamplingSet random resamplingBreak data train testBreak data train testFit model training data, make predictions measure\nerror test dataFit model training data, make predictions measure\nerror test dataReturn resultsReturn resultsCheck errors warningsCheck errors warningsTest different data setTest different data set","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"exercise-re-read-the-steps-above-how-we-will-work-backwards-to-come-up-with-the-function-we-need.","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.3 Exercise: Re-read the steps above how we will work backwards to come up with the function we need.","text":"","code":""},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bagged-random-forest","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.4 1. Bagged Random Forest","text":"Exercise: Try : Change values train, test \nvalidation, number resamples. See change \nresult.One : Find numerical data set, make bagged random\nforest function data set. (example, may use Auto\ndata set ISLR package. need remove last column,\nvehicle name. Model mpg function features using \nBagged Random Forest function, numerical data set work).Post: Share social first results making numerical function\n(screen shot/video optional stage, learning \nlater)example, “first data science function building making\nensembles later . Got everything run, errors. #AIEnsembles”Now build remaining 22 models numerical data. \nbuilt using structure, foundation.Now know build basic function, let’s build 22 \nsets tools need make ensemble, starting bagging:","code":"\nlibrary(e1071) # will allow us to use a tuned random forest model\nlibrary(Metrics) # Will allow us to calculate the root mean squared error\nlibrary(randomForest) # To use the random forest function\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\nlibrary(tidyverse) # Amazing set of tools for data science\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::combine()  masks randomForest::combine()\n#> ✖ dplyr::filter()   masks stats::filter()\n#> ✖ dplyr::lag()      masks stats::lag()\n#> ✖ ggplot2::margin() masks randomForest::margin()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n# Set initial values to 0. The function will return an error if any of these are left out.\n\nbag_rf_holdout_RMSE <- 0\nbag_rf_holdout_RMSE_mean <- 0\nbag_rf_train_RMSE <- 0\nbag_rf_test_RMSE <- 0\nbag_rf_validation_RMSE <- 0\n\n# Define the function\n\nnumerical_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n#Set up random resampling\n\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col())\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n#Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model to the training data, make predictions on the testing data, then calculate the error rates on the testing data sets.\nbag_rf_train_fit <- e1071::tune.randomForest(x = train, y = train$y, mtry = ncol(train) - 1)\nbag_rf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict( object = bag_rf_train_fit$best.model, newdata = train))\nbag_rf_train_RMSE_mean <- mean(bag_rf_train_RMSE)\nbag_rf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict( object = bag_rf_train_fit$best.model, newdata = test))\nbag_rf_test_RMSE_mean <- mean(bag_rf_test_RMSE)\n\n# Itemize the error on the holdout data sets, and calculate the mean of the results\nbag_rf_holdout_RMSE[i] <- mean(bag_rf_test_RMSE_mean)\nbag_rf_holdout_RMSE_mean <- mean(c(bag_rf_holdout_RMSE))\n\n# These are the predictions we will need when we make the ensembles\nbag_rf_test_predict_value <- as.numeric(predict(object = bag_rf_train_fit$best.model, newdata = test))\n\n\n# Return the mean of the results to the user\n\n} # closing brace for numresamples\n  return(bag_rf_holdout_RMSE_mean)\n\n} # closing brace for numerical_1 function\n\n# Here is our first numerical function in actual use. We will use 25 resamples\n\nnumerical_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 0.3023503\nwarnings() # no warnings, the best possible result"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bagging-bootstrap-aggregating","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.5 2. Bagging (bootstrap aggregating)","text":"","code":"\nlibrary(ipred) #for the bagging function\n\n# Set initial values to 0\nbagging_train_RMSE <- 0\nbagging_test_RMSE <- 0\nbagging_validation_RMSE <- 0\nbagging_holdout_RMSE <- 0\nbagging_test_predict_value <- 0\nbagging_validation_predict_value <- 0\n\n#Create the function:\n\nbagging_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\n\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model to the training data, calculate error, make predictions on the holdout data\n\nbagging_train_fit <- ipred::bagging(formula = y ~ ., data = train)\nbagging_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bagging_train_fit, newdata = train))\nbagging_train_RMSE_mean <- mean(bagging_train_RMSE)\nbagging_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bagging_train_fit, newdata = test))\nbagging_test_RMSE_mean <- mean(bagging_test_RMSE)\nbagging_holdout_RMSE[i] <- mean(bagging_test_RMSE_mean)\nbagging_holdout_RMSE_mean <- mean(bagging_holdout_RMSE)\ny_hat_bagging <- c(bagging_test_predict_value)\n\n} # closing braces for the resampling function\n  return(bagging_holdout_RMSE_mean)\n  \n} # closing braces for the bagging function\n\n# Test the function:\nbagging_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.20, numresamples = 25)\n#> [1] 4.378991\nwarnings() # no warnings"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bayesglm","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.6 3. BayesGLM","text":"","code":"\nlibrary(arm) # to use bayesglm function\n#> Loading required package: MASS\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\n\n# Set initial values to 0\nbayesglm_train_RMSE <- 0\nbayesglm_test_RMSE <- 0\nbayesglm_validation_RMSE <- 0\nbayesglm_holdout_RMSE <- 0\nbayesglm_test_predict_value <- 0\nbayesglm_validation_predict_value <- 0\n\n# Create the function:\nbayesglm_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n#Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n#Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = gaussian(link = \"identity\"))\nbayesglm_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesglm_train_fit, newdata = train))\nbayesglm_train_RMSE_mean <- mean(bayesglm_train_RMSE)\nbayesglm_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesglm_train_fit, newdata = test))\nbayesglm_test_RMSE_mean <- mean(bayesglm_test_RMSE) \ny_hat_bayesglm <- c(bayesglm_test_predict_value)\n\n} # closing braces for resampling\n  return(bayesglm_test_RMSE_mean)\n  \n} # closing braces for the function\n\nbayesglm_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.20, numresamples = 25)\n#> [1] 4.843296\nwarnings() # no warnings"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"bayesrnn","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.7 4. BayesRNN","text":"","code":"\nlibrary(brnn) # so we can use the BayesRNN function\n#> Loading required package: Formula\n#> Loading required package: truncnorm\n\n#Set initial values to 0\n\nbayesrnn_train_RMSE <- 0\nbayesrnn_test_RMSE <- 0\nbayesrnn_validation_RMSE <- 0\nbayesrnn_holdout_RMSE <- 0\nbayesrnn_test_predict_value <- 0\nbayesrnn_validation_predict_value <- 0\n\n# Create the function:\n\nbayesrnn_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model on the training data, make predictions on the testing data\nbayesrnn_train_fit <- brnn::brnn(x = as.matrix(train), y = train$y)\nbayesrnn_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesrnn_train_fit, newdata = train))\nbayesrnn_train_RMSE_mean <- mean(bayesrnn_train_RMSE)\nbayesrnn_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesrnn_train_fit, newdata = test))\nbayesrnn_test_RMSE_mean <- mean(bayesrnn_test_RMSE)\n\ny_hat_bayesrnn <- c(bayesrnn_test_predict_value)\n\n} # Closing brace for number of resamples \n  return(bayesrnn_test_RMSE_mean)\n\n} # Closing brace for the function\n\nbayesrnn_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.701572 \n#> gamma= 31.2244    alpha= 3.0586   beta= 38683.82 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015275 \n#> gamma= 29.2045    alpha= 2.067    beta= 15308.9 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016356 \n#> gamma= 30.3801    alpha= 2.9574   beta= 50623.19 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016411 \n#> gamma= 29.6522    alpha= 2.1858   beta= 13789.78 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016411 \n#> gamma= 30.9251    alpha= 3.9807   beta= 13284.05 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015619 \n#> gamma= 30.4682    alpha= 5.0886   beta= 16331.64 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016085 \n#> gamma= 31.192     alpha= 5.1146   beta= 19552.41 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015669 \n#> gamma= 30.1789    alpha= 4.3      beta= 35285.68 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015669 \n#> gamma= 31.3872    alpha= 3.8202   beta= 15992.69 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7017538 \n#> gamma= 31.2203    alpha= 5.3491   beta= 13244.21 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015771 \n#> gamma= 31.0916    alpha= 4.1637   beta= 15897.53 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016085 \n#> gamma= 31.5276    alpha= 4.968    beta= 17564.28 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016636 \n#> gamma= 31.0946    alpha= 5.251    beta= 15223.9 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015771 \n#> gamma= 31.4853    alpha= 4.6814   beta= 15895.73 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015619 \n#> gamma= 31.5488    alpha= 4.4819   beta= 14880.51 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.701542 \n#> gamma= 31.2898    alpha= 3.9508   beta= 15560.15 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016926 \n#> gamma= 31.6292    alpha= 4.1742   beta= 13535.13 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015275 \n#> gamma= 29.9586    alpha= 3.9612   beta= 38416.76 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015227 \n#> gamma= 31.4039    alpha= 5.6172   beta= 15691.66 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016246 \n#> gamma= 31.3544    alpha= 5.3271   beta= 15780.51 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016138 \n#> gamma= 31.3198    alpha= 5.7416   beta= 15094.68 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016246 \n#> gamma= 31.0694    alpha= 5.2206   beta= 21175.37 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.701735 \n#> gamma= 31.4069    alpha= 5.6578   beta= 14047.93 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016579 \n#> gamma= 31.289     alpha= 5.201    beta= 13275.09 \n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016411 \n#> gamma= 30.9163    alpha= 4.1354   beta= 25590.17\n#> [1] 0.1331119\n\nwarnings() # no warnings for BayesRNN function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"boosted-random-forest","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.8 5. Boosted Random Forest","text":"","code":"\nlibrary(e1071)\nlibrary(randomForest)\nlibrary(tidyverse)\n\n#Set initial values to 0\nboost_rf_train_RMSE <- 0\nboost_rf_test_RMSE <- 0\nboost_rf_validation_RMSE <- 0\nboost_rf_holdout_RMSE <- 0\nboost_rf_test_predict_value <- 0\nboost_rf_validation_predict_value <- 0\n\n#Create the function:\nboost_rf_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n#Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit boosted random forest model on the training data, make predictions on holdout data\n\nboost_rf_train_fit <- e1071::tune.randomForest(x = train, y = train$y, mtry = ncol(train) - 1)\nboost_rf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict( object = boost_rf_train_fit$best.model, newdata = train\n  ))\nboost_rf_train_RMSE_mean <- mean(boost_rf_train_RMSE)\nboost_rf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict( object = boost_rf_train_fit$best.model, newdata = test\n  ))\nboost_rf_test_RMSE_mean <- mean(boost_rf_test_RMSE)\n\n} # closing brace for numresamples\n  return(boost_rf_test_RMSE_mean)\n  \n} # closing brace for the function\n\nboost_rf_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 0.3040725\nwarnings() # no warnings for Boosted Random Forest function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"cubist","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.9 6. Cubist","text":"","code":"\nlibrary(Cubist)\n#> Loading required package: lattice\nlibrary(tidyverse)\n\n# Set initial values to 0\n\ncubist_train_RMSE <- 0\ncubist_test_RMSE <- 0\ncubist_validation_RMSE <- 0\ncubist_holdout_RMSE <- 0\ncubist_test_predict_value <- 0\n\n# Create the function:\n\ncubist_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n#Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Fit the model on the training data, make predictions on the holdout data\ncubist_train_fit <- Cubist::cubist(x = train[, 1:ncol(train) - 1], y = train$y)\ncubist_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = cubist_train_fit, newdata = train))\ncubist_train_RMSE_mean <- mean(cubist_train_RMSE)\ncubist_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = cubist_train_fit, newdata = test))\ncubist_test_RMSE_mean <- mean(cubist_test_RMSE)\n\n} # closing braces for numresamples\n  return(cubist_test_RMSE_mean)\n  \n} # closing braces for the function\n\ncubist_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.383964\nwarnings() # no warnings for individual cubist function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"elastic","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.10 7. Elastic","text":"","code":"\n\nlibrary(glmnet) # So we can run the elastic model\n#> Loaded glmnet 4.1-8\nlibrary(tidyverse)\n\n# Set initial values to 0\n\nelastic_train_RMSE <- 0\nelastic_test_RMSE <- 0\nelastic_validation_RMSE <- 0\nelastic_holdout_RMSE <- 0\nelastic_test_predict_value <- 0\nelastic_validation_predict_value <- 0\nelastic_test_RMSE <- 0\nelastic_test_RMSE_df <- data.frame(elastic_test_RMSE)\nelastic_validation_RMSE <- 0\nelastic_validation_RMSE_df <- data.frame(elastic_validation_RMSE)\nelastic_holdout_RMSE <- 0\nelastic_holdout_RMSE_df <- data.frame(elastic_holdout_RMSE)\n\n# Create the function:\nelastic_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\n# Set up the elastic model\n\ny <- train$y\nx <- data.matrix(train %>% dplyr::select(-y))\nelastic_model <- glmnet::glmnet(x, y, alpha = 0.5)\nelastic_cv <- cv.glmnet(x, y, alpha = 0.5)\nbest_elastic_lambda <- elastic_cv$lambda.min\nbest_elastic_model <- glmnet::glmnet(x, y, alpha = 0, lambda = best_elastic_lambda)\nelastic_test_pred <- predict(best_elastic_model, s = best_elastic_lambda, newx = data.matrix(test %>% dplyr::select(-y)))\n\nelastic_test_RMSE <- Metrics::rmse(actual = test$y, predicted = elastic_test_pred)\nelastic_test_RMSE_df <- rbind(elastic_test_RMSE_df, elastic_test_RMSE)\nelastic_test_RMSE_mean <- mean(elastic_test_RMSE_df$elastic_test_RMSE[2:nrow(elastic_test_RMSE_df)])\n\nelastic_holdout_RMSE <- mean(elastic_test_RMSE_mean)\nelastic_holdout_RMSE_df <- rbind(elastic_holdout_RMSE_df, elastic_holdout_RMSE)\nelastic_holdout_RMSE_mean <- mean(elastic_holdout_RMSE_df$elastic_holdout_RMSE[2:nrow(elastic_holdout_RMSE_df)])\n\n} # closing brace for numresample\n  return(elastic_holdout_RMSE_mean)\n  \n} # closing brace for the elastic function\n\nelastic_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.926706\nwarnings() # no warnings for individual elastic function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"generalized-additive-models-with-smoothing-splines","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.11 8. Generalized Additive Models with smoothing splines","text":"","code":"\nlibrary(gam) # for fitting generalized additive models\n#> Loading required package: splines\n#> Loading required package: foreach\n#> \n#> Attaching package: 'foreach'\n#> The following objects are masked from 'package:purrr':\n#> \n#>     accumulate, when\n#> Loaded gam 1.22-3\n\n# Set initial values to 0\n\ngam_train_RMSE <- 0\ngam_test_RMSE <- 0\ngam_holdout_RMSE <- 0\ngam_test_predict_value <- 0\n\n# Create the function:\ngam1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Set up random resampling\n\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\n# Set up to fit the model on the training data\n\nn_unique_vals <- purrr::map_dbl(df, dplyr::n_distinct)\n\n# Names of columns with >= 4 unique vals\nkeep <- names(n_unique_vals)[n_unique_vals >= 4]\n\ngam_data <- df %>% dplyr::select(dplyr::all_of(keep))\n\n# Model data\n\ntrain1 <- train %>% dplyr::select(dplyr::all_of(keep))\n\ntest1 <- test %>% dplyr::select(dplyr::all_of(keep))\n\nnames_df <- names(gam_data[, 1:ncol(gam_data) - 1])\nf2 <- stats::as.formula(paste0(\"y ~\", paste0(\"gam::s(\", names_df, \")\", collapse = \"+\")))\n\ngam_train_fit <- gam::gam(f2, data = train1)\ngam_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = gam_train_fit, newdata = train))\ngam_train_RMSE_mean <- mean(gam_train_RMSE)\ngam_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = gam_train_fit, newdata = test))\ngam_test_RMSE_mean <- mean(gam_test_RMSE)\ngam_holdout_RMSE[i] <- mean(gam_test_RMSE_mean)\ngam_holdout_RMSE_mean <- mean(gam_holdout_RMSE)\n\n} # closing braces for numresamples\n  return(gam_holdout_RMSE_mean)\n  \n} # closing braces for gam function\n\ngam1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.880676\nwarnings() # no warnings for individual gam function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"gradient-boosted","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.12 9. Gradient Boosted","text":"","code":"\nlibrary(gbm) # to allow use of gradient boosted models\n#> Loaded gbm 2.1.9\n#> This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\n# Set initial values to 0\ngb_train_RMSE <- 0\ngb_test_RMSE <- 0\ngb_validation_RMSE <- 0\ngb_holdout_RMSE <- 0\ngb_test_predict_value <- 0\ngb_validation_predict_value <- 0\n\ngb1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\ngb_train_fit <- gbm::gbm(train$y ~ ., data = train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\ngb_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = gb_train_fit, newdata = train))\ngb_train_RMSE_mean <- mean(gb_train_RMSE)\ngb_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = gb_train_fit, newdata = test))\ngb_test_RMSE_mean <- mean(gb_test_RMSE)\n\n} # closing brace for numresamples\n  return(gb_test_RMSE_mean)\n  \n} # closing brace for gb1 function\n\ngb1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> Using 100 trees...\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> [1] 3.527798\nwarnings() # no warnings for individual gradient boosted function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"k-nearest-neighbors-tuned","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.13 10. K-Nearest Neighbors (tuned)","text":"","code":"\n\nlibrary(e1071)\n\n# Set initial values to 0\nknn_train_RMSE <- 0\nknn_test_RMSE <- 0\nknn_validation_RMSE <- 0\nknn_holdout_RMSE <- 0\nknn_test_predict_value <- 0\nknn_validation_predict_value <- 0\n\nknn1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nknn_train_fit <- e1071::tune.gknn(x = train[, 1:ncol(train) - 1], y = train$y, scale = TRUE, k = c(1:25))\nknn_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict( object = knn_train_fit$best.model,\n    newdata = train[, 1:ncol(train) - 1], k = knn_train_fit$best_model$k))\nknn_train_RMSE_mean <- mean(knn_train_RMSE)\nknn_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict( object = knn_train_fit$best.model,\n    k = knn_train_fit$best_model$k, newdata = test[, 1:ncol(test) - 1]))\nknn_test_RMSE_mean <- mean(knn_test_RMSE)\nknn_holdout_RMSE[i] <- mean(c(knn_test_RMSE_mean))\nknn_holdout_RMSE_mean <- mean(knn_holdout_RMSE)\n\n} # closing brace for numresamples\n  return(knn_holdout_RMSE_mean)\n  \n} # closing brace for knn1 function\n\nknn1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 6.64997\nwarnings() # no warnings for individual knn function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"lasso","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.14 11. Lasso","text":"","code":"\nlibrary(glmnet) # So we can run the lasso model\n\n# Set initial values to 0\n\nlasso_train_RMSE <- 0\nlasso_test_RMSE <- 0\nlasso_validation_RMSE <- 0\nlasso_holdout_RMSE <- 0\nlasso_test_predict_value <- 0\nlasso_validation_predict_value <- 0\nlasso_test_RMSE <- 0\nlasso_test_RMSE_df <- data.frame(lasso_test_RMSE)\nlasso_validation_RMSE <- 0\nlasso_validation_RMSE_df <- data.frame(lasso_validation_RMSE)\nlasso_holdout_RMSE <- 0\nlasso_holdout_RMSE_df <- data.frame(lasso_holdout_RMSE)\n\n# Create the function:\nlasso_1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Set up the lasso model\ny <- train$y\nx <- data.matrix(train %>% dplyr::select(-y))\nlasso_model <- glmnet::glmnet(x, y, alpha = 1.0)\nlasso_cv <- cv.glmnet(x, y, alpha = 1.0)\nbest_lasso_lambda <- lasso_cv$lambda.min\nbest_lasso_model <- glmnet::glmnet(x, y, alpha = 0, lambda = best_lasso_lambda)\nlasso_test_pred <- predict(best_lasso_model, s = best_lasso_lambda, newx = data.matrix(test %>% dplyr::select(-y)))\n\nlasso_test_RMSE <- Metrics::rmse(actual = test$y, predicted = lasso_test_pred)\nlasso_test_RMSE_df <- rbind(lasso_test_RMSE_df, lasso_test_RMSE)\nlasso_test_RMSE_mean <- mean(lasso_test_RMSE_df$lasso_test_RMSE[2:nrow(lasso_test_RMSE_df)])\n\nlasso_holdout_RMSE <- mean(lasso_test_RMSE_mean)\nlasso_holdout_RMSE_df <- rbind(lasso_holdout_RMSE_df, lasso_holdout_RMSE)\nlasso_holdout_RMSE_mean <- mean(lasso_holdout_RMSE_df$lasso_holdout_RMSE[2:nrow(lasso_holdout_RMSE_df)])\n\n} # closing brace for numresample\n  return(lasso_holdout_RMSE_mean)\n  \n} # closing brace for the lasso_1 function\n\nlasso_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.843122\nwarnings() # no warnings for individual lasso function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"linear-tuned","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.15 12. Linear (tuned)","text":"","code":"\n\nlibrary(e1071) # for tuned linear models\n\n# Set initial values to 0\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_holdout_RMSE <- 0\n\n# Set up the function\nlinear1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nlinear_train_fit <- e1071::tune.rpart(formula = y ~ ., data = train)\nlinear_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = linear_train_fit$best.model, newdata = train))\nlinear_train_RMSE_mean <- mean(linear_train_RMSE)\nlinear_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = linear_train_fit$best.model, newdata = test))\nlinear_holdout_RMSE_mean <- mean(linear_test_RMSE)\n\n} # closing brace for numresamples\n  return(linear_holdout_RMSE_mean)\n  \n} # closing brace for linear1 function\n\nlinear1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.798766\nwarnings() # no warnings for individual lasso function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"lqs","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.16 13. LQS","text":"","code":"\n\nlibrary(MASS) # to allow us to run LQS models\n\n# Set initial values to 0\n\nlqs_train_RMSE <- 0\nlqs_test_RMSE <- 0\nlqs_validation_RMSE <- 0\nlqs_holdout_RMSE <- 0\nlqs_test_predict_value <- 0\nlqs_validation_predict_value <- 0\n\nlqs1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nlqs_train_fit <- MASS::lqs(train$y ~ ., data = train)\nlqs_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = lqs_train_fit, newdata = train))\nlqs_train_RMSE_mean <- mean(lqs_train_RMSE)\nlqs_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = lqs_train_fit, newdata = test))\nlqs_test_RMSE_mean <- mean(lqs_test_RMSE)\n\ny_hat_lqs <- c(lqs_test_predict_value, lqs_validation_predict_value)\n\n} # Closing brace for numresamples\n    return(lqs_test_RMSE_mean)\n\n} # Closing brace for lqs1 function\n\nlqs1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 7.011302\nwarnings() # no warnings for individual lqs function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"neuralnet","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.17 14. Neuralnet","text":"","code":"\nlibrary(neuralnet)\n#> \n#> Attaching package: 'neuralnet'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     compute\n\n#Set initial values to 0\n\nneuralnet_train_RMSE <- 0\nneuralnet_test_RMSE <- 0\nneuralnet_validation_RMSE <- 0\nneuralnet_holdout_RMSE <- 0\nneuralnet_test_predict_value <- 0\nneuralnet_validation_predict_value <- 0\n\n# Fit the model to the training data\nneuralnet1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test data sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nmaxs <- apply(df, 2, max)\nmins <- apply(df, 2, min)\nscaled <- as.data.frame(scale(df, center = mins, scale = maxs - mins))\ntrain_ <- scaled[idx == 1, ]\ntest_ <- scaled[idx == 2, ]\nn <- names(train_)\nf <- as.formula(paste(\"y ~\", paste(n[!n %in% \"y\"], collapse = \" + \")))\nnn <- neuralnet(f, data = train_, hidden = c(5, 3), linear.output = TRUE)\npredict_test_nn <- neuralnet::compute(nn, test_[, 1:ncol(df) - 1])\npredict_test_nn_ <- predict_test_nn$net.result * (max(df$y) - min(df$y)) + min(df$y)\npredict_train_nn <- neuralnet::compute(nn, train_[, 1:ncol(df) - 1])\npredict_train_nn_ <- predict_train_nn$net.result * (max(df$y) - min(df$y)) + min(df$y)\nneuralnet_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict_train_nn_)\nneuralnet_train_RMSE_mean <- mean(neuralnet_train_RMSE)\nneuralnet_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict_test_nn_)\nneuralnet_test_RMSE_mean <- mean(neuralnet_test_RMSE)\n\nneuralnet_holdout_RMSE[i] <- mean(c(neuralnet_test_RMSE))\nneuralnet_holdout_RMSE_mean <- mean(neuralnet_holdout_RMSE)\n\n} # Closing brace for numresamples\n  return(neuralnet_holdout_RMSE_mean)\n  \n} # closing brace for neuralnet1 function\n\nneuralnet1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.140954\nwarnings() # no warnings for individual neuralnet function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"partial-least-squares","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.18 15. Partial Least Squares","text":"","code":"\n\nlibrary(pls)\n#> \n#> Attaching package: 'pls'\n#> The following objects are masked from 'package:arm':\n#> \n#>     coefplot, corrplot\n#> The following object is masked from 'package:stats':\n#> \n#>     loadings\n\n# Set initial values to 0\npls_train_RMSE <- 0\npls_test_RMSE <- 0\npls_validation_RMSE <- 0\npls_holdout_RMSE <- 0\npls_test_predict_value <- 0\npls_validation_predict_value <- 0\n\npls1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\npls_train_fit <- pls::plsr(train$y ~ ., data = train)\npls_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = pls_train_fit, newdata = train))\npls_train_RMSE_mean <- mean(pls_train_RMSE)\npls_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = pls_train_fit, newdata = test))\npls_test_RMSE_mean <- mean(pls_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return( pls_test_RMSE_mean)\n  \n} # Closing brace for pls1 function\n\npls1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 6.106539\nwarnings() # no warnings for individual pls function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"principal-components-regression","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.19 16. Principal Components Regression","text":"","code":"\n\nlibrary(pls) # To run pcr models\n\n#Set initial values to 0\npcr_train_RMSE <- 0\npcr_test_RMSE <- 0\npcr_validation_RMSE <- 0\npcr_holdout_RMSE <- 0\npcr_test_predict_value <- 0\npcr_validation_predict_value <- 0\n\npcr1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\npcr_train_fit <- pls::pcr(train$y ~ ., data = train)\npcr_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = pcr_train_fit, newdata = train))\npcr_train_RMSE_mean <- mean(pcr_train_RMSE)\npcr_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = pcr_train_fit, newdata = test))\npcr_test_RMSE_mean <- mean(pcr_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return(pcr_test_RMSE_mean)\n  \n} # Closing brace for PCR function\n\npcr1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 6.715986\nwarnings() # no warnings for individual pls function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"random-forest","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.20 17. Random Forest","text":"","code":"\nlibrary(randomForest)\n\n# Set initial values to 0\nrf_train_RMSE <- 0\nrf_test_RMSE <- 0\nrf_validation_RMSE <- 0\nrf_holdout_RMSE <- 0\nrf_test_predict_value <- 0\nrf_validation_predict_value <- 0\n\n# Set up the function\nrf1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n#Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nrf_train_fit <- tune.randomForest(x = train, y = train$y, data = train)\nrf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = rf_train_fit$best.model, newdata = train))\nrf_train_RMSE_mean <- mean(rf_train_RMSE)\nrf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = rf_train_fit$best.model, newdata = test))\nrf_test_RMSE_mean <- mean(rf_test_RMSE)\n\n} # Closing brace for numresamples loop\nreturn(rf_test_RMSE_mean)\n  \n} # Closing brace for rf1 function\n\nrf1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 1.779677\nwarnings() # no warnings for individual random forest function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"ridge-regression","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.21 18. Ridge Regression","text":"","code":"\n\nlibrary(glmnet) # So we can run the ridge model\n\n# Set initial values to 0\nridge_train_RMSE <- 0\nridge_test_RMSE <- 0\nridge_validation_RMSE <- 0\nridge_holdout_RMSE <- 0\nridge_test_predict_value <- 0\nridge_validation_predict_value <- 0\nridge_test_RMSE <- 0\nridge_test_RMSE_df <- data.frame(ridge_test_RMSE)\nridge_validation_RMSE <- 0\nridge_validation_RMSE_df <- data.frame(ridge_validation_RMSE)\nridge_holdout_RMSE <- 0\nridge_holdout_RMSE_df <- data.frame(ridge_holdout_RMSE)\n\n# Create the function:\nridge1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train, test and validation sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Set up the ridge model\ny <- train$y\nx <- data.matrix(train %>% dplyr::select(-y))\nridge_model <- glmnet::glmnet(x, y, alpha = 0)\nridge_cv <- cv.glmnet(x, y, alpha = 0)\nbest_ridge_lambda <- ridge_cv$lambda.min\nbest_ridge_model <- glmnet::glmnet(x, y, alpha = 0, lambda = best_ridge_lambda)\nridge_test_pred <- predict(best_ridge_model, s = best_ridge_lambda, newx = data.matrix(test %>% dplyr::select(-y)))\n\nridge_test_RMSE <- Metrics::rmse(actual = test$y, predicted = ridge_test_pred)\nridge_test_RMSE_df <- rbind(ridge_test_RMSE_df, ridge_test_RMSE)\nridge_test_RMSE_mean <- mean(ridge_test_RMSE_df$ridge_test_RMSE[2:nrow(ridge_test_RMSE_df)])\n\nridge_holdout_RMSE <- mean(ridge_test_RMSE_mean)\nridge_holdout_RMSE_df <- rbind(ridge_holdout_RMSE_df, ridge_holdout_RMSE)\nridge_holdout_RMSE_mean <- mean(ridge_holdout_RMSE_df$ridge_holdout_RMSE[2:nrow(ridge_holdout_RMSE_df)])\n\n} # closing brace for numresample\n  return(ridge_holdout_RMSE_mean)\n  \n} # closing brace for the ridge function\n\nridge1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 5.043585\nwarnings() # no warnings for individual ridge function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"robust-regression","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.22 19. Robust Regression","text":"","code":"\n\nlibrary(MASS) # To run rlm function for robust regression\n\n# Set initial values to 0\nrobust_train_RMSE <- 0\nrobust_test_RMSE <- 0\nrobust_validation_RMSE <- 0\nrobust_holdout_RMSE <- 0\nrobust_test_predict_value <- 0\nrobust_validation_predict_value <- 0\n\n# Make the function\nrobust1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nrobust_train_fit <- MASS::rlm(x = train[, 1:ncol(df) - 1], y = train$y)\nrobust_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = robust_train_fit$fitted.values)\nrobust_train_RMSE_mean <- mean(robust_train_RMSE)\nrobust_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = MASS::rlm(y ~ ., data = train), newdata = test))\nrobust_test_RMSE_mean <- mean(robust_test_RMSE) \n\n} # Closing brace for numresamples loop\nreturn(robust_test_RMSE_mean)\n  \n} # Closing brace for robust1 function\n\nrobust1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> Warning in rlm.default(x = train[, 1:ncol(df) - 1], y =\n#> train$y): 'rlm' failed to converge in 20 steps\n\n#> Warning in rlm.default(x = train[, 1:ncol(df) - 1], y =\n#> train$y): 'rlm' failed to converge in 20 steps\n#> [1] 4.952534\nwarnings() # no warnings for individual robust function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"rpart","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.23 20. Rpart","text":"","code":"\n\nlibrary(rpart)\n\n# Set initial values to 0\nrpart_train_RMSE <- 0\nrpart_test_RMSE <- 0\nrpart_validation_RMSE <- 0\nrpart_holdout_RMSE <- 0\nrpart_test_predict_value <- 0\nrpart_validation_predict_value <- 0\n\n# Make the function\nrpart1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nrpart_train_fit <- rpart::rpart(train$y ~ ., data = train)\nrpart_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = rpart_train_fit, newdata = train))\nrpart_train_RMSE_mean <- mean(rpart_train_RMSE)\nrpart_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = rpart_train_fit, newdata = test))\nrpart_test_RMSE_mean <- mean(rpart_test_RMSE)\n\n} # Closing loop for numresamples\nreturn(rpart_test_RMSE_mean)\n  \n} # Closing brace for rpart1 function\n\nrpart1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 4.919528\nwarnings() # no warnings for individual rpart function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"support-vector-machines","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.24 21. Support Vector Machines","text":"","code":"\n\nlibrary(e1071)\n\n# Set initial values to 0\nsvm_train_RMSE <- 0\nsvm_test_RMSE <- 0\nsvm_validation_RMSE <- 0\nsvm_holdout_RMSE <- 0\nsvm_test_predict_value <- 0\nsvm_validation_predict_value <- 0\n\n# Make the function\nsvm1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\nsvm_train_fit <- e1071::tune.svm(x = train, y = train$y, data = train)\nsvm_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = svm_train_fit$best.model, newdata = train))\nsvm_train_RMSE_mean <- mean(svm_train_RMSE)\nsvm_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = svm_train_fit$best.model, newdata = test))\nsvm_test_RMSE_mean <- mean(svm_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return(svm_test_RMSE_mean)\n\n} # Closing brace for svm1 function\n\nsvm1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 2.288686\nwarnings() # no warnings for individual Support Vector Machines function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"trees","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.25 22. Trees","text":"","code":"\n\nlibrary(tree)\n\n# Set initial values to 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_validation_RMSE <- 0\ntree_holdout_RMSE <- 0\ntree_test_predict_value <- 0\ntree_validation_predict_value <- 0\n\n# Make the function\ntree1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\ntree_train_fit <- tree::tree(train$y ~ ., data = train)\ntree_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = tree_train_fit, newdata = train))\ntree_train_RMSE_mean <- mean(tree_train_RMSE)\ntree_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = tree_train_fit, newdata = test))\ntree_test_RMSE_mean <- mean(tree_test_RMSE)\n\n} # Closing brace for numresamples loop\n  return(tree_test_RMSE_mean)\n  \n} # Closing brace for tree1 function\n\ntree1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1] 5.019375\nwarnings() # no warnings for individual tree function"},{"path":"numerical-data-how-to-make-23-individual-models-and-basic-skills-with-functions.html","id":"xgboost","chapter":"3 Numerical data: How to make 23 individual models, and basic skills with functions","heading":"3.0.26 23. XGBoost","text":"","code":"\nlibrary(xgboost)\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\n\n# Set initial values to 0\nxgb_train_RMSE <- 0\nxgb_test_RMSE <- 0\nxgb_validation_RMSE <- 0\nxgb_holdout_RMSE <- 0\nxgb_test_predict_value <- 0\nxgb_validation_predict_value <- 0\n\n# Create the function\nxgb1 <- function(data, colnum, train_amount, test_amount, validation_amount, numresamples){\n\n# Set up random resampling\nfor (i in 1:numresamples) {\n\n# Changes the name of the target column to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Moves the target column to the last column on the right\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ] # randomizes the rows\n\n# Breaks the data into train and test sets\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1,]\ntest <- df[idx == 2, ]\n\ntrain_x <- data.matrix(train[, -ncol(train)])\ntrain_y <- train[, ncol(train)]\n\n# define predictor and response variables in test set\ntest_x <- data.matrix(test[, -ncol(test)])\ntest_y <- test[, ncol(test)]\n\n# define final train, test and validation sets\n\nxgb_train <- xgboost::xgb.DMatrix(data = train_x, label = train_y)\nxgb_test <- xgboost::xgb.DMatrix(data = test_x, label = test_y)\n\n# define watchlist\nwatchlist <- list(train = xgb_train)\nwatchlist_test <- list(train = xgb_train, test = xgb_test)\n\n# fit XGBoost model and display training and validation data at each round\nxgb_model <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_test, nrounds = 70)\n\nxgboost_min <- which.min(xgb_model$evaluation_log$validation_rmse)\n\nxgb_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = xgb_model, newdata = train_x))\nxgb_train_RMSE_mean <- mean(xgb_train_RMSE)\nxgb_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = xgb_model, newdata = test_x))\nxgb_test_RMSE_mean <- mean(xgb_test_RMSE)\n\nxgb_holdout_RMSE[i] <- mean(xgb_test_RMSE_mean)\nxgb_holdout_RMSE_mean <- mean(xgb_holdout_RMSE)\n\n} # Closing brace for numresamples loop\n  return(xgb_holdout_RMSE_mean)\n  \n} # Closing brace for xgb1 function\n\nxgb1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [1]  train-rmse:17.486497    test-rmse:16.481208 \n#> [2]  train-rmse:12.680142    test-rmse:11.986695 \n#> [3]  train-rmse:9.347974 test-rmse:8.749361 \n#> [4]  train-rmse:7.008353 test-rmse:6.675755 \n#> [5]  train-rmse:5.419391 test-rmse:5.367051 \n#> [6]  train-rmse:4.361966 test-rmse:4.561522 \n#> [7]  train-rmse:3.624947 test-rmse:4.032414 \n#> [8]  train-rmse:3.160847 test-rmse:3.820689 \n#> [9]  train-rmse:2.824743 test-rmse:3.610561 \n#> [10] train-rmse:2.619698 test-rmse:3.507650 \n#> [11] train-rmse:2.446987 test-rmse:3.484689 \n#> [12] train-rmse:2.329737 test-rmse:3.439376 \n#> [13] train-rmse:2.240210 test-rmse:3.430371 \n#> [14] train-rmse:2.145244 test-rmse:3.391075 \n#> [15] train-rmse:2.084449 test-rmse:3.360280 \n#> [16] train-rmse:2.037245 test-rmse:3.379184 \n#> [17] train-rmse:2.002897 test-rmse:3.366566 \n#> [18] train-rmse:1.945952 test-rmse:3.376486 \n#> [19] train-rmse:1.897742 test-rmse:3.367665 \n#> [20] train-rmse:1.854231 test-rmse:3.348448 \n#> [21] train-rmse:1.822154 test-rmse:3.349097 \n#> [22] train-rmse:1.787204 test-rmse:3.304920 \n#> [23] train-rmse:1.754270 test-rmse:3.293085 \n#> [24] train-rmse:1.709816 test-rmse:3.272922 \n#> [25] train-rmse:1.674145 test-rmse:3.269630 \n#> [26] train-rmse:1.658251 test-rmse:3.258794 \n#> [27] train-rmse:1.641139 test-rmse:3.242577 \n#> [28] train-rmse:1.614929 test-rmse:3.240223 \n#> [29] train-rmse:1.567291 test-rmse:3.240452 \n#> [30] train-rmse:1.505544 test-rmse:3.201182 \n#> [31] train-rmse:1.476929 test-rmse:3.205659 \n#> [32] train-rmse:1.449647 test-rmse:3.194901 \n#> [33] train-rmse:1.416530 test-rmse:3.203879 \n#> [34] train-rmse:1.374986 test-rmse:3.187995 \n#> [35] train-rmse:1.351330 test-rmse:3.190713 \n#> [36] train-rmse:1.316853 test-rmse:3.191331 \n#> [37] train-rmse:1.309591 test-rmse:3.182175 \n#> [38] train-rmse:1.294090 test-rmse:3.175570 \n#> [39] train-rmse:1.277757 test-rmse:3.180537 \n#> [40] train-rmse:1.242837 test-rmse:3.186998 \n#> [41] train-rmse:1.216383 test-rmse:3.187467 \n#> [42] train-rmse:1.189996 test-rmse:3.184045 \n#> [43] train-rmse:1.154658 test-rmse:3.159547 \n#> [44] train-rmse:1.127478 test-rmse:3.139461 \n#> [45] train-rmse:1.115798 test-rmse:3.134894 \n#> [46] train-rmse:1.104211 test-rmse:3.128842 \n#> [47] train-rmse:1.071158 test-rmse:3.123660 \n#> [48] train-rmse:1.054669 test-rmse:3.138948 \n#> [49] train-rmse:1.046175 test-rmse:3.131209 \n#> [50] train-rmse:1.027679 test-rmse:3.124061 \n#> [51] train-rmse:1.014130 test-rmse:3.101524 \n#> [52] train-rmse:0.993697 test-rmse:3.111012 \n#> [53] train-rmse:0.971893 test-rmse:3.107055 \n#> [54] train-rmse:0.950926 test-rmse:3.110510 \n#> [55] train-rmse:0.941455 test-rmse:3.112877 \n#> [56] train-rmse:0.933776 test-rmse:3.106882 \n#> [57] train-rmse:0.929408 test-rmse:3.108469 \n#> [58] train-rmse:0.923074 test-rmse:3.107276 \n#> [59] train-rmse:0.905856 test-rmse:3.098320 \n#> [60] train-rmse:0.899052 test-rmse:3.094325 \n#> [61] train-rmse:0.885400 test-rmse:3.100936 \n#> [62] train-rmse:0.880479 test-rmse:3.099746 \n#> [63] train-rmse:0.865969 test-rmse:3.094835 \n#> [64] train-rmse:0.844476 test-rmse:3.100751 \n#> [65] train-rmse:0.820401 test-rmse:3.091629 \n#> [66] train-rmse:0.805832 test-rmse:3.097608 \n#> [67] train-rmse:0.789846 test-rmse:3.092611 \n#> [68] train-rmse:0.777879 test-rmse:3.098650 \n#> [69] train-rmse:0.765226 test-rmse:3.102803 \n#> [70] train-rmse:0.760578 test-rmse:3.103667 \n#> [1]  train-rmse:17.271000    test-rmse:16.919832 \n#> [2]  train-rmse:12.515867    test-rmse:12.332933 \n#> [3]  train-rmse:9.176305 test-rmse:9.186107 \n#> [4]  train-rmse:6.859001 test-rmse:7.174320 \n#> [5]  train-rmse:5.312209 test-rmse:5.824901 \n#> [6]  train-rmse:4.221931 test-rmse:5.080645 \n#> [7]  train-rmse:3.523406 test-rmse:4.570028 \n#> [8]  train-rmse:3.052697 test-rmse:4.377020 \n#> [9]  train-rmse:2.707503 test-rmse:4.246629 \n#> [10] train-rmse:2.520544 test-rmse:4.196648 \n#> [11] train-rmse:2.365464 test-rmse:4.141710 \n#> [12] train-rmse:2.269290 test-rmse:4.131828 \n#> [13] train-rmse:2.185941 test-rmse:4.165351 \n#> [14] train-rmse:2.101735 test-rmse:4.138577 \n#> [15] train-rmse:2.024846 test-rmse:4.137209 \n#> [16] train-rmse:1.993709 test-rmse:4.152307 \n#> [17] train-rmse:1.945708 test-rmse:4.175986 \n#> [18] train-rmse:1.847535 test-rmse:4.171342 \n#> [19] train-rmse:1.807317 test-rmse:4.181224 \n#> [20] train-rmse:1.769414 test-rmse:4.178264 \n#> [21] train-rmse:1.731118 test-rmse:4.161658 \n#> [22] train-rmse:1.700002 test-rmse:4.176502 \n#> [23] train-rmse:1.676733 test-rmse:4.192347 \n#> [24] train-rmse:1.640807 test-rmse:4.199664 \n#> [25] train-rmse:1.619645 test-rmse:4.182642 \n#> [26] train-rmse:1.585201 test-rmse:4.188219 \n#> [27] train-rmse:1.538859 test-rmse:4.193522 \n#> [28] train-rmse:1.516105 test-rmse:4.200009 \n#> [29] train-rmse:1.471954 test-rmse:4.171491 \n#> [30] train-rmse:1.447593 test-rmse:4.184000 \n#> [31] train-rmse:1.410964 test-rmse:4.163903 \n#> [32] train-rmse:1.385057 test-rmse:4.159180 \n#> [33] train-rmse:1.370980 test-rmse:4.157527 \n#> [34] train-rmse:1.342666 test-rmse:4.163018 \n#> [35] train-rmse:1.305707 test-rmse:4.167307 \n#> [36] train-rmse:1.276964 test-rmse:4.157421 \n#> [37] train-rmse:1.260409 test-rmse:4.156494 \n#> [38] train-rmse:1.247981 test-rmse:4.153613 \n#> [39] train-rmse:1.206421 test-rmse:4.146320 \n#> [40] train-rmse:1.192975 test-rmse:4.150320 \n#> [41] train-rmse:1.181422 test-rmse:4.152663 \n#> [42] train-rmse:1.163184 test-rmse:4.153904 \n#> [43] train-rmse:1.129128 test-rmse:4.171885 \n#> [44] train-rmse:1.117972 test-rmse:4.162864 \n#> [45] train-rmse:1.089349 test-rmse:4.159964 \n#> [46] train-rmse:1.071597 test-rmse:4.175901 \n#> [47] train-rmse:1.049902 test-rmse:4.172292 \n#> [48] train-rmse:1.033387 test-rmse:4.176787 \n#> [49] train-rmse:1.025796 test-rmse:4.179763 \n#> [50] train-rmse:1.014940 test-rmse:4.177002 \n#> [51] train-rmse:0.988000 test-rmse:4.167849 \n#> [52] train-rmse:0.968624 test-rmse:4.168490 \n#> [53] train-rmse:0.954143 test-rmse:4.184151 \n#> [54] train-rmse:0.938822 test-rmse:4.176074 \n#> [55] train-rmse:0.924480 test-rmse:4.195964 \n#> [56] train-rmse:0.915237 test-rmse:4.191507 \n#> [57] train-rmse:0.896332 test-rmse:4.198041 \n#> [58] train-rmse:0.888031 test-rmse:4.190549 \n#> [59] train-rmse:0.878456 test-rmse:4.190976 \n#> [60] train-rmse:0.866349 test-rmse:4.188612 \n#> [61] train-rmse:0.850429 test-rmse:4.171399 \n#> [62] train-rmse:0.838005 test-rmse:4.171440 \n#> [63] train-rmse:0.818213 test-rmse:4.164876 \n#> [64] train-rmse:0.812399 test-rmse:4.160797 \n#> [65] train-rmse:0.805299 test-rmse:4.160084 \n#> [66] train-rmse:0.785106 test-rmse:4.159871 \n#> [67] train-rmse:0.765562 test-rmse:4.169879 \n#> [68] train-rmse:0.752423 test-rmse:4.161290 \n#> [69] train-rmse:0.744580 test-rmse:4.158569 \n#> [70] train-rmse:0.737120 test-rmse:4.149397 \n#> [1]  train-rmse:17.395489    test-rmse:16.510984 \n#> [2]  train-rmse:12.561541    test-rmse:12.113637 \n#> [3]  train-rmse:9.194729 test-rmse:9.024100 \n#> [4]  train-rmse:6.871181 test-rmse:6.819280 \n#> [5]  train-rmse:5.283668 test-rmse:5.467459 \n#> [6]  train-rmse:4.159390 test-rmse:4.582748 \n#> [7]  train-rmse:3.437048 test-rmse:4.007805 \n#> [8]  train-rmse:2.938627 test-rmse:3.730191 \n#> [9]  train-rmse:2.607829 test-rmse:3.559813 \n#> [10] train-rmse:2.387627 test-rmse:3.441103 \n#> [11] train-rmse:2.259817 test-rmse:3.354965 \n#> [12] train-rmse:2.163043 test-rmse:3.318793 \n#> [13] train-rmse:2.090711 test-rmse:3.260732 \n#> [14] train-rmse:2.001951 test-rmse:3.229978 \n#> [15] train-rmse:1.947110 test-rmse:3.248306 \n#> [16] train-rmse:1.877404 test-rmse:3.216982 \n#> [17] train-rmse:1.825344 test-rmse:3.195938 \n#> [18] train-rmse:1.776860 test-rmse:3.173275 \n#> [19] train-rmse:1.749610 test-rmse:3.168493 \n#> [20] train-rmse:1.696357 test-rmse:3.144102 \n#> [21] train-rmse:1.670021 test-rmse:3.126842 \n#> [22] train-rmse:1.643466 test-rmse:3.105513 \n#> [23] train-rmse:1.596573 test-rmse:3.110415 \n#> [24] train-rmse:1.576580 test-rmse:3.104983 \n#> [25] train-rmse:1.563637 test-rmse:3.099317 \n#> [26] train-rmse:1.540325 test-rmse:3.092570 \n#> [27] train-rmse:1.530043 test-rmse:3.093045 \n#> [28] train-rmse:1.509687 test-rmse:3.106363 \n#> [29] train-rmse:1.481246 test-rmse:3.082575 \n#> [30] train-rmse:1.449740 test-rmse:3.083743 \n#> [31] train-rmse:1.425405 test-rmse:3.098682 \n#> [32] train-rmse:1.382464 test-rmse:3.101694 \n#> [33] train-rmse:1.363775 test-rmse:3.083235 \n#> [34] train-rmse:1.349192 test-rmse:3.075221 \n#> [35] train-rmse:1.323078 test-rmse:3.076637 \n#> [36] train-rmse:1.314223 test-rmse:3.073725 \n#> [37] train-rmse:1.303232 test-rmse:3.069449 \n#> [38] train-rmse:1.284872 test-rmse:3.079453 \n#> [39] train-rmse:1.263972 test-rmse:3.068180 \n#> [40] train-rmse:1.239298 test-rmse:3.084689 \n#> [41] train-rmse:1.213229 test-rmse:3.096412 \n#> [42] train-rmse:1.202738 test-rmse:3.099342 \n#> [43] train-rmse:1.181719 test-rmse:3.097213 \n#> [44] train-rmse:1.175878 test-rmse:3.091491 \n#> [45] train-rmse:1.156718 test-rmse:3.087315 \n#> [46] train-rmse:1.144089 test-rmse:3.095235 \n#> [47] train-rmse:1.130485 test-rmse:3.094214 \n#> [48] train-rmse:1.100811 test-rmse:3.071416 \n#> [49] train-rmse:1.085433 test-rmse:3.068388 \n#> [50] train-rmse:1.053607 test-rmse:3.061209 \n#> [51] train-rmse:1.038903 test-rmse:3.052623 \n#> [52] train-rmse:1.014005 test-rmse:3.035290 \n#> [53] train-rmse:0.996117 test-rmse:3.040920 \n#> [54] train-rmse:0.968683 test-rmse:3.043354 \n#> [55] train-rmse:0.947833 test-rmse:3.020012 \n#> [56] train-rmse:0.935359 test-rmse:3.012993 \n#> [57] train-rmse:0.917079 test-rmse:3.002739 \n#> [58] train-rmse:0.901128 test-rmse:3.009477 \n#> [59] train-rmse:0.887161 test-rmse:2.998941 \n#> [60] train-rmse:0.878894 test-rmse:2.993458 \n#> [61] train-rmse:0.870364 test-rmse:2.988974 \n#> [62] train-rmse:0.854939 test-rmse:2.986013 \n#> [63] train-rmse:0.847620 test-rmse:2.983280 \n#> [64] train-rmse:0.844183 test-rmse:2.980246 \n#> [65] train-rmse:0.834523 test-rmse:2.972377 \n#> [66] train-rmse:0.820483 test-rmse:2.973813 \n#> [67] train-rmse:0.811202 test-rmse:2.962869 \n#> [68] train-rmse:0.795636 test-rmse:2.959582 \n#> [69] train-rmse:0.778703 test-rmse:2.950396 \n#> [70] train-rmse:0.768674 test-rmse:2.959586 \n#> [1]  train-rmse:17.380206    test-rmse:17.031772 \n#> [2]  train-rmse:12.588357    test-rmse:12.494238 \n#> [3]  train-rmse:9.244460 test-rmse:9.391844 \n#> [4]  train-rmse:6.930907 test-rmse:7.257440 \n#> [5]  train-rmse:5.306428 test-rmse:5.935620 \n#> [6]  train-rmse:4.212014 test-rmse:5.070906 \n#> [7]  train-rmse:3.480988 test-rmse:4.427449 \n#> [8]  train-rmse:2.971855 test-rmse:4.134873 \n#> [9]  train-rmse:2.632878 test-rmse:3.940482 \n#> [10] train-rmse:2.428489 test-rmse:3.843724 \n#> [11] train-rmse:2.242322 test-rmse:3.705470 \n#> [12] train-rmse:2.134822 test-rmse:3.654557 \n#> [13] train-rmse:2.039841 test-rmse:3.597136 \n#> [14] train-rmse:1.959057 test-rmse:3.540029 \n#> [15] train-rmse:1.875066 test-rmse:3.546649 \n#> [16] train-rmse:1.829356 test-rmse:3.556482 \n#> [17] train-rmse:1.778048 test-rmse:3.541097 \n#> [18] train-rmse:1.748775 test-rmse:3.534900 \n#> [19] train-rmse:1.681980 test-rmse:3.511449 \n#> [20] train-rmse:1.653488 test-rmse:3.488497 \n#> [21] train-rmse:1.634753 test-rmse:3.488181 \n#> [22] train-rmse:1.588483 test-rmse:3.465585 \n#> [23] train-rmse:1.549751 test-rmse:3.468146 \n#> [24] train-rmse:1.521693 test-rmse:3.459599 \n#> [25] train-rmse:1.484244 test-rmse:3.458086 \n#> [26] train-rmse:1.461672 test-rmse:3.469976 \n#> [27] train-rmse:1.428051 test-rmse:3.464792 \n#> [28] train-rmse:1.397826 test-rmse:3.483781 \n#> [29] train-rmse:1.370554 test-rmse:3.475009 \n#> [30] train-rmse:1.344898 test-rmse:3.475376 \n#> [31] train-rmse:1.330812 test-rmse:3.474912 \n#> [32] train-rmse:1.318602 test-rmse:3.481810 \n#> [33] train-rmse:1.289089 test-rmse:3.468368 \n#> [34] train-rmse:1.267479 test-rmse:3.470361 \n#> [35] train-rmse:1.250811 test-rmse:3.467983 \n#> [36] train-rmse:1.230678 test-rmse:3.485372 \n#> [37] train-rmse:1.209182 test-rmse:3.490498 \n#> [38] train-rmse:1.198441 test-rmse:3.480437 \n#> [39] train-rmse:1.182828 test-rmse:3.484867 \n#> [40] train-rmse:1.172483 test-rmse:3.487583 \n#> [41] train-rmse:1.144719 test-rmse:3.484114 \n#> [42] train-rmse:1.120774 test-rmse:3.478865 \n#> [43] train-rmse:1.094923 test-rmse:3.470110 \n#> [44] train-rmse:1.082106 test-rmse:3.477357 \n#> [45] train-rmse:1.049023 test-rmse:3.470272 \n#> [46] train-rmse:1.023616 test-rmse:3.466944 \n#> [47] train-rmse:1.016039 test-rmse:3.466201 \n#> [48] train-rmse:0.992702 test-rmse:3.473398 \n#> [49] train-rmse:0.969921 test-rmse:3.472677 \n#> [50] train-rmse:0.959166 test-rmse:3.483063 \n#> [51] train-rmse:0.937026 test-rmse:3.475435 \n#> [52] train-rmse:0.919188 test-rmse:3.464488 \n#> [53] train-rmse:0.907835 test-rmse:3.460747 \n#> [54] train-rmse:0.888932 test-rmse:3.459577 \n#> [55] train-rmse:0.883432 test-rmse:3.455239 \n#> [56] train-rmse:0.869079 test-rmse:3.452456 \n#> [57] train-rmse:0.857464 test-rmse:3.453599 \n#> [58] train-rmse:0.843735 test-rmse:3.448456 \n#> [59] train-rmse:0.830671 test-rmse:3.445266 \n#> [60] train-rmse:0.827228 test-rmse:3.443131 \n#> [61] train-rmse:0.811642 test-rmse:3.438849 \n#> [62] train-rmse:0.785364 test-rmse:3.432073 \n#> [63] train-rmse:0.776901 test-rmse:3.436418 \n#> [64] train-rmse:0.769347 test-rmse:3.437193 \n#> [65] train-rmse:0.751765 test-rmse:3.433901 \n#> [66] train-rmse:0.738440 test-rmse:3.432187 \n#> [67] train-rmse:0.726259 test-rmse:3.429399 \n#> [68] train-rmse:0.719352 test-rmse:3.424938 \n#> [69] train-rmse:0.714389 test-rmse:3.432535 \n#> [70] train-rmse:0.709031 test-rmse:3.433105 \n#> [1]  train-rmse:16.984875    test-rmse:17.697009 \n#> [2]  train-rmse:12.324817    test-rmse:12.953677 \n#> [3]  train-rmse:9.045915 test-rmse:9.644390 \n#> [4]  train-rmse:6.784900 test-rmse:7.500049 \n#> [5]  train-rmse:5.253843 test-rmse:6.065439 \n#> [6]  train-rmse:4.178446 test-rmse:5.162901 \n#> [7]  train-rmse:3.480812 test-rmse:4.724464 \n#> [8]  train-rmse:3.021814 test-rmse:4.466702 \n#> [9]  train-rmse:2.700977 test-rmse:4.238377 \n#> [10] train-rmse:2.466828 test-rmse:4.115037 \n#> [11] train-rmse:2.308248 test-rmse:4.046978 \n#> [12] train-rmse:2.200536 test-rmse:4.003357 \n#> [13] train-rmse:2.077451 test-rmse:3.937996 \n#> [14] train-rmse:1.993869 test-rmse:3.915808 \n#> [15] train-rmse:1.935727 test-rmse:3.875300 \n#> [16] train-rmse:1.880466 test-rmse:3.874977 \n#> [17] train-rmse:1.844265 test-rmse:3.858995 \n#> [18] train-rmse:1.806236 test-rmse:3.880547 \n#> [19] train-rmse:1.746038 test-rmse:3.860371 \n#> [20] train-rmse:1.698725 test-rmse:3.859303 \n#> [21] train-rmse:1.677481 test-rmse:3.847714 \n#> [22] train-rmse:1.644719 test-rmse:3.846820 \n#> [23] train-rmse:1.619180 test-rmse:3.864139 \n#> [24] train-rmse:1.586591 test-rmse:3.864736 \n#> [25] train-rmse:1.535714 test-rmse:3.855715 \n#> [26] train-rmse:1.523442 test-rmse:3.848466 \n#> [27] train-rmse:1.493008 test-rmse:3.820469 \n#> [28] train-rmse:1.468519 test-rmse:3.814175 \n#> [29] train-rmse:1.455317 test-rmse:3.819515 \n#> [30] train-rmse:1.422747 test-rmse:3.834053 \n#> [31] train-rmse:1.366023 test-rmse:3.834437 \n#> [32] train-rmse:1.331921 test-rmse:3.818818 \n#> [33] train-rmse:1.302149 test-rmse:3.814104 \n#> [34] train-rmse:1.288872 test-rmse:3.815516 \n#> [35] train-rmse:1.276577 test-rmse:3.801725 \n#> [36] train-rmse:1.253637 test-rmse:3.793647 \n#> [37] train-rmse:1.237153 test-rmse:3.813853 \n#> [38] train-rmse:1.230567 test-rmse:3.813858 \n#> [39] train-rmse:1.219340 test-rmse:3.813696 \n#> [40] train-rmse:1.206634 test-rmse:3.811301 \n#> [41] train-rmse:1.178934 test-rmse:3.817181 \n#> [42] train-rmse:1.153515 test-rmse:3.819049 \n#> [43] train-rmse:1.137846 test-rmse:3.813922 \n#> [44] train-rmse:1.120400 test-rmse:3.797407 \n#> [45] train-rmse:1.110760 test-rmse:3.797720 \n#> [46] train-rmse:1.102493 test-rmse:3.794102 \n#> [47] train-rmse:1.082844 test-rmse:3.787301 \n#> [48] train-rmse:1.066238 test-rmse:3.797389 \n#> [49] train-rmse:1.041964 test-rmse:3.800018 \n#> [50] train-rmse:1.029571 test-rmse:3.801595 \n#> [51] train-rmse:1.014837 test-rmse:3.802143 \n#> [52] train-rmse:0.986400 test-rmse:3.804814 \n#> [53] train-rmse:0.982557 test-rmse:3.799498 \n#> [54] train-rmse:0.972485 test-rmse:3.806163 \n#> [55] train-rmse:0.956978 test-rmse:3.808189 \n#> [56] train-rmse:0.933708 test-rmse:3.801178 \n#> [57] train-rmse:0.918174 test-rmse:3.798300 \n#> [58] train-rmse:0.906931 test-rmse:3.798485 \n#> [59] train-rmse:0.902619 test-rmse:3.801572 \n#> [60] train-rmse:0.898097 test-rmse:3.797797 \n#> [61] train-rmse:0.889969 test-rmse:3.790598 \n#> [62] train-rmse:0.874802 test-rmse:3.789076 \n#> [63] train-rmse:0.858111 test-rmse:3.781398 \n#> [64] train-rmse:0.847368 test-rmse:3.782311 \n#> [65] train-rmse:0.840788 test-rmse:3.784421 \n#> [66] train-rmse:0.824566 test-rmse:3.786287 \n#> [67] train-rmse:0.820674 test-rmse:3.785419 \n#> [68] train-rmse:0.806288 test-rmse:3.782655 \n#> [69] train-rmse:0.794688 test-rmse:3.777987 \n#> [70] train-rmse:0.787247 test-rmse:3.781246 \n#> [1]  train-rmse:17.713810    test-rmse:16.241058 \n#> [2]  train-rmse:12.828331    test-rmse:11.856221 \n#> [3]  train-rmse:9.419469 test-rmse:8.836258 \n#> [4]  train-rmse:7.026210 test-rmse:6.923035 \n#> [5]  train-rmse:5.389105 test-rmse:5.719839 \n#> [6]  train-rmse:4.278109 test-rmse:5.031750 \n#> [7]  train-rmse:3.544784 test-rmse:4.677846 \n#> [8]  train-rmse:3.083702 test-rmse:4.488374 \n#> [9]  train-rmse:2.784028 test-rmse:4.375848 \n#> [10] train-rmse:2.607710 test-rmse:4.356227 \n#> [11] train-rmse:2.457488 test-rmse:4.225116 \n#> [12] train-rmse:2.308632 test-rmse:4.120038 \n#> [13] train-rmse:2.213545 test-rmse:4.118951 \n#> [14] train-rmse:2.101512 test-rmse:4.099988 \n#> [15] train-rmse:2.064077 test-rmse:4.084281 \n#> [16] train-rmse:1.992316 test-rmse:4.112185 \n#> [17] train-rmse:1.941101 test-rmse:4.068031 \n#> [18] train-rmse:1.889634 test-rmse:4.047856 \n#> [19] train-rmse:1.837156 test-rmse:4.047730 \n#> [20] train-rmse:1.796701 test-rmse:4.030062 \n#> [21] train-rmse:1.753100 test-rmse:4.037393 \n#> [22] train-rmse:1.716282 test-rmse:4.035883 \n#> [23] train-rmse:1.688023 test-rmse:4.044036 \n#> [24] train-rmse:1.637178 test-rmse:4.025172 \n#> [25] train-rmse:1.590153 test-rmse:4.001287 \n#> [26] train-rmse:1.546720 test-rmse:3.999570 \n#> [27] train-rmse:1.518661 test-rmse:3.996621 \n#> [28] train-rmse:1.504567 test-rmse:3.997688 \n#> [29] train-rmse:1.494228 test-rmse:4.000123 \n#> [30] train-rmse:1.452043 test-rmse:3.998804 \n#> [31] train-rmse:1.432224 test-rmse:3.995112 \n#> [32] train-rmse:1.414814 test-rmse:3.993324 \n#> [33] train-rmse:1.385945 test-rmse:3.988753 \n#> [34] train-rmse:1.358210 test-rmse:3.974604 \n#> [35] train-rmse:1.327928 test-rmse:3.959248 \n#> [36] train-rmse:1.292086 test-rmse:3.921402 \n#> [37] train-rmse:1.270701 test-rmse:3.925025 \n#> [38] train-rmse:1.250430 test-rmse:3.928783 \n#> [39] train-rmse:1.216950 test-rmse:3.926792 \n#> [40] train-rmse:1.194814 test-rmse:3.923330 \n#> [41] train-rmse:1.179701 test-rmse:3.927600 \n#> [42] train-rmse:1.165947 test-rmse:3.926244 \n#> [43] train-rmse:1.139723 test-rmse:3.925082 \n#> [44] train-rmse:1.126418 test-rmse:3.931113 \n#> [45] train-rmse:1.108641 test-rmse:3.927126 \n#> [46] train-rmse:1.079074 test-rmse:3.912517 \n#> [47] train-rmse:1.067105 test-rmse:3.914097 \n#> [48] train-rmse:1.057996 test-rmse:3.912245 \n#> [49] train-rmse:1.038300 test-rmse:3.911922 \n#> [50] train-rmse:1.026288 test-rmse:3.906951 \n#> [51] train-rmse:1.017630 test-rmse:3.903757 \n#> [52] train-rmse:1.009007 test-rmse:3.906669 \n#> [53] train-rmse:0.986859 test-rmse:3.904847 \n#> [54] train-rmse:0.962731 test-rmse:3.899167 \n#> [55] train-rmse:0.952384 test-rmse:3.887059 \n#> [56] train-rmse:0.939516 test-rmse:3.885651 \n#> [57] train-rmse:0.932103 test-rmse:3.884431 \n#> [58] train-rmse:0.919283 test-rmse:3.878682 \n#> [59] train-rmse:0.905617 test-rmse:3.876369 \n#> [60] train-rmse:0.881147 test-rmse:3.883946 \n#> [61] train-rmse:0.858171 test-rmse:3.884682 \n#> [62] train-rmse:0.844415 test-rmse:3.882521 \n#> [63] train-rmse:0.837669 test-rmse:3.882943 \n#> [64] train-rmse:0.832174 test-rmse:3.880904 \n#> [65] train-rmse:0.810570 test-rmse:3.889605 \n#> [66] train-rmse:0.804834 test-rmse:3.894057 \n#> [67] train-rmse:0.791626 test-rmse:3.893028 \n#> [68] train-rmse:0.768779 test-rmse:3.879874 \n#> [69] train-rmse:0.758766 test-rmse:3.879197 \n#> [70] train-rmse:0.749967 test-rmse:3.879459 \n#> [1]  train-rmse:16.997379    test-rmse:17.451532 \n#> [2]  train-rmse:12.327516    test-rmse:12.704180 \n#> [3]  train-rmse:9.040641 test-rmse:9.445564 \n#> [4]  train-rmse:6.807076 test-rmse:7.205961 \n#> [5]  train-rmse:5.244356 test-rmse:5.800310 \n#> [6]  train-rmse:4.159316 test-rmse:4.861555 \n#> [7]  train-rmse:3.462708 test-rmse:4.311892 \n#> [8]  train-rmse:3.014366 test-rmse:4.028696 \n#> [9]  train-rmse:2.720645 test-rmse:3.843606 \n#> [10] train-rmse:2.520422 test-rmse:3.689043 \n#> [11] train-rmse:2.373553 test-rmse:3.583901 \n#> [12] train-rmse:2.277313 test-rmse:3.518069 \n#> [13] train-rmse:2.201194 test-rmse:3.488645 \n#> [14] train-rmse:2.100364 test-rmse:3.458100 \n#> [15] train-rmse:2.038767 test-rmse:3.436961 \n#> [16] train-rmse:1.981704 test-rmse:3.423819 \n#> [17] train-rmse:1.939475 test-rmse:3.405802 \n#> [18] train-rmse:1.878451 test-rmse:3.391935 \n#> [19] train-rmse:1.822122 test-rmse:3.356745 \n#> [20] train-rmse:1.760522 test-rmse:3.348070 \n#> [21] train-rmse:1.728997 test-rmse:3.338443 \n#> [22] train-rmse:1.678775 test-rmse:3.331897 \n#> [23] train-rmse:1.619983 test-rmse:3.302209 \n#> [24] train-rmse:1.587433 test-rmse:3.294017 \n#> [25] train-rmse:1.558678 test-rmse:3.283673 \n#> [26] train-rmse:1.526820 test-rmse:3.272382 \n#> [27] train-rmse:1.484628 test-rmse:3.279604 \n#> [28] train-rmse:1.453687 test-rmse:3.294263 \n#> [29] train-rmse:1.420792 test-rmse:3.286631 \n#> [30] train-rmse:1.393862 test-rmse:3.271825 \n#> [31] train-rmse:1.375700 test-rmse:3.273577 \n#> [32] train-rmse:1.349823 test-rmse:3.277158 \n#> [33] train-rmse:1.330934 test-rmse:3.276069 \n#> [34] train-rmse:1.306806 test-rmse:3.261469 \n#> [35] train-rmse:1.290785 test-rmse:3.248870 \n#> [36] train-rmse:1.276678 test-rmse:3.256351 \n#> [37] train-rmse:1.250650 test-rmse:3.249693 \n#> [38] train-rmse:1.239255 test-rmse:3.236318 \n#> [39] train-rmse:1.220674 test-rmse:3.239413 \n#> [40] train-rmse:1.183392 test-rmse:3.232085 \n#> [41] train-rmse:1.164879 test-rmse:3.237248 \n#> [42] train-rmse:1.150750 test-rmse:3.229254 \n#> [43] train-rmse:1.145172 test-rmse:3.225219 \n#> [44] train-rmse:1.127229 test-rmse:3.224636 \n#> [45] train-rmse:1.101395 test-rmse:3.223810 \n#> [46] train-rmse:1.091903 test-rmse:3.213180 \n#> [47] train-rmse:1.076728 test-rmse:3.220686 \n#> [48] train-rmse:1.061354 test-rmse:3.215525 \n#> [49] train-rmse:1.051190 test-rmse:3.225935 \n#> [50] train-rmse:1.021991 test-rmse:3.224229 \n#> [51] train-rmse:1.010068 test-rmse:3.224404 \n#> [52] train-rmse:0.999470 test-rmse:3.223292 \n#> [53] train-rmse:0.970067 test-rmse:3.206137 \n#> [54] train-rmse:0.964163 test-rmse:3.209393 \n#> [55] train-rmse:0.942628 test-rmse:3.200447 \n#> [56] train-rmse:0.934682 test-rmse:3.211937 \n#> [57] train-rmse:0.924950 test-rmse:3.206770 \n#> [58] train-rmse:0.904265 test-rmse:3.206695 \n#> [59] train-rmse:0.881713 test-rmse:3.209321 \n#> [60] train-rmse:0.867272 test-rmse:3.208244 \n#> [61] train-rmse:0.854953 test-rmse:3.211635 \n#> [62] train-rmse:0.842953 test-rmse:3.216044 \n#> [63] train-rmse:0.836405 test-rmse:3.213471 \n#> [64] train-rmse:0.830658 test-rmse:3.210704 \n#> [65] train-rmse:0.807639 test-rmse:3.200923 \n#> [66] train-rmse:0.799781 test-rmse:3.199911 \n#> [67] train-rmse:0.788423 test-rmse:3.192200 \n#> [68] train-rmse:0.772786 test-rmse:3.195580 \n#> [69] train-rmse:0.757038 test-rmse:3.199317 \n#> [70] train-rmse:0.752378 test-rmse:3.192875 \n#> [1]  train-rmse:17.057881    test-rmse:17.368816 \n#> [2]  train-rmse:12.397999    test-rmse:12.721176 \n#> [3]  train-rmse:9.179658 test-rmse:9.397576 \n#> [4]  train-rmse:6.899103 test-rmse:7.217575 \n#> [5]  train-rmse:5.339070 test-rmse:5.748317 \n#> [6]  train-rmse:4.268698 test-rmse:4.793658 \n#> [7]  train-rmse:3.590526 test-rmse:4.206053 \n#> [8]  train-rmse:3.118371 test-rmse:3.869369 \n#> [9]  train-rmse:2.790172 test-rmse:3.652297 \n#> [10] train-rmse:2.544289 test-rmse:3.532488 \n#> [11] train-rmse:2.385160 test-rmse:3.444538 \n#> [12] train-rmse:2.267360 test-rmse:3.375863 \n#> [13] train-rmse:2.164550 test-rmse:3.327617 \n#> [14] train-rmse:2.077892 test-rmse:3.311205 \n#> [15] train-rmse:2.007370 test-rmse:3.300080 \n#> [16] train-rmse:1.963144 test-rmse:3.297826 \n#> [17] train-rmse:1.890445 test-rmse:3.244709 \n#> [18] train-rmse:1.840363 test-rmse:3.226010 \n#> [19] train-rmse:1.807124 test-rmse:3.226659 \n#> [20] train-rmse:1.731958 test-rmse:3.221266 \n#> [21] train-rmse:1.712411 test-rmse:3.213196 \n#> [22] train-rmse:1.671912 test-rmse:3.209998 \n#> [23] train-rmse:1.650644 test-rmse:3.207031 \n#> [24] train-rmse:1.597803 test-rmse:3.192777 \n#> [25] train-rmse:1.544983 test-rmse:3.176058 \n#> [26] train-rmse:1.512009 test-rmse:3.164827 \n#> [27] train-rmse:1.490265 test-rmse:3.165031 \n#> [28] train-rmse:1.463455 test-rmse:3.145430 \n#> [29] train-rmse:1.413624 test-rmse:3.118182 \n#> [30] train-rmse:1.381894 test-rmse:3.114592 \n#> [31] train-rmse:1.373177 test-rmse:3.115505 \n#> [32] train-rmse:1.333080 test-rmse:3.099397 \n#> [33] train-rmse:1.296291 test-rmse:3.102611 \n#> [34] train-rmse:1.284492 test-rmse:3.100417 \n#> [35] train-rmse:1.270912 test-rmse:3.096351 \n#> [36] train-rmse:1.255799 test-rmse:3.103873 \n#> [37] train-rmse:1.212618 test-rmse:3.110615 \n#> [38] train-rmse:1.178545 test-rmse:3.099686 \n#> [39] train-rmse:1.165140 test-rmse:3.092176 \n#> [40] train-rmse:1.148148 test-rmse:3.088673 \n#> [41] train-rmse:1.133738 test-rmse:3.073517 \n#> [42] train-rmse:1.118084 test-rmse:3.060142 \n#> [43] train-rmse:1.109201 test-rmse:3.060820 \n#> [44] train-rmse:1.101690 test-rmse:3.059667 \n#> [45] train-rmse:1.090387 test-rmse:3.056292 \n#> [46] train-rmse:1.067051 test-rmse:3.052778 \n#> [47] train-rmse:1.054564 test-rmse:3.050693 \n#> [48] train-rmse:1.032769 test-rmse:3.049460 \n#> [49] train-rmse:1.024019 test-rmse:3.047590 \n#> [50] train-rmse:1.002596 test-rmse:3.046943 \n#> [51] train-rmse:0.977975 test-rmse:3.050363 \n#> [52] train-rmse:0.967894 test-rmse:3.053568 \n#> [53] train-rmse:0.957275 test-rmse:3.051814 \n#> [54] train-rmse:0.940385 test-rmse:3.055941 \n#> [55] train-rmse:0.928878 test-rmse:3.055873 \n#> [56] train-rmse:0.905830 test-rmse:3.052251 \n#> [57] train-rmse:0.896455 test-rmse:3.052866 \n#> [58] train-rmse:0.887365 test-rmse:3.057423 \n#> [59] train-rmse:0.874122 test-rmse:3.056521 \n#> [60] train-rmse:0.865980 test-rmse:3.057188 \n#> [61] train-rmse:0.855847 test-rmse:3.051740 \n#> [62] train-rmse:0.842819 test-rmse:3.047978 \n#> [63] train-rmse:0.832470 test-rmse:3.051294 \n#> [64] train-rmse:0.811250 test-rmse:3.051492 \n#> [65] train-rmse:0.808786 test-rmse:3.051843 \n#> [66] train-rmse:0.798990 test-rmse:3.057929 \n#> [67] train-rmse:0.783010 test-rmse:3.064119 \n#> [68] train-rmse:0.777822 test-rmse:3.062087 \n#> [69] train-rmse:0.770918 test-rmse:3.059189 \n#> [70] train-rmse:0.754899 test-rmse:3.059076 \n#> [1]  train-rmse:17.412960    test-rmse:16.681394 \n#> [2]  train-rmse:12.652660    test-rmse:12.189434 \n#> [3]  train-rmse:9.296199 test-rmse:8.954743 \n#> [4]  train-rmse:6.973352 test-rmse:6.788588 \n#> [5]  train-rmse:5.367166 test-rmse:5.339162 \n#> [6]  train-rmse:4.311594 test-rmse:4.608044 \n#> [7]  train-rmse:3.607140 test-rmse:4.078680 \n#> [8]  train-rmse:3.163652 test-rmse:3.843206 \n#> [9]  train-rmse:2.849114 test-rmse:3.649577 \n#> [10] train-rmse:2.590645 test-rmse:3.550432 \n#> [11] train-rmse:2.440900 test-rmse:3.519477 \n#> [12] train-rmse:2.340064 test-rmse:3.474820 \n#> [13] train-rmse:2.265105 test-rmse:3.441133 \n#> [14] train-rmse:2.189529 test-rmse:3.415320 \n#> [15] train-rmse:2.141899 test-rmse:3.384855 \n#> [16] train-rmse:2.069348 test-rmse:3.358941 \n#> [17] train-rmse:2.034522 test-rmse:3.340774 \n#> [18] train-rmse:1.999661 test-rmse:3.344826 \n#> [19] train-rmse:1.938931 test-rmse:3.319317 \n#> [20] train-rmse:1.912546 test-rmse:3.293988 \n#> [21] train-rmse:1.886576 test-rmse:3.258163 \n#> [22] train-rmse:1.847894 test-rmse:3.253090 \n#> [23] train-rmse:1.831469 test-rmse:3.234605 \n#> [24] train-rmse:1.751538 test-rmse:3.214050 \n#> [25] train-rmse:1.740301 test-rmse:3.206351 \n#> [26] train-rmse:1.688788 test-rmse:3.189390 \n#> [27] train-rmse:1.649900 test-rmse:3.176641 \n#> [28] train-rmse:1.613690 test-rmse:3.180401 \n#> [29] train-rmse:1.569443 test-rmse:3.179498 \n#> [30] train-rmse:1.549176 test-rmse:3.178520 \n#> [31] train-rmse:1.523100 test-rmse:3.163688 \n#> [32] train-rmse:1.508207 test-rmse:3.164436 \n#> [33] train-rmse:1.458552 test-rmse:3.158960 \n#> [34] train-rmse:1.446232 test-rmse:3.143078 \n#> [35] train-rmse:1.431799 test-rmse:3.140469 \n#> [36] train-rmse:1.400321 test-rmse:3.133625 \n#> [37] train-rmse:1.369973 test-rmse:3.115593 \n#> [38] train-rmse:1.352752 test-rmse:3.129119 \n#> [39] train-rmse:1.301946 test-rmse:3.116484 \n#> [40] train-rmse:1.281978 test-rmse:3.113416 \n#> [41] train-rmse:1.259055 test-rmse:3.109692 \n#> [42] train-rmse:1.236656 test-rmse:3.115181 \n#> [43] train-rmse:1.224697 test-rmse:3.121522 \n#> [44] train-rmse:1.215824 test-rmse:3.118802 \n#> [45] train-rmse:1.198657 test-rmse:3.124249 \n#> [46] train-rmse:1.186001 test-rmse:3.116307 \n#> [47] train-rmse:1.171970 test-rmse:3.117426 \n#> [48] train-rmse:1.154399 test-rmse:3.107584 \n#> [49] train-rmse:1.148894 test-rmse:3.108027 \n#> [50] train-rmse:1.137316 test-rmse:3.111089 \n#> [51] train-rmse:1.107201 test-rmse:3.109275 \n#> [52] train-rmse:1.085121 test-rmse:3.120356 \n#> [53] train-rmse:1.061365 test-rmse:3.113679 \n#> [54] train-rmse:1.052869 test-rmse:3.113354 \n#> [55] train-rmse:1.027213 test-rmse:3.112807 \n#> [56] train-rmse:1.018945 test-rmse:3.116606 \n#> [57] train-rmse:1.009530 test-rmse:3.117851 \n#> [58] train-rmse:1.002134 test-rmse:3.105817 \n#> [59] train-rmse:0.990317 test-rmse:3.100990 \n#> [60] train-rmse:0.965903 test-rmse:3.090352 \n#> [61] train-rmse:0.954821 test-rmse:3.085283 \n#> [62] train-rmse:0.944863 test-rmse:3.089749 \n#> [63] train-rmse:0.927950 test-rmse:3.091168 \n#> [64] train-rmse:0.907285 test-rmse:3.087667 \n#> [65] train-rmse:0.889235 test-rmse:3.076890 \n#> [66] train-rmse:0.875737 test-rmse:3.087059 \n#> [67] train-rmse:0.862707 test-rmse:3.090322 \n#> [68] train-rmse:0.853752 test-rmse:3.085296 \n#> [69] train-rmse:0.837307 test-rmse:3.084133 \n#> [70] train-rmse:0.822196 test-rmse:3.084090 \n#> [1]  train-rmse:17.169506    test-rmse:17.225022 \n#> [2]  train-rmse:12.455913    test-rmse:12.752763 \n#> [3]  train-rmse:9.151926 test-rmse:9.553192 \n#> [4]  train-rmse:6.884537 test-rmse:7.505901 \n#> [5]  train-rmse:5.325362 test-rmse:6.039607 \n#> [6]  train-rmse:4.302497 test-rmse:5.109753 \n#> [7]  train-rmse:3.606837 test-rmse:4.558336 \n#> [8]  train-rmse:3.093282 test-rmse:4.084655 \n#> [9]  train-rmse:2.800401 test-rmse:3.924412 \n#> [10] train-rmse:2.540384 test-rmse:3.814947 \n#> [11] train-rmse:2.354165 test-rmse:3.688904 \n#> [12] train-rmse:2.256497 test-rmse:3.611516 \n#> [13] train-rmse:2.132042 test-rmse:3.511207 \n#> [14] train-rmse:2.069947 test-rmse:3.471555 \n#> [15] train-rmse:1.997296 test-rmse:3.390646 \n#> [16] train-rmse:1.940055 test-rmse:3.368159 \n#> [17] train-rmse:1.885953 test-rmse:3.361642 \n#> [18] train-rmse:1.838922 test-rmse:3.390506 \n#> [19] train-rmse:1.798524 test-rmse:3.387458 \n#> [20] train-rmse:1.756683 test-rmse:3.367634 \n#> [21] train-rmse:1.724023 test-rmse:3.356533 \n#> [22] train-rmse:1.685320 test-rmse:3.336750 \n#> [23] train-rmse:1.661951 test-rmse:3.335087 \n#> [24] train-rmse:1.603331 test-rmse:3.323405 \n#> [25] train-rmse:1.555986 test-rmse:3.305589 \n#> [26] train-rmse:1.529450 test-rmse:3.297357 \n#> [27] train-rmse:1.504871 test-rmse:3.280927 \n#> [28] train-rmse:1.485139 test-rmse:3.281735 \n#> [29] train-rmse:1.469784 test-rmse:3.283056 \n#> [30] train-rmse:1.420708 test-rmse:3.286066 \n#> [31] train-rmse:1.392765 test-rmse:3.280234 \n#> [32] train-rmse:1.383273 test-rmse:3.281691 \n#> [33] train-rmse:1.362129 test-rmse:3.280715 \n#> [34] train-rmse:1.331734 test-rmse:3.284393 \n#> [35] train-rmse:1.310938 test-rmse:3.280485 \n#> [36] train-rmse:1.302003 test-rmse:3.268143 \n#> [37] train-rmse:1.285602 test-rmse:3.267923 \n#> [38] train-rmse:1.270166 test-rmse:3.263989 \n#> [39] train-rmse:1.259880 test-rmse:3.269257 \n#> [40] train-rmse:1.234744 test-rmse:3.278989 \n#> [41] train-rmse:1.217414 test-rmse:3.276972 \n#> [42] train-rmse:1.211225 test-rmse:3.271526 \n#> [43] train-rmse:1.199504 test-rmse:3.277273 \n#> [44] train-rmse:1.179804 test-rmse:3.278123 \n#> [45] train-rmse:1.147487 test-rmse:3.258568 \n#> [46] train-rmse:1.134460 test-rmse:3.259104 \n#> [47] train-rmse:1.106672 test-rmse:3.251910 \n#> [48] train-rmse:1.089201 test-rmse:3.247609 \n#> [49] train-rmse:1.079810 test-rmse:3.244909 \n#> [50] train-rmse:1.047328 test-rmse:3.236303 \n#> [51] train-rmse:1.035362 test-rmse:3.237717 \n#> [52] train-rmse:1.004347 test-rmse:3.237464 \n#> [53] train-rmse:0.990274 test-rmse:3.240111 \n#> [54] train-rmse:0.979509 test-rmse:3.245756 \n#> [55] train-rmse:0.975044 test-rmse:3.245766 \n#> [56] train-rmse:0.955212 test-rmse:3.227891 \n#> [57] train-rmse:0.934651 test-rmse:3.233161 \n#> [58] train-rmse:0.912748 test-rmse:3.221678 \n#> [59] train-rmse:0.897412 test-rmse:3.223187 \n#> [60] train-rmse:0.881171 test-rmse:3.222607 \n#> [61] train-rmse:0.871897 test-rmse:3.222442 \n#> [62] train-rmse:0.851408 test-rmse:3.219850 \n#> [63] train-rmse:0.833504 test-rmse:3.221976 \n#> [64] train-rmse:0.826423 test-rmse:3.220464 \n#> [65] train-rmse:0.819444 test-rmse:3.218707 \n#> [66] train-rmse:0.803940 test-rmse:3.222166 \n#> [67] train-rmse:0.798206 test-rmse:3.222465 \n#> [68] train-rmse:0.785378 test-rmse:3.218759 \n#> [69] train-rmse:0.780135 test-rmse:3.219273 \n#> [70] train-rmse:0.773542 test-rmse:3.219333 \n#> [1]  train-rmse:17.164552    test-rmse:17.242814 \n#> [2]  train-rmse:12.471723    test-rmse:12.579258 \n#> [3]  train-rmse:9.207161 test-rmse:9.448715 \n#> [4]  train-rmse:6.867933 test-rmse:7.289990 \n#> [5]  train-rmse:5.303911 test-rmse:5.857567 \n#> [6]  train-rmse:4.206712 test-rmse:4.998401 \n#> [7]  train-rmse:3.465868 test-rmse:4.376139 \n#> [8]  train-rmse:2.991414 test-rmse:3.999812 \n#> [9]  train-rmse:2.668558 test-rmse:3.791950 \n#> [10] train-rmse:2.428843 test-rmse:3.626125 \n#> [11] train-rmse:2.295141 test-rmse:3.532538 \n#> [12] train-rmse:2.209122 test-rmse:3.487994 \n#> [13] train-rmse:2.135309 test-rmse:3.409080 \n#> [14] train-rmse:2.062738 test-rmse:3.342879 \n#> [15] train-rmse:1.972212 test-rmse:3.341761 \n#> [16] train-rmse:1.905831 test-rmse:3.312981 \n#> [17] train-rmse:1.864826 test-rmse:3.294408 \n#> [18] train-rmse:1.821740 test-rmse:3.285478 \n#> [19] train-rmse:1.776895 test-rmse:3.276286 \n#> [20] train-rmse:1.739859 test-rmse:3.256479 \n#> [21] train-rmse:1.679596 test-rmse:3.271925 \n#> [22] train-rmse:1.623119 test-rmse:3.269289 \n#> [23] train-rmse:1.604576 test-rmse:3.269806 \n#> [24] train-rmse:1.577794 test-rmse:3.256249 \n#> [25] train-rmse:1.544814 test-rmse:3.256503 \n#> [26] train-rmse:1.519333 test-rmse:3.246662 \n#> [27] train-rmse:1.498719 test-rmse:3.244969 \n#> [28] train-rmse:1.458306 test-rmse:3.245996 \n#> [29] train-rmse:1.437625 test-rmse:3.240521 \n#> [30] train-rmse:1.410925 test-rmse:3.234643 \n#> [31] train-rmse:1.387669 test-rmse:3.218191 \n#> [32] train-rmse:1.346276 test-rmse:3.200522 \n#> [33] train-rmse:1.322706 test-rmse:3.205671 \n#> [34] train-rmse:1.292469 test-rmse:3.201344 \n#> [35] train-rmse:1.270995 test-rmse:3.193838 \n#> [36] train-rmse:1.243724 test-rmse:3.191893 \n#> [37] train-rmse:1.212973 test-rmse:3.184971 \n#> [38] train-rmse:1.202365 test-rmse:3.172221 \n#> [39] train-rmse:1.163736 test-rmse:3.169551 \n#> [40] train-rmse:1.157949 test-rmse:3.173690 \n#> [41] train-rmse:1.130852 test-rmse:3.183101 \n#> [42] train-rmse:1.118583 test-rmse:3.181193 \n#> [43] train-rmse:1.096160 test-rmse:3.180477 \n#> [44] train-rmse:1.086879 test-rmse:3.167624 \n#> [45] train-rmse:1.077244 test-rmse:3.174473 \n#> [46] train-rmse:1.064686 test-rmse:3.181578 \n#> [47] train-rmse:1.046804 test-rmse:3.178921 \n#> [48] train-rmse:1.036998 test-rmse:3.179391 \n#> [49] train-rmse:1.022280 test-rmse:3.176065 \n#> [50] train-rmse:1.010822 test-rmse:3.174446 \n#> [51] train-rmse:1.001944 test-rmse:3.166613 \n#> [52] train-rmse:0.983659 test-rmse:3.177306 \n#> [53] train-rmse:0.963870 test-rmse:3.166899 \n#> [54] train-rmse:0.945945 test-rmse:3.164375 \n#> [55] train-rmse:0.926894 test-rmse:3.165051 \n#> [56] train-rmse:0.907626 test-rmse:3.156174 \n#> [57] train-rmse:0.892841 test-rmse:3.158669 \n#> [58] train-rmse:0.884488 test-rmse:3.153873 \n#> [59] train-rmse:0.878829 test-rmse:3.155999 \n#> [60] train-rmse:0.874050 test-rmse:3.156696 \n#> [61] train-rmse:0.854229 test-rmse:3.148281 \n#> [62] train-rmse:0.834348 test-rmse:3.143383 \n#> [63] train-rmse:0.821748 test-rmse:3.139207 \n#> [64] train-rmse:0.815034 test-rmse:3.132746 \n#> [65] train-rmse:0.808634 test-rmse:3.126192 \n#> [66] train-rmse:0.805577 test-rmse:3.128236 \n#> [67] train-rmse:0.791926 test-rmse:3.133038 \n#> [68] train-rmse:0.786542 test-rmse:3.127416 \n#> [69] train-rmse:0.782589 test-rmse:3.126790 \n#> [70] train-rmse:0.759756 test-rmse:3.117040 \n#> [1]  train-rmse:17.365677    test-rmse:16.981133 \n#> [2]  train-rmse:12.627085    test-rmse:12.371665 \n#> [3]  train-rmse:9.283167 test-rmse:9.081143 \n#> [4]  train-rmse:7.007401 test-rmse:6.895097 \n#> [5]  train-rmse:5.414292 test-rmse:5.378851 \n#> [6]  train-rmse:4.327584 test-rmse:4.426966 \n#> [7]  train-rmse:3.650806 test-rmse:3.858720 \n#> [8]  train-rmse:3.187126 test-rmse:3.570247 \n#> [9]  train-rmse:2.841159 test-rmse:3.415336 \n#> [10] train-rmse:2.613889 test-rmse:3.311968 \n#> [11] train-rmse:2.442909 test-rmse:3.265147 \n#> [12] train-rmse:2.289956 test-rmse:3.127611 \n#> [13] train-rmse:2.185121 test-rmse:3.084449 \n#> [14] train-rmse:2.089459 test-rmse:3.048229 \n#> [15] train-rmse:2.019436 test-rmse:3.046697 \n#> [16] train-rmse:1.927465 test-rmse:3.041559 \n#> [17] train-rmse:1.874340 test-rmse:3.059228 \n#> [18] train-rmse:1.827809 test-rmse:3.033781 \n#> [19] train-rmse:1.759303 test-rmse:3.054434 \n#> [20] train-rmse:1.736359 test-rmse:3.050429 \n#> [21] train-rmse:1.698841 test-rmse:3.060269 \n#> [22] train-rmse:1.668556 test-rmse:3.057619 \n#> [23] train-rmse:1.645172 test-rmse:3.067866 \n#> [24] train-rmse:1.591316 test-rmse:3.093922 \n#> [25] train-rmse:1.564144 test-rmse:3.092687 \n#> [26] train-rmse:1.525636 test-rmse:3.078053 \n#> [27] train-rmse:1.498496 test-rmse:3.073215 \n#> [28] train-rmse:1.444955 test-rmse:3.091102 \n#> [29] train-rmse:1.412006 test-rmse:3.088701 \n#> [30] train-rmse:1.391050 test-rmse:3.080158 \n#> [31] train-rmse:1.367150 test-rmse:3.089364 \n#> [32] train-rmse:1.328217 test-rmse:3.094719 \n#> [33] train-rmse:1.313198 test-rmse:3.094374 \n#> [34] train-rmse:1.276102 test-rmse:3.094325 \n#> [35] train-rmse:1.246812 test-rmse:3.087894 \n#> [36] train-rmse:1.226088 test-rmse:3.090196 \n#> [37] train-rmse:1.210656 test-rmse:3.096260 \n#> [38] train-rmse:1.193339 test-rmse:3.106484 \n#> [39] train-rmse:1.182922 test-rmse:3.114498 \n#> [40] train-rmse:1.173687 test-rmse:3.112313 \n#> [41] train-rmse:1.160263 test-rmse:3.105608 \n#> [42] train-rmse:1.144793 test-rmse:3.123336 \n#> [43] train-rmse:1.131790 test-rmse:3.123308 \n#> [44] train-rmse:1.098635 test-rmse:3.122902 \n#> [45] train-rmse:1.089013 test-rmse:3.116918 \n#> [46] train-rmse:1.069510 test-rmse:3.120333 \n#> [47] train-rmse:1.055863 test-rmse:3.128643 \n#> [48] train-rmse:1.039188 test-rmse:3.131400 \n#> [49] train-rmse:1.006483 test-rmse:3.125944 \n#> [50] train-rmse:0.984161 test-rmse:3.137047 \n#> [51] train-rmse:0.968635 test-rmse:3.144871 \n#> [52] train-rmse:0.952427 test-rmse:3.139466 \n#> [53] train-rmse:0.930935 test-rmse:3.131501 \n#> [54] train-rmse:0.918740 test-rmse:3.138763 \n#> [55] train-rmse:0.899297 test-rmse:3.138424 \n#> [56] train-rmse:0.891931 test-rmse:3.147669 \n#> [57] train-rmse:0.886033 test-rmse:3.149456 \n#> [58] train-rmse:0.874594 test-rmse:3.151274 \n#> [59] train-rmse:0.860872 test-rmse:3.142111 \n#> [60] train-rmse:0.847046 test-rmse:3.137227 \n#> [61] train-rmse:0.840928 test-rmse:3.141958 \n#> [62] train-rmse:0.819154 test-rmse:3.147682 \n#> [63] train-rmse:0.807491 test-rmse:3.147035 \n#> [64] train-rmse:0.799199 test-rmse:3.149395 \n#> [65] train-rmse:0.785173 test-rmse:3.151627 \n#> [66] train-rmse:0.779256 test-rmse:3.155283 \n#> [67] train-rmse:0.767509 test-rmse:3.151191 \n#> [68] train-rmse:0.752762 test-rmse:3.140265 \n#> [69] train-rmse:0.747249 test-rmse:3.138050 \n#> [70] train-rmse:0.740448 test-rmse:3.142820 \n#> [1]  train-rmse:17.106814    test-rmse:17.250180 \n#> [2]  train-rmse:12.377038    test-rmse:12.772505 \n#> [3]  train-rmse:9.067686 test-rmse:9.741168 \n#> [4]  train-rmse:6.773091 test-rmse:7.754187 \n#> [5]  train-rmse:5.224824 test-rmse:6.525330 \n#> [6]  train-rmse:4.144283 test-rmse:5.684826 \n#> [7]  train-rmse:3.430951 test-rmse:5.265582 \n#> [8]  train-rmse:2.964625 test-rmse:4.986421 \n#> [9]  train-rmse:2.648607 test-rmse:4.886857 \n#> [10] train-rmse:2.416541 test-rmse:4.725083 \n#> [11] train-rmse:2.242877 test-rmse:4.528417 \n#> [12] train-rmse:2.122946 test-rmse:4.397233 \n#> [13] train-rmse:2.045196 test-rmse:4.360970 \n#> [14] train-rmse:1.977138 test-rmse:4.332111 \n#> [15] train-rmse:1.889960 test-rmse:4.321811 \n#> [16] train-rmse:1.814296 test-rmse:4.311061 \n#> [17] train-rmse:1.765740 test-rmse:4.285000 \n#> [18] train-rmse:1.720769 test-rmse:4.266297 \n#> [19] train-rmse:1.694682 test-rmse:4.260227 \n#> [20] train-rmse:1.673406 test-rmse:4.248314 \n#> [21] train-rmse:1.640517 test-rmse:4.242088 \n#> [22] train-rmse:1.613464 test-rmse:4.247419 \n#> [23] train-rmse:1.583350 test-rmse:4.238865 \n#> [24] train-rmse:1.540880 test-rmse:4.231536 \n#> [25] train-rmse:1.495060 test-rmse:4.252198 \n#> [26] train-rmse:1.464764 test-rmse:4.244365 \n#> [27] train-rmse:1.448202 test-rmse:4.198582 \n#> [28] train-rmse:1.425685 test-rmse:4.188188 \n#> [29] train-rmse:1.406633 test-rmse:4.192507 \n#> [30] train-rmse:1.395349 test-rmse:4.193589 \n#> [31] train-rmse:1.376466 test-rmse:4.190414 \n#> [32] train-rmse:1.336125 test-rmse:4.189229 \n#> [33] train-rmse:1.321900 test-rmse:4.154224 \n#> [34] train-rmse:1.305214 test-rmse:4.146886 \n#> [35] train-rmse:1.280075 test-rmse:4.143943 \n#> [36] train-rmse:1.259627 test-rmse:4.139292 \n#> [37] train-rmse:1.229639 test-rmse:4.132941 \n#> [38] train-rmse:1.220821 test-rmse:4.138425 \n#> [39] train-rmse:1.200879 test-rmse:4.129853 \n#> [40] train-rmse:1.186947 test-rmse:4.104963 \n#> [41] train-rmse:1.171959 test-rmse:4.102531 \n#> [42] train-rmse:1.141721 test-rmse:4.086933 \n#> [43] train-rmse:1.110592 test-rmse:4.099258 \n#> [44] train-rmse:1.095057 test-rmse:4.098190 \n#> [45] train-rmse:1.068194 test-rmse:4.097428 \n#> [46] train-rmse:1.046173 test-rmse:4.089784 \n#> [47] train-rmse:1.033784 test-rmse:4.076871 \n#> [48] train-rmse:1.010406 test-rmse:4.068876 \n#> [49] train-rmse:0.987087 test-rmse:4.080664 \n#> [50] train-rmse:0.973166 test-rmse:4.082035 \n#> [51] train-rmse:0.969764 test-rmse:4.082538 \n#> [52] train-rmse:0.963108 test-rmse:4.085631 \n#> [53] train-rmse:0.946885 test-rmse:4.088078 \n#> [54] train-rmse:0.928519 test-rmse:4.095592 \n#> [55] train-rmse:0.915581 test-rmse:4.091596 \n#> [56] train-rmse:0.905270 test-rmse:4.087808 \n#> [57] train-rmse:0.890827 test-rmse:4.082792 \n#> [58] train-rmse:0.879801 test-rmse:4.077462 \n#> [59] train-rmse:0.872119 test-rmse:4.084161 \n#> [60] train-rmse:0.853952 test-rmse:4.089163 \n#> [61] train-rmse:0.842883 test-rmse:4.088458 \n#> [62] train-rmse:0.823886 test-rmse:4.080652 \n#> [63] train-rmse:0.812544 test-rmse:4.080648 \n#> [64] train-rmse:0.800273 test-rmse:4.082317 \n#> [65] train-rmse:0.785629 test-rmse:4.078284 \n#> [66] train-rmse:0.764952 test-rmse:4.077993 \n#> [67] train-rmse:0.748780 test-rmse:4.068827 \n#> [68] train-rmse:0.740748 test-rmse:4.063622 \n#> [69] train-rmse:0.728420 test-rmse:4.071308 \n#> [70] train-rmse:0.713275 test-rmse:4.068478 \n#> [1]  train-rmse:16.843327    test-rmse:17.630448 \n#> [2]  train-rmse:12.230086    test-rmse:13.025456 \n#> [3]  train-rmse:9.033050 test-rmse:9.805279 \n#> [4]  train-rmse:6.802771 test-rmse:7.573263 \n#> [5]  train-rmse:5.277781 test-rmse:6.218051 \n#> [6]  train-rmse:4.181441 test-rmse:5.410309 \n#> [7]  train-rmse:3.508052 test-rmse:4.869971 \n#> [8]  train-rmse:3.041484 test-rmse:4.473363 \n#> [9]  train-rmse:2.702130 test-rmse:4.224139 \n#> [10] train-rmse:2.477018 test-rmse:4.049994 \n#> [11] train-rmse:2.295460 test-rmse:3.941098 \n#> [12] train-rmse:2.158347 test-rmse:3.840117 \n#> [13] train-rmse:2.087225 test-rmse:3.776254 \n#> [14] train-rmse:2.011524 test-rmse:3.755294 \n#> [15] train-rmse:1.923977 test-rmse:3.706982 \n#> [16] train-rmse:1.853436 test-rmse:3.697812 \n#> [17] train-rmse:1.789510 test-rmse:3.637131 \n#> [18] train-rmse:1.735547 test-rmse:3.615964 \n#> [19] train-rmse:1.706160 test-rmse:3.613229 \n#> [20] train-rmse:1.655062 test-rmse:3.593897 \n#> [21] train-rmse:1.627220 test-rmse:3.571851 \n#> [22] train-rmse:1.597271 test-rmse:3.553981 \n#> [23] train-rmse:1.560725 test-rmse:3.536863 \n#> [24] train-rmse:1.541744 test-rmse:3.518866 \n#> [25] train-rmse:1.476404 test-rmse:3.508670 \n#> [26] train-rmse:1.457586 test-rmse:3.496073 \n#> [27] train-rmse:1.401413 test-rmse:3.477101 \n#> [28] train-rmse:1.357625 test-rmse:3.455486 \n#> [29] train-rmse:1.343528 test-rmse:3.465116 \n#> [30] train-rmse:1.331339 test-rmse:3.450320 \n#> [31] train-rmse:1.304855 test-rmse:3.438186 \n#> [32] train-rmse:1.277205 test-rmse:3.429056 \n#> [33] train-rmse:1.267476 test-rmse:3.431337 \n#> [34] train-rmse:1.260252 test-rmse:3.429773 \n#> [35] train-rmse:1.239489 test-rmse:3.418485 \n#> [36] train-rmse:1.211899 test-rmse:3.418756 \n#> [37] train-rmse:1.175095 test-rmse:3.421404 \n#> [38] train-rmse:1.168278 test-rmse:3.431754 \n#> [39] train-rmse:1.145681 test-rmse:3.428072 \n#> [40] train-rmse:1.110845 test-rmse:3.425727 \n#> [41] train-rmse:1.099152 test-rmse:3.415371 \n#> [42] train-rmse:1.072962 test-rmse:3.406520 \n#> [43] train-rmse:1.063818 test-rmse:3.409243 \n#> [44] train-rmse:1.055725 test-rmse:3.402111 \n#> [45] train-rmse:1.039420 test-rmse:3.396152 \n#> [46] train-rmse:1.018120 test-rmse:3.388016 \n#> [47] train-rmse:1.012710 test-rmse:3.396671 \n#> [48] train-rmse:1.007173 test-rmse:3.396966 \n#> [49] train-rmse:0.988877 test-rmse:3.403745 \n#> [50] train-rmse:0.964777 test-rmse:3.414491 \n#> [51] train-rmse:0.950579 test-rmse:3.420110 \n#> [52] train-rmse:0.942251 test-rmse:3.408809 \n#> [53] train-rmse:0.934431 test-rmse:3.404427 \n#> [54] train-rmse:0.910658 test-rmse:3.406597 \n#> [55] train-rmse:0.897827 test-rmse:3.399108 \n#> [56] train-rmse:0.881912 test-rmse:3.396343 \n#> [57] train-rmse:0.864401 test-rmse:3.394349 \n#> [58] train-rmse:0.845182 test-rmse:3.403125 \n#> [59] train-rmse:0.836448 test-rmse:3.400948 \n#> [60] train-rmse:0.820609 test-rmse:3.404693 \n#> [61] train-rmse:0.800922 test-rmse:3.401149 \n#> [62] train-rmse:0.782807 test-rmse:3.398375 \n#> [63] train-rmse:0.763460 test-rmse:3.392056 \n#> [64] train-rmse:0.752839 test-rmse:3.403859 \n#> [65] train-rmse:0.745040 test-rmse:3.398278 \n#> [66] train-rmse:0.738352 test-rmse:3.407817 \n#> [67] train-rmse:0.728654 test-rmse:3.412509 \n#> [68] train-rmse:0.718183 test-rmse:3.410390 \n#> [69] train-rmse:0.708034 test-rmse:3.409943 \n#> [70] train-rmse:0.691025 test-rmse:3.409508 \n#> [1]  train-rmse:16.949845    test-rmse:17.824195 \n#> [2]  train-rmse:12.341275    test-rmse:13.165814 \n#> [3]  train-rmse:9.089034 test-rmse:10.017984 \n#> [4]  train-rmse:6.861677 test-rmse:7.900584 \n#> [5]  train-rmse:5.303419 test-rmse:6.384306 \n#> [6]  train-rmse:4.259769 test-rmse:5.392201 \n#> [7]  train-rmse:3.576851 test-rmse:4.763239 \n#> [8]  train-rmse:3.067836 test-rmse:4.400579 \n#> [9]  train-rmse:2.758527 test-rmse:4.167037 \n#> [10] train-rmse:2.537287 test-rmse:4.075754 \n#> [11] train-rmse:2.385793 test-rmse:4.035144 \n#> [12] train-rmse:2.275513 test-rmse:3.953069 \n#> [13] train-rmse:2.200654 test-rmse:3.912271 \n#> [14] train-rmse:2.080634 test-rmse:3.865646 \n#> [15] train-rmse:2.031579 test-rmse:3.864741 \n#> [16] train-rmse:1.952998 test-rmse:3.852482 \n#> [17] train-rmse:1.925618 test-rmse:3.874928 \n#> [18] train-rmse:1.880780 test-rmse:3.859902 \n#> [19] train-rmse:1.842236 test-rmse:3.840464 \n#> [20] train-rmse:1.801217 test-rmse:3.860147 \n#> [21] train-rmse:1.734936 test-rmse:3.875298 \n#> [22] train-rmse:1.693691 test-rmse:3.847974 \n#> [23] train-rmse:1.664794 test-rmse:3.830507 \n#> [24] train-rmse:1.632001 test-rmse:3.840560 \n#> [25] train-rmse:1.585405 test-rmse:3.813598 \n#> [26] train-rmse:1.562326 test-rmse:3.813233 \n#> [27] train-rmse:1.513265 test-rmse:3.817980 \n#> [28] train-rmse:1.496845 test-rmse:3.816506 \n#> [29] train-rmse:1.482189 test-rmse:3.831948 \n#> [30] train-rmse:1.463350 test-rmse:3.821553 \n#> [31] train-rmse:1.447672 test-rmse:3.823210 \n#> [32] train-rmse:1.436055 test-rmse:3.821823 \n#> [33] train-rmse:1.409757 test-rmse:3.811259 \n#> [34] train-rmse:1.369539 test-rmse:3.807964 \n#> [35] train-rmse:1.343920 test-rmse:3.805067 \n#> [36] train-rmse:1.314949 test-rmse:3.817181 \n#> [37] train-rmse:1.293520 test-rmse:3.817634 \n#> [38] train-rmse:1.270368 test-rmse:3.808399 \n#> [39] train-rmse:1.249667 test-rmse:3.813187 \n#> [40] train-rmse:1.236692 test-rmse:3.811879 \n#> [41] train-rmse:1.213191 test-rmse:3.798800 \n#> [42] train-rmse:1.176563 test-rmse:3.798468 \n#> [43] train-rmse:1.165220 test-rmse:3.800355 \n#> [44] train-rmse:1.135637 test-rmse:3.807197 \n#> [45] train-rmse:1.115599 test-rmse:3.788496 \n#> [46] train-rmse:1.103269 test-rmse:3.792169 \n#> [47] train-rmse:1.091085 test-rmse:3.784152 \n#> [48] train-rmse:1.062291 test-rmse:3.779102 \n#> [49] train-rmse:1.037378 test-rmse:3.759805 \n#> [50] train-rmse:1.010207 test-rmse:3.756861 \n#> [51] train-rmse:0.994277 test-rmse:3.751901 \n#> [52] train-rmse:0.976165 test-rmse:3.748819 \n#> [53] train-rmse:0.953688 test-rmse:3.742070 \n#> [54] train-rmse:0.934867 test-rmse:3.747525 \n#> [55] train-rmse:0.918004 test-rmse:3.745348 \n#> [56] train-rmse:0.906836 test-rmse:3.749111 \n#> [57] train-rmse:0.890291 test-rmse:3.746518 \n#> [58] train-rmse:0.879625 test-rmse:3.747108 \n#> [59] train-rmse:0.864893 test-rmse:3.742562 \n#> [60] train-rmse:0.844640 test-rmse:3.739284 \n#> [61] train-rmse:0.840344 test-rmse:3.736408 \n#> [62] train-rmse:0.828862 test-rmse:3.729288 \n#> [63] train-rmse:0.814817 test-rmse:3.730951 \n#> [64] train-rmse:0.808629 test-rmse:3.725393 \n#> [65] train-rmse:0.802357 test-rmse:3.725300 \n#> [66] train-rmse:0.785755 test-rmse:3.723347 \n#> [67] train-rmse:0.778525 test-rmse:3.717697 \n#> [68] train-rmse:0.767660 test-rmse:3.720729 \n#> [69] train-rmse:0.758042 test-rmse:3.722188 \n#> [70] train-rmse:0.751187 test-rmse:3.714882 \n#> [1]  train-rmse:17.164234    test-rmse:17.290856 \n#> [2]  train-rmse:12.385317    test-rmse:12.873364 \n#> [3]  train-rmse:9.038469 test-rmse:9.814048 \n#> [4]  train-rmse:6.712088 test-rmse:7.904425 \n#> [5]  train-rmse:5.123116 test-rmse:6.569116 \n#> [6]  train-rmse:4.057769 test-rmse:5.819921 \n#> [7]  train-rmse:3.311019 test-rmse:5.398898 \n#> [8]  train-rmse:2.833277 test-rmse:5.063300 \n#> [9]  train-rmse:2.526343 test-rmse:4.910978 \n#> [10] train-rmse:2.330799 test-rmse:4.824775 \n#> [11] train-rmse:2.190479 test-rmse:4.778139 \n#> [12] train-rmse:2.039613 test-rmse:4.722453 \n#> [13] train-rmse:1.937872 test-rmse:4.606422 \n#> [14] train-rmse:1.879678 test-rmse:4.581573 \n#> [15] train-rmse:1.825661 test-rmse:4.561776 \n#> [16] train-rmse:1.759723 test-rmse:4.534257 \n#> [17] train-rmse:1.710874 test-rmse:4.524862 \n#> [18] train-rmse:1.664751 test-rmse:4.508217 \n#> [19] train-rmse:1.626281 test-rmse:4.503706 \n#> [20] train-rmse:1.597528 test-rmse:4.489805 \n#> [21] train-rmse:1.566941 test-rmse:4.483998 \n#> [22] train-rmse:1.531071 test-rmse:4.464565 \n#> [23] train-rmse:1.487685 test-rmse:4.456293 \n#> [24] train-rmse:1.449060 test-rmse:4.454694 \n#> [25] train-rmse:1.413143 test-rmse:4.465825 \n#> [26] train-rmse:1.382532 test-rmse:4.457578 \n#> [27] train-rmse:1.340500 test-rmse:4.452180 \n#> [28] train-rmse:1.314598 test-rmse:4.453048 \n#> [29] train-rmse:1.299256 test-rmse:4.453786 \n#> [30] train-rmse:1.271586 test-rmse:4.434573 \n#> [31] train-rmse:1.229094 test-rmse:4.418154 \n#> [32] train-rmse:1.197729 test-rmse:4.394206 \n#> [33] train-rmse:1.184765 test-rmse:4.397141 \n#> [34] train-rmse:1.166993 test-rmse:4.383526 \n#> [35] train-rmse:1.152211 test-rmse:4.381627 \n#> [36] train-rmse:1.141174 test-rmse:4.381599 \n#> [37] train-rmse:1.112470 test-rmse:4.372176 \n#> [38] train-rmse:1.097799 test-rmse:4.376624 \n#> [39] train-rmse:1.067566 test-rmse:4.378681 \n#> [40] train-rmse:1.053726 test-rmse:4.372034 \n#> [41] train-rmse:1.034273 test-rmse:4.369001 \n#> [42] train-rmse:1.011066 test-rmse:4.363326 \n#> [43] train-rmse:0.985568 test-rmse:4.371523 \n#> [44] train-rmse:0.967149 test-rmse:4.371314 \n#> [45] train-rmse:0.951092 test-rmse:4.372927 \n#> [46] train-rmse:0.945378 test-rmse:4.373406 \n#> [47] train-rmse:0.931181 test-rmse:4.370639 \n#> [48] train-rmse:0.919773 test-rmse:4.368897 \n#> [49] train-rmse:0.898064 test-rmse:4.364811 \n#> [50] train-rmse:0.886152 test-rmse:4.364254 \n#> [51] train-rmse:0.867474 test-rmse:4.369778 \n#> [52] train-rmse:0.860538 test-rmse:4.361710 \n#> [53] train-rmse:0.855384 test-rmse:4.357094 \n#> [54] train-rmse:0.844501 test-rmse:4.357192 \n#> [55] train-rmse:0.828474 test-rmse:4.347019 \n#> [56] train-rmse:0.805482 test-rmse:4.339889 \n#> [57] train-rmse:0.796325 test-rmse:4.339752 \n#> [58] train-rmse:0.783212 test-rmse:4.338486 \n#> [59] train-rmse:0.774764 test-rmse:4.337017 \n#> [60] train-rmse:0.754201 test-rmse:4.343769 \n#> [61] train-rmse:0.748837 test-rmse:4.343565 \n#> [62] train-rmse:0.734607 test-rmse:4.341011 \n#> [63] train-rmse:0.719883 test-rmse:4.342027 \n#> [64] train-rmse:0.711308 test-rmse:4.341379 \n#> [65] train-rmse:0.697173 test-rmse:4.335417 \n#> [66] train-rmse:0.681067 test-rmse:4.336891 \n#> [67] train-rmse:0.675150 test-rmse:4.334851 \n#> [68] train-rmse:0.664202 test-rmse:4.336332 \n#> [69] train-rmse:0.653607 test-rmse:4.336920 \n#> [70] train-rmse:0.646397 test-rmse:4.327037 \n#> [1]  train-rmse:17.339362    test-rmse:16.886611 \n#> [2]  train-rmse:12.586488    test-rmse:12.335260 \n#> [3]  train-rmse:9.224821 test-rmse:9.145505 \n#> [4]  train-rmse:6.890770 test-rmse:7.107080 \n#> [5]  train-rmse:5.279757 test-rmse:5.951127 \n#> [6]  train-rmse:4.172018 test-rmse:5.249710 \n#> [7]  train-rmse:3.427074 test-rmse:4.850307 \n#> [8]  train-rmse:2.963032 test-rmse:4.625386 \n#> [9]  train-rmse:2.662795 test-rmse:4.498435 \n#> [10] train-rmse:2.442344 test-rmse:4.363990 \n#> [11] train-rmse:2.308797 test-rmse:4.349434 \n#> [12] train-rmse:2.185629 test-rmse:4.349068 \n#> [13] train-rmse:2.084013 test-rmse:4.316079 \n#> [14] train-rmse:2.019330 test-rmse:4.297881 \n#> [15] train-rmse:1.966460 test-rmse:4.297906 \n#> [16] train-rmse:1.929337 test-rmse:4.293474 \n#> [17] train-rmse:1.893997 test-rmse:4.273471 \n#> [18] train-rmse:1.852781 test-rmse:4.273586 \n#> [19] train-rmse:1.800863 test-rmse:4.273792 \n#> [20] train-rmse:1.749531 test-rmse:4.262150 \n#> [21] train-rmse:1.722285 test-rmse:4.259934 \n#> [22] train-rmse:1.648740 test-rmse:4.265721 \n#> [23] train-rmse:1.614853 test-rmse:4.250797 \n#> [24] train-rmse:1.587401 test-rmse:4.249821 \n#> [25] train-rmse:1.549045 test-rmse:4.246630 \n#> [26] train-rmse:1.525117 test-rmse:4.224750 \n#> [27] train-rmse:1.489872 test-rmse:4.221634 \n#> [28] train-rmse:1.472498 test-rmse:4.218214 \n#> [29] train-rmse:1.428341 test-rmse:4.228701 \n#> [30] train-rmse:1.409449 test-rmse:4.228808 \n#> [31] train-rmse:1.348274 test-rmse:4.225690 \n#> [32] train-rmse:1.326855 test-rmse:4.216593 \n#> [33] train-rmse:1.305855 test-rmse:4.216675 \n#> [34] train-rmse:1.290848 test-rmse:4.212561 \n#> [35] train-rmse:1.279023 test-rmse:4.215831 \n#> [36] train-rmse:1.266642 test-rmse:4.217184 \n#> [37] train-rmse:1.250990 test-rmse:4.218370 \n#> [38] train-rmse:1.234323 test-rmse:4.219821 \n#> [39] train-rmse:1.221149 test-rmse:4.224593 \n#> [40] train-rmse:1.212641 test-rmse:4.227159 \n#> [41] train-rmse:1.188258 test-rmse:4.228749 \n#> [42] train-rmse:1.152960 test-rmse:4.223627 \n#> [43] train-rmse:1.132255 test-rmse:4.227086 \n#> [44] train-rmse:1.106895 test-rmse:4.225276 \n#> [45] train-rmse:1.096884 test-rmse:4.214814 \n#> [46] train-rmse:1.084345 test-rmse:4.218354 \n#> [47] train-rmse:1.058253 test-rmse:4.217997 \n#> [48] train-rmse:1.029234 test-rmse:4.218360 \n#> [49] train-rmse:1.002912 test-rmse:4.218650 \n#> [50] train-rmse:0.988225 test-rmse:4.207876 \n#> [51] train-rmse:0.974329 test-rmse:4.206019 \n#> [52] train-rmse:0.961530 test-rmse:4.213996 \n#> [53] train-rmse:0.953320 test-rmse:4.212837 \n#> [54] train-rmse:0.943970 test-rmse:4.218621 \n#> [55] train-rmse:0.936817 test-rmse:4.221017 \n#> [56] train-rmse:0.927453 test-rmse:4.219435 \n#> [57] train-rmse:0.921098 test-rmse:4.225236 \n#> [58] train-rmse:0.904326 test-rmse:4.219251 \n#> [59] train-rmse:0.890045 test-rmse:4.223911 \n#> [60] train-rmse:0.881177 test-rmse:4.227973 \n#> [61] train-rmse:0.870725 test-rmse:4.235689 \n#> [62] train-rmse:0.847676 test-rmse:4.243785 \n#> [63] train-rmse:0.839300 test-rmse:4.245725 \n#> [64] train-rmse:0.820093 test-rmse:4.249943 \n#> [65] train-rmse:0.812403 test-rmse:4.243110 \n#> [66] train-rmse:0.804941 test-rmse:4.240816 \n#> [67] train-rmse:0.789119 test-rmse:4.244490 \n#> [68] train-rmse:0.778546 test-rmse:4.248261 \n#> [69] train-rmse:0.768634 test-rmse:4.253473 \n#> [70] train-rmse:0.753906 test-rmse:4.255495 \n#> [1]  train-rmse:17.043464    test-rmse:17.270172 \n#> [2]  train-rmse:12.361571    test-rmse:12.583885 \n#> [3]  train-rmse:9.066605 test-rmse:9.319331 \n#> [4]  train-rmse:6.774027 test-rmse:7.262226 \n#> [5]  train-rmse:5.211588 test-rmse:5.923800 \n#> [6]  train-rmse:4.099974 test-rmse:5.122749 \n#> [7]  train-rmse:3.398566 test-rmse:4.580629 \n#> [8]  train-rmse:2.931773 test-rmse:4.301766 \n#> [9]  train-rmse:2.582530 test-rmse:4.112700 \n#> [10] train-rmse:2.355689 test-rmse:3.955240 \n#> [11] train-rmse:2.214995 test-rmse:3.847516 \n#> [12] train-rmse:2.099497 test-rmse:3.745548 \n#> [13] train-rmse:2.035166 test-rmse:3.738093 \n#> [14] train-rmse:1.990925 test-rmse:3.708766 \n#> [15] train-rmse:1.946326 test-rmse:3.659753 \n#> [16] train-rmse:1.920013 test-rmse:3.660893 \n#> [17] train-rmse:1.866716 test-rmse:3.627870 \n#> [18] train-rmse:1.803806 test-rmse:3.578629 \n#> [19] train-rmse:1.736163 test-rmse:3.574666 \n#> [20] train-rmse:1.714589 test-rmse:3.559812 \n#> [21] train-rmse:1.681649 test-rmse:3.542513 \n#> [22] train-rmse:1.656912 test-rmse:3.531143 \n#> [23] train-rmse:1.622786 test-rmse:3.529608 \n#> [24] train-rmse:1.600940 test-rmse:3.519877 \n#> [25] train-rmse:1.566961 test-rmse:3.510495 \n#> [26] train-rmse:1.547053 test-rmse:3.501320 \n#> [27] train-rmse:1.531756 test-rmse:3.500481 \n#> [28] train-rmse:1.485721 test-rmse:3.460774 \n#> [29] train-rmse:1.462208 test-rmse:3.464586 \n#> [30] train-rmse:1.435784 test-rmse:3.436003 \n#> [31] train-rmse:1.422993 test-rmse:3.423357 \n#> [32] train-rmse:1.396143 test-rmse:3.423470 \n#> [33] train-rmse:1.382470 test-rmse:3.419188 \n#> [34] train-rmse:1.364982 test-rmse:3.422158 \n#> [35] train-rmse:1.354400 test-rmse:3.425399 \n#> [36] train-rmse:1.349015 test-rmse:3.425106 \n#> [37] train-rmse:1.331291 test-rmse:3.417051 \n#> [38] train-rmse:1.322026 test-rmse:3.414100 \n#> [39] train-rmse:1.301484 test-rmse:3.407506 \n#> [40] train-rmse:1.287670 test-rmse:3.409145 \n#> [41] train-rmse:1.273148 test-rmse:3.408364 \n#> [42] train-rmse:1.243073 test-rmse:3.414236 \n#> [43] train-rmse:1.217938 test-rmse:3.411057 \n#> [44] train-rmse:1.204210 test-rmse:3.418863 \n#> [45] train-rmse:1.199341 test-rmse:3.411529 \n#> [46] train-rmse:1.174599 test-rmse:3.378846 \n#> [47] train-rmse:1.163850 test-rmse:3.382828 \n#> [48] train-rmse:1.119012 test-rmse:3.382510 \n#> [49] train-rmse:1.088558 test-rmse:3.381638 \n#> [50] train-rmse:1.081114 test-rmse:3.382802 \n#> [51] train-rmse:1.064135 test-rmse:3.381489 \n#> [52] train-rmse:1.043606 test-rmse:3.398191 \n#> [53] train-rmse:1.029898 test-rmse:3.387862 \n#> [54] train-rmse:1.014860 test-rmse:3.381437 \n#> [55] train-rmse:0.986339 test-rmse:3.373278 \n#> [56] train-rmse:0.981199 test-rmse:3.367442 \n#> [57] train-rmse:0.968102 test-rmse:3.368859 \n#> [58] train-rmse:0.959230 test-rmse:3.364833 \n#> [59] train-rmse:0.951625 test-rmse:3.361161 \n#> [60] train-rmse:0.942096 test-rmse:3.359646 \n#> [61] train-rmse:0.929283 test-rmse:3.348098 \n#> [62] train-rmse:0.921466 test-rmse:3.347140 \n#> [63] train-rmse:0.902562 test-rmse:3.349948 \n#> [64] train-rmse:0.895032 test-rmse:3.351796 \n#> [65] train-rmse:0.873948 test-rmse:3.367141 \n#> [66] train-rmse:0.867975 test-rmse:3.363937 \n#> [67] train-rmse:0.861314 test-rmse:3.372426 \n#> [68] train-rmse:0.843317 test-rmse:3.368465 \n#> [69] train-rmse:0.837294 test-rmse:3.365844 \n#> [70] train-rmse:0.831373 test-rmse:3.364705 \n#> [1]  train-rmse:17.456190    test-rmse:16.653549 \n#> [2]  train-rmse:12.648194    test-rmse:12.092145 \n#> [3]  train-rmse:9.313788 test-rmse:9.082147 \n#> [4]  train-rmse:6.970995 test-rmse:7.073219 \n#> [5]  train-rmse:5.380071 test-rmse:5.699679 \n#> [6]  train-rmse:4.270889 test-rmse:4.754919 \n#> [7]  train-rmse:3.515352 test-rmse:4.246503 \n#> [8]  train-rmse:3.029775 test-rmse:3.914680 \n#> [9]  train-rmse:2.697642 test-rmse:3.754900 \n#> [10] train-rmse:2.504079 test-rmse:3.652176 \n#> [11] train-rmse:2.343365 test-rmse:3.606815 \n#> [12] train-rmse:2.220072 test-rmse:3.558213 \n#> [13] train-rmse:2.124556 test-rmse:3.502181 \n#> [14] train-rmse:2.072974 test-rmse:3.465366 \n#> [15] train-rmse:2.000467 test-rmse:3.433915 \n#> [16] train-rmse:1.924200 test-rmse:3.384735 \n#> [17] train-rmse:1.856506 test-rmse:3.363236 \n#> [18] train-rmse:1.816794 test-rmse:3.357258 \n#> [19] train-rmse:1.750301 test-rmse:3.319879 \n#> [20] train-rmse:1.723350 test-rmse:3.289218 \n#> [21] train-rmse:1.700047 test-rmse:3.265925 \n#> [22] train-rmse:1.653667 test-rmse:3.256395 \n#> [23] train-rmse:1.628537 test-rmse:3.256971 \n#> [24] train-rmse:1.602954 test-rmse:3.249408 \n#> [25] train-rmse:1.570623 test-rmse:3.248375 \n#> [26] train-rmse:1.556067 test-rmse:3.246089 \n#> [27] train-rmse:1.530178 test-rmse:3.247561 \n#> [28] train-rmse:1.504508 test-rmse:3.226086 \n#> [29] train-rmse:1.451478 test-rmse:3.204355 \n#> [30] train-rmse:1.442616 test-rmse:3.207756 \n#> [31] train-rmse:1.379040 test-rmse:3.207266 \n#> [32] train-rmse:1.361567 test-rmse:3.212331 \n#> [33] train-rmse:1.330051 test-rmse:3.209703 \n#> [34] train-rmse:1.309706 test-rmse:3.208598 \n#> [35] train-rmse:1.300922 test-rmse:3.210971 \n#> [36] train-rmse:1.291950 test-rmse:3.213551 \n#> [37] train-rmse:1.265668 test-rmse:3.217192 \n#> [38] train-rmse:1.242499 test-rmse:3.210560 \n#> [39] train-rmse:1.234691 test-rmse:3.209650 \n#> [40] train-rmse:1.203055 test-rmse:3.220415 \n#> [41] train-rmse:1.196558 test-rmse:3.214507 \n#> [42] train-rmse:1.183184 test-rmse:3.203664 \n#> [43] train-rmse:1.162531 test-rmse:3.211341 \n#> [44] train-rmse:1.144301 test-rmse:3.200769 \n#> [45] train-rmse:1.127309 test-rmse:3.210206 \n#> [46] train-rmse:1.104384 test-rmse:3.196201 \n#> [47] train-rmse:1.093444 test-rmse:3.200203 \n#> [48] train-rmse:1.084574 test-rmse:3.194495 \n#> [49] train-rmse:1.065494 test-rmse:3.199787 \n#> [50] train-rmse:1.047072 test-rmse:3.183444 \n#> [51] train-rmse:1.034626 test-rmse:3.183327 \n#> [52] train-rmse:1.012602 test-rmse:3.176554 \n#> [53] train-rmse:0.990191 test-rmse:3.176441 \n#> [54] train-rmse:0.981658 test-rmse:3.171199 \n#> [55] train-rmse:0.971126 test-rmse:3.163631 \n#> [56] train-rmse:0.956953 test-rmse:3.168607 \n#> [57] train-rmse:0.946457 test-rmse:3.174608 \n#> [58] train-rmse:0.938780 test-rmse:3.175984 \n#> [59] train-rmse:0.931616 test-rmse:3.169231 \n#> [60] train-rmse:0.922506 test-rmse:3.166960 \n#> [61] train-rmse:0.913599 test-rmse:3.160929 \n#> [62] train-rmse:0.902772 test-rmse:3.155219 \n#> [63] train-rmse:0.890311 test-rmse:3.152159 \n#> [64] train-rmse:0.880465 test-rmse:3.152945 \n#> [65] train-rmse:0.871798 test-rmse:3.151441 \n#> [66] train-rmse:0.859197 test-rmse:3.153839 \n#> [67] train-rmse:0.846615 test-rmse:3.156859 \n#> [68] train-rmse:0.831658 test-rmse:3.157734 \n#> [69] train-rmse:0.824605 test-rmse:3.156717 \n#> [70] train-rmse:0.804884 test-rmse:3.162866 \n#> [1]  train-rmse:17.485217    test-rmse:16.537413 \n#> [2]  train-rmse:12.660004    test-rmse:11.973589 \n#> [3]  train-rmse:9.300846 test-rmse:8.862548 \n#> [4]  train-rmse:6.987644 test-rmse:6.763105 \n#> [5]  train-rmse:5.339208 test-rmse:5.351333 \n#> [6]  train-rmse:4.226457 test-rmse:4.455682 \n#> [7]  train-rmse:3.502347 test-rmse:3.927905 \n#> [8]  train-rmse:3.019808 test-rmse:3.673145 \n#> [9]  train-rmse:2.685198 test-rmse:3.550330 \n#> [10] train-rmse:2.471760 test-rmse:3.448850 \n#> [11] train-rmse:2.301642 test-rmse:3.351629 \n#> [12] train-rmse:2.180760 test-rmse:3.323814 \n#> [13] train-rmse:2.106010 test-rmse:3.271801 \n#> [14] train-rmse:2.035941 test-rmse:3.256801 \n#> [15] train-rmse:1.951739 test-rmse:3.261532 \n#> [16] train-rmse:1.884756 test-rmse:3.243790 \n#> [17] train-rmse:1.851105 test-rmse:3.235069 \n#> [18] train-rmse:1.807456 test-rmse:3.204093 \n#> [19] train-rmse:1.770250 test-rmse:3.183853 \n#> [20] train-rmse:1.708237 test-rmse:3.175278 \n#> [21] train-rmse:1.657705 test-rmse:3.158464 \n#> [22] train-rmse:1.626835 test-rmse:3.152306 \n#> [23] train-rmse:1.590751 test-rmse:3.154191 \n#> [24] train-rmse:1.563488 test-rmse:3.151662 \n#> [25] train-rmse:1.528616 test-rmse:3.149880 \n#> [26] train-rmse:1.491629 test-rmse:3.143521 \n#> [27] train-rmse:1.476580 test-rmse:3.132445 \n#> [28] train-rmse:1.432317 test-rmse:3.117654 \n#> [29] train-rmse:1.412865 test-rmse:3.097791 \n#> [30] train-rmse:1.378458 test-rmse:3.103508 \n#> [31] train-rmse:1.332700 test-rmse:3.105340 \n#> [32] train-rmse:1.291118 test-rmse:3.082222 \n#> [33] train-rmse:1.263577 test-rmse:3.082855 \n#> [34] train-rmse:1.237787 test-rmse:3.076044 \n#> [35] train-rmse:1.228565 test-rmse:3.071678 \n#> [36] train-rmse:1.221586 test-rmse:3.067981 \n#> [37] train-rmse:1.202036 test-rmse:3.076470 \n#> [38] train-rmse:1.182295 test-rmse:3.077216 \n#> [39] train-rmse:1.174065 test-rmse:3.069785 \n#> [40] train-rmse:1.143644 test-rmse:3.075048 \n#> [41] train-rmse:1.118070 test-rmse:3.086533 \n#> [42] train-rmse:1.083684 test-rmse:3.076990 \n#> [43] train-rmse:1.067657 test-rmse:3.085092 \n#> [44] train-rmse:1.056737 test-rmse:3.075374 \n#> [45] train-rmse:1.042070 test-rmse:3.074781 \n#> [46] train-rmse:1.034100 test-rmse:3.069572 \n#> [47] train-rmse:1.018872 test-rmse:3.060232 \n#> [48] train-rmse:1.009841 test-rmse:3.057160 \n#> [49] train-rmse:0.988343 test-rmse:3.040026 \n#> [50] train-rmse:0.969062 test-rmse:3.043817 \n#> [51] train-rmse:0.948150 test-rmse:3.044994 \n#> [52] train-rmse:0.931555 test-rmse:3.055855 \n#> [53] train-rmse:0.927358 test-rmse:3.052031 \n#> [54] train-rmse:0.919497 test-rmse:3.048521 \n#> [55] train-rmse:0.903379 test-rmse:3.048413 \n#> [56] train-rmse:0.891003 test-rmse:3.043118 \n#> [57] train-rmse:0.878901 test-rmse:3.054984 \n#> [58] train-rmse:0.868691 test-rmse:3.054022 \n#> [59] train-rmse:0.855001 test-rmse:3.048966 \n#> [60] train-rmse:0.834860 test-rmse:3.035608 \n#> [61] train-rmse:0.816049 test-rmse:3.037272 \n#> [62] train-rmse:0.798031 test-rmse:3.025575 \n#> [63] train-rmse:0.783671 test-rmse:3.025780 \n#> [64] train-rmse:0.772990 test-rmse:3.024913 \n#> [65] train-rmse:0.760798 test-rmse:3.019602 \n#> [66] train-rmse:0.743240 test-rmse:3.011554 \n#> [67] train-rmse:0.733533 test-rmse:3.008570 \n#> [68] train-rmse:0.721323 test-rmse:3.005192 \n#> [69] train-rmse:0.704381 test-rmse:3.005989 \n#> [70] train-rmse:0.701263 test-rmse:3.002740 \n#> [1]  train-rmse:17.111532    test-rmse:17.435272 \n#> [2]  train-rmse:12.387211    test-rmse:13.025047 \n#> [3]  train-rmse:9.085726 test-rmse:9.873093 \n#> [4]  train-rmse:6.789982 test-rmse:7.752298 \n#> [5]  train-rmse:5.207629 test-rmse:6.384945 \n#> [6]  train-rmse:4.115838 test-rmse:5.447950 \n#> [7]  train-rmse:3.366020 test-rmse:4.894496 \n#> [8]  train-rmse:2.879771 test-rmse:4.580277 \n#> [9]  train-rmse:2.569030 test-rmse:4.453151 \n#> [10] train-rmse:2.325202 test-rmse:4.457078 \n#> [11] train-rmse:2.191495 test-rmse:4.411079 \n#> [12] train-rmse:2.052944 test-rmse:4.384738 \n#> [13] train-rmse:1.977015 test-rmse:4.345249 \n#> [14] train-rmse:1.924209 test-rmse:4.342288 \n#> [15] train-rmse:1.831577 test-rmse:4.261086 \n#> [16] train-rmse:1.796415 test-rmse:4.231911 \n#> [17] train-rmse:1.754201 test-rmse:4.226254 \n#> [18] train-rmse:1.701770 test-rmse:4.220641 \n#> [19] train-rmse:1.634846 test-rmse:4.220300 \n#> [20] train-rmse:1.616856 test-rmse:4.206086 \n#> [21] train-rmse:1.572590 test-rmse:4.177235 \n#> [22] train-rmse:1.546168 test-rmse:4.169682 \n#> [23] train-rmse:1.504574 test-rmse:4.170857 \n#> [24] train-rmse:1.482746 test-rmse:4.172313 \n#> [25] train-rmse:1.430754 test-rmse:4.178851 \n#> [26] train-rmse:1.404922 test-rmse:4.172277 \n#> [27] train-rmse:1.374729 test-rmse:4.174905 \n#> [28] train-rmse:1.323425 test-rmse:4.161761 \n#> [29] train-rmse:1.307501 test-rmse:4.154302 \n#> [30] train-rmse:1.282739 test-rmse:4.150683 \n#> [31] train-rmse:1.251377 test-rmse:4.131608 \n#> [32] train-rmse:1.221794 test-rmse:4.143298 \n#> [33] train-rmse:1.185037 test-rmse:4.148625 \n#> [34] train-rmse:1.150993 test-rmse:4.156402 \n#> [35] train-rmse:1.119772 test-rmse:4.160789 \n#> [36] train-rmse:1.090457 test-rmse:4.160090 \n#> [37] train-rmse:1.060700 test-rmse:4.162218 \n#> [38] train-rmse:1.044813 test-rmse:4.165447 \n#> [39] train-rmse:1.032574 test-rmse:4.157378 \n#> [40] train-rmse:1.013405 test-rmse:4.153388 \n#> [41] train-rmse:1.006148 test-rmse:4.142999 \n#> [42] train-rmse:1.000142 test-rmse:4.140634 \n#> [43] train-rmse:0.976399 test-rmse:4.131345 \n#> [44] train-rmse:0.963216 test-rmse:4.132121 \n#> [45] train-rmse:0.943239 test-rmse:4.130291 \n#> [46] train-rmse:0.933755 test-rmse:4.129046 \n#> [47] train-rmse:0.923550 test-rmse:4.116899 \n#> [48] train-rmse:0.901284 test-rmse:4.114111 \n#> [49] train-rmse:0.884423 test-rmse:4.106702 \n#> [50] train-rmse:0.870152 test-rmse:4.107586 \n#> [51] train-rmse:0.859978 test-rmse:4.108229 \n#> [52] train-rmse:0.840108 test-rmse:4.108760 \n#> [53] train-rmse:0.816931 test-rmse:4.108813 \n#> [54] train-rmse:0.811576 test-rmse:4.109076 \n#> [55] train-rmse:0.800264 test-rmse:4.109824 \n#> [56] train-rmse:0.781994 test-rmse:4.102288 \n#> [57] train-rmse:0.775396 test-rmse:4.101467 \n#> [58] train-rmse:0.767829 test-rmse:4.104483 \n#> [59] train-rmse:0.760129 test-rmse:4.106617 \n#> [60] train-rmse:0.754196 test-rmse:4.106342 \n#> [61] train-rmse:0.749437 test-rmse:4.105491 \n#> [62] train-rmse:0.731089 test-rmse:4.104831 \n#> [63] train-rmse:0.715924 test-rmse:4.100628 \n#> [64] train-rmse:0.704701 test-rmse:4.105410 \n#> [65] train-rmse:0.693998 test-rmse:4.100563 \n#> [66] train-rmse:0.683421 test-rmse:4.099929 \n#> [67] train-rmse:0.674604 test-rmse:4.101201 \n#> [68] train-rmse:0.667805 test-rmse:4.101812 \n#> [69] train-rmse:0.654981 test-rmse:4.102751 \n#> [70] train-rmse:0.647825 test-rmse:4.100728 \n#> [1]  train-rmse:16.923413    test-rmse:17.858287 \n#> [2]  train-rmse:12.250960    test-rmse:13.245702 \n#> [3]  train-rmse:9.008995 test-rmse:10.269762 \n#> [4]  train-rmse:6.790111 test-rmse:8.118332 \n#> [5]  train-rmse:5.263897 test-rmse:6.737373 \n#> [6]  train-rmse:4.226298 test-rmse:5.910818 \n#> [7]  train-rmse:3.513853 test-rmse:5.386048 \n#> [8]  train-rmse:3.060588 test-rmse:4.971588 \n#> [9]  train-rmse:2.704180 test-rmse:4.667367 \n#> [10] train-rmse:2.500957 test-rmse:4.520007 \n#> [11] train-rmse:2.360276 test-rmse:4.424577 \n#> [12] train-rmse:2.255008 test-rmse:4.268448 \n#> [13] train-rmse:2.150293 test-rmse:4.239951 \n#> [14] train-rmse:2.050543 test-rmse:4.181015 \n#> [15] train-rmse:2.003452 test-rmse:4.137171 \n#> [16] train-rmse:1.924608 test-rmse:4.095233 \n#> [17] train-rmse:1.861110 test-rmse:4.081775 \n#> [18] train-rmse:1.794015 test-rmse:4.070671 \n#> [19] train-rmse:1.750976 test-rmse:3.980266 \n#> [20] train-rmse:1.704884 test-rmse:3.943720 \n#> [21] train-rmse:1.633313 test-rmse:3.872674 \n#> [22] train-rmse:1.600509 test-rmse:3.859818 \n#> [23] train-rmse:1.575060 test-rmse:3.850316 \n#> [24] train-rmse:1.549222 test-rmse:3.848650 \n#> [25] train-rmse:1.512176 test-rmse:3.804888 \n#> [26] train-rmse:1.490101 test-rmse:3.806708 \n#> [27] train-rmse:1.437855 test-rmse:3.796689 \n#> [28] train-rmse:1.413658 test-rmse:3.800822 \n#> [29] train-rmse:1.391086 test-rmse:3.784102 \n#> [30] train-rmse:1.368214 test-rmse:3.751162 \n#> [31] train-rmse:1.341678 test-rmse:3.765602 \n#> [32] train-rmse:1.324352 test-rmse:3.766968 \n#> [33] train-rmse:1.267682 test-rmse:3.733680 \n#> [34] train-rmse:1.231068 test-rmse:3.735550 \n#> [35] train-rmse:1.211650 test-rmse:3.737077 \n#> [36] train-rmse:1.179801 test-rmse:3.718929 \n#> [37] train-rmse:1.145159 test-rmse:3.727204 \n#> [38] train-rmse:1.128239 test-rmse:3.711813 \n#> [39] train-rmse:1.103570 test-rmse:3.699716 \n#> [40] train-rmse:1.085133 test-rmse:3.701658 \n#> [41] train-rmse:1.073124 test-rmse:3.702788 \n#> [42] train-rmse:1.062533 test-rmse:3.699433 \n#> [43] train-rmse:1.045937 test-rmse:3.693495 \n#> [44] train-rmse:1.022968 test-rmse:3.688230 \n#> [45] train-rmse:1.012610 test-rmse:3.700308 \n#> [46] train-rmse:1.002660 test-rmse:3.698268 \n#> [47] train-rmse:0.995678 test-rmse:3.708165 \n#> [48] train-rmse:0.987307 test-rmse:3.690949 \n#> [49] train-rmse:0.976949 test-rmse:3.690472 \n#> [50] train-rmse:0.969292 test-rmse:3.701235 \n#> [51] train-rmse:0.951285 test-rmse:3.692314 \n#> [52] train-rmse:0.929873 test-rmse:3.662860 \n#> [53] train-rmse:0.918629 test-rmse:3.656849 \n#> [54] train-rmse:0.895356 test-rmse:3.640781 \n#> [55] train-rmse:0.881782 test-rmse:3.643711 \n#> [56] train-rmse:0.866274 test-rmse:3.652540 \n#> [57] train-rmse:0.856896 test-rmse:3.650975 \n#> [58] train-rmse:0.842923 test-rmse:3.640053 \n#> [59] train-rmse:0.834285 test-rmse:3.638969 \n#> [60] train-rmse:0.829124 test-rmse:3.643577 \n#> [61] train-rmse:0.818701 test-rmse:3.631377 \n#> [62] train-rmse:0.793543 test-rmse:3.628659 \n#> [63] train-rmse:0.780059 test-rmse:3.628344 \n#> [64] train-rmse:0.766899 test-rmse:3.633353 \n#> [65] train-rmse:0.756453 test-rmse:3.630871 \n#> [66] train-rmse:0.734865 test-rmse:3.627766 \n#> [67] train-rmse:0.730521 test-rmse:3.624072 \n#> [68] train-rmse:0.712303 test-rmse:3.613017 \n#> [69] train-rmse:0.706751 test-rmse:3.618362 \n#> [70] train-rmse:0.691661 test-rmse:3.615868 \n#> [1]  train-rmse:17.094472    test-rmse:17.443979 \n#> [2]  train-rmse:12.370325    test-rmse:12.648890 \n#> [3]  train-rmse:9.136447 test-rmse:9.424303 \n#> [4]  train-rmse:6.815337 test-rmse:7.274856 \n#> [5]  train-rmse:5.243661 test-rmse:5.833762 \n#> [6]  train-rmse:4.178753 test-rmse:4.908622 \n#> [7]  train-rmse:3.467034 test-rmse:4.275468 \n#> [8]  train-rmse:3.008385 test-rmse:3.939571 \n#> [9]  train-rmse:2.675041 test-rmse:3.773246 \n#> [10] train-rmse:2.474950 test-rmse:3.632611 \n#> [11] train-rmse:2.307720 test-rmse:3.533710 \n#> [12] train-rmse:2.177433 test-rmse:3.477157 \n#> [13] train-rmse:2.079583 test-rmse:3.439424 \n#> [14] train-rmse:1.998522 test-rmse:3.388455 \n#> [15] train-rmse:1.900939 test-rmse:3.346838 \n#> [16] train-rmse:1.821591 test-rmse:3.344501 \n#> [17] train-rmse:1.778242 test-rmse:3.331574 \n#> [18] train-rmse:1.716799 test-rmse:3.347789 \n#> [19] train-rmse:1.676291 test-rmse:3.338689 \n#> [20] train-rmse:1.632078 test-rmse:3.342672 \n#> [21] train-rmse:1.610130 test-rmse:3.323012 \n#> [22] train-rmse:1.591450 test-rmse:3.319949 \n#> [23] train-rmse:1.551755 test-rmse:3.299841 \n#> [24] train-rmse:1.536861 test-rmse:3.301777 \n#> [25] train-rmse:1.481012 test-rmse:3.276894 \n#> [26] train-rmse:1.454397 test-rmse:3.269283 \n#> [27] train-rmse:1.415888 test-rmse:3.254819 \n#> [28] train-rmse:1.384320 test-rmse:3.249840 \n#> [29] train-rmse:1.368003 test-rmse:3.243054 \n#> [30] train-rmse:1.344136 test-rmse:3.254499 \n#> [31] train-rmse:1.329390 test-rmse:3.257611 \n#> [32] train-rmse:1.301899 test-rmse:3.254902 \n#> [33] train-rmse:1.285813 test-rmse:3.253238 \n#> [34] train-rmse:1.275745 test-rmse:3.243495 \n#> [35] train-rmse:1.267246 test-rmse:3.239862 \n#> [36] train-rmse:1.244695 test-rmse:3.241509 \n#> [37] train-rmse:1.233288 test-rmse:3.246018 \n#> [38] train-rmse:1.223915 test-rmse:3.244156 \n#> [39] train-rmse:1.211425 test-rmse:3.244399 \n#> [40] train-rmse:1.180329 test-rmse:3.242740 \n#> [41] train-rmse:1.171058 test-rmse:3.241273 \n#> [42] train-rmse:1.161724 test-rmse:3.237195 \n#> [43] train-rmse:1.156259 test-rmse:3.238887 \n#> [44] train-rmse:1.134589 test-rmse:3.234876 \n#> [45] train-rmse:1.111284 test-rmse:3.224325 \n#> [46] train-rmse:1.084095 test-rmse:3.223480 \n#> [47] train-rmse:1.067216 test-rmse:3.226552 \n#> [48] train-rmse:1.059544 test-rmse:3.227267 \n#> [49] train-rmse:1.052601 test-rmse:3.232803 \n#> [50] train-rmse:1.029946 test-rmse:3.229616 \n#> [51] train-rmse:1.007692 test-rmse:3.233152 \n#> [52] train-rmse:0.989256 test-rmse:3.236416 \n#> [53] train-rmse:0.964192 test-rmse:3.234155 \n#> [54] train-rmse:0.947695 test-rmse:3.242226 \n#> [55] train-rmse:0.926670 test-rmse:3.237352 \n#> [56] train-rmse:0.905204 test-rmse:3.235260 \n#> [57] train-rmse:0.890484 test-rmse:3.230032 \n#> [58] train-rmse:0.881130 test-rmse:3.232472 \n#> [59] train-rmse:0.865016 test-rmse:3.228469 \n#> [60] train-rmse:0.842762 test-rmse:3.229937 \n#> [61] train-rmse:0.828306 test-rmse:3.236761 \n#> [62] train-rmse:0.810322 test-rmse:3.237922 \n#> [63] train-rmse:0.793606 test-rmse:3.239330 \n#> [64] train-rmse:0.784651 test-rmse:3.244624 \n#> [65] train-rmse:0.765355 test-rmse:3.251084 \n#> [66] train-rmse:0.754189 test-rmse:3.247930 \n#> [67] train-rmse:0.738733 test-rmse:3.249863 \n#> [68] train-rmse:0.729666 test-rmse:3.248329 \n#> [69] train-rmse:0.714781 test-rmse:3.248858 \n#> [70] train-rmse:0.705361 test-rmse:3.250069 \n#> [1]  train-rmse:17.170011    test-rmse:17.171257 \n#> [2]  train-rmse:12.426524    test-rmse:12.525679 \n#> [3]  train-rmse:9.117093 test-rmse:9.459849 \n#> [4]  train-rmse:6.793125 test-rmse:7.351778 \n#> [5]  train-rmse:5.236309 test-rmse:5.996796 \n#> [6]  train-rmse:4.151535 test-rmse:5.057847 \n#> [7]  train-rmse:3.447785 test-rmse:4.453109 \n#> [8]  train-rmse:2.964770 test-rmse:4.025576 \n#> [9]  train-rmse:2.648792 test-rmse:3.819552 \n#> [10] train-rmse:2.440856 test-rmse:3.740400 \n#> [11] train-rmse:2.284757 test-rmse:3.673686 \n#> [12] train-rmse:2.147940 test-rmse:3.634061 \n#> [13] train-rmse:2.056752 test-rmse:3.567147 \n#> [14] train-rmse:1.988819 test-rmse:3.519069 \n#> [15] train-rmse:1.910203 test-rmse:3.509014 \n#> [16] train-rmse:1.855831 test-rmse:3.488891 \n#> [17] train-rmse:1.825984 test-rmse:3.455567 \n#> [18] train-rmse:1.784249 test-rmse:3.450392 \n#> [19] train-rmse:1.742281 test-rmse:3.429652 \n#> [20] train-rmse:1.683531 test-rmse:3.424123 \n#> [21] train-rmse:1.644295 test-rmse:3.415885 \n#> [22] train-rmse:1.620427 test-rmse:3.397409 \n#> [23] train-rmse:1.591121 test-rmse:3.396034 \n#> [24] train-rmse:1.557370 test-rmse:3.359075 \n#> [25] train-rmse:1.517061 test-rmse:3.363934 \n#> [26] train-rmse:1.476960 test-rmse:3.358385 \n#> [27] train-rmse:1.459281 test-rmse:3.353978 \n#> [28] train-rmse:1.434989 test-rmse:3.373883 \n#> [29] train-rmse:1.417344 test-rmse:3.367324 \n#> [30] train-rmse:1.391160 test-rmse:3.361120 \n#> [31] train-rmse:1.371772 test-rmse:3.367616 \n#> [32] train-rmse:1.337820 test-rmse:3.368026 \n#> [33] train-rmse:1.319915 test-rmse:3.360235 \n#> [34] train-rmse:1.296343 test-rmse:3.350689 \n#> [35] train-rmse:1.263589 test-rmse:3.361618 \n#> [36] train-rmse:1.244863 test-rmse:3.366146 \n#> [37] train-rmse:1.194421 test-rmse:3.335086 \n#> [38] train-rmse:1.184094 test-rmse:3.334097 \n#> [39] train-rmse:1.156357 test-rmse:3.328963 \n#> [40] train-rmse:1.144884 test-rmse:3.326506 \n#> [41] train-rmse:1.117081 test-rmse:3.340203 \n#> [42] train-rmse:1.106197 test-rmse:3.341041 \n#> [43] train-rmse:1.100150 test-rmse:3.342239 \n#> [44] train-rmse:1.077481 test-rmse:3.347784 \n#> [45] train-rmse:1.046810 test-rmse:3.338964 \n#> [46] train-rmse:1.034026 test-rmse:3.351019 \n#> [47] train-rmse:1.023361 test-rmse:3.342007 \n#> [48] train-rmse:0.994702 test-rmse:3.341222 \n#> [49] train-rmse:0.989948 test-rmse:3.337105 \n#> [50] train-rmse:0.975026 test-rmse:3.336172 \n#> [51] train-rmse:0.952664 test-rmse:3.331605 \n#> [52] train-rmse:0.945468 test-rmse:3.334575 \n#> [53] train-rmse:0.922853 test-rmse:3.341329 \n#> [54] train-rmse:0.909634 test-rmse:3.337151 \n#> [55] train-rmse:0.903042 test-rmse:3.333116 \n#> [56] train-rmse:0.884904 test-rmse:3.326880 \n#> [57] train-rmse:0.874922 test-rmse:3.320552 \n#> [58] train-rmse:0.868196 test-rmse:3.319108 \n#> [59] train-rmse:0.848900 test-rmse:3.319444 \n#> [60] train-rmse:0.841934 test-rmse:3.326078 \n#> [61] train-rmse:0.838304 test-rmse:3.326217 \n#> [62] train-rmse:0.822154 test-rmse:3.318379 \n#> [63] train-rmse:0.807249 test-rmse:3.313129 \n#> [64] train-rmse:0.797583 test-rmse:3.309362 \n#> [65] train-rmse:0.782169 test-rmse:3.302684 \n#> [66] train-rmse:0.770329 test-rmse:3.298346 \n#> [67] train-rmse:0.765190 test-rmse:3.305168 \n#> [68] train-rmse:0.758894 test-rmse:3.307550 \n#> [69] train-rmse:0.745356 test-rmse:3.311324 \n#> [70] train-rmse:0.739772 test-rmse:3.313405 \n#> [1]  train-rmse:16.582907    test-rmse:18.207723 \n#> [2]  train-rmse:12.001622    test-rmse:13.722996 \n#> [3]  train-rmse:8.787949 test-rmse:10.474234 \n#> [4]  train-rmse:6.600602 test-rmse:8.351502 \n#> [5]  train-rmse:5.044425 test-rmse:6.781755 \n#> [6]  train-rmse:3.998929 test-rmse:5.842432 \n#> [7]  train-rmse:3.313856 test-rmse:5.326463 \n#> [8]  train-rmse:2.880562 test-rmse:5.021548 \n#> [9]  train-rmse:2.574312 test-rmse:4.744890 \n#> [10] train-rmse:2.372165 test-rmse:4.600522 \n#> [11] train-rmse:2.222040 test-rmse:4.535886 \n#> [12] train-rmse:2.115739 test-rmse:4.437634 \n#> [13] train-rmse:2.042316 test-rmse:4.430661 \n#> [14] train-rmse:1.938443 test-rmse:4.418263 \n#> [15] train-rmse:1.876714 test-rmse:4.404265 \n#> [16] train-rmse:1.823690 test-rmse:4.384746 \n#> [17] train-rmse:1.757006 test-rmse:4.377393 \n#> [18] train-rmse:1.708880 test-rmse:4.372659 \n#> [19] train-rmse:1.670362 test-rmse:4.356115 \n#> [20] train-rmse:1.624955 test-rmse:4.323082 \n#> [21] train-rmse:1.586701 test-rmse:4.314782 \n#> [22] train-rmse:1.563975 test-rmse:4.308469 \n#> [23] train-rmse:1.528573 test-rmse:4.300380 \n#> [24] train-rmse:1.513181 test-rmse:4.301769 \n#> [25] train-rmse:1.456300 test-rmse:4.268720 \n#> [26] train-rmse:1.399742 test-rmse:4.257959 \n#> [27] train-rmse:1.361863 test-rmse:4.220169 \n#> [28] train-rmse:1.345355 test-rmse:4.214260 \n#> [29] train-rmse:1.308177 test-rmse:4.189072 \n#> [30] train-rmse:1.283864 test-rmse:4.176219 \n#> [31] train-rmse:1.259614 test-rmse:4.195716 \n#> [32] train-rmse:1.222292 test-rmse:4.186206 \n#> [33] train-rmse:1.204266 test-rmse:4.187286 \n#> [34] train-rmse:1.173770 test-rmse:4.192971 \n#> [35] train-rmse:1.146007 test-rmse:4.190610 \n#> [36] train-rmse:1.135612 test-rmse:4.191723 \n#> [37] train-rmse:1.116079 test-rmse:4.191155 \n#> [38] train-rmse:1.101917 test-rmse:4.190144 \n#> [39] train-rmse:1.066302 test-rmse:4.181161 \n#> [40] train-rmse:1.045433 test-rmse:4.181066 \n#> [41] train-rmse:1.021536 test-rmse:4.182448 \n#> [42] train-rmse:1.012055 test-rmse:4.172824 \n#> [43] train-rmse:0.998246 test-rmse:4.164631 \n#> [44] train-rmse:0.981645 test-rmse:4.157344 \n#> [45] train-rmse:0.969975 test-rmse:4.158212 \n#> [46] train-rmse:0.962992 test-rmse:4.159398 \n#> [47] train-rmse:0.938172 test-rmse:4.162487 \n#> [48] train-rmse:0.909808 test-rmse:4.149913 \n#> [49] train-rmse:0.888373 test-rmse:4.146672 \n#> [50] train-rmse:0.883702 test-rmse:4.145806 \n#> [51] train-rmse:0.878323 test-rmse:4.147840 \n#> [52] train-rmse:0.865686 test-rmse:4.147482 \n#> [53] train-rmse:0.856467 test-rmse:4.141376 \n#> [54] train-rmse:0.851313 test-rmse:4.140475 \n#> [55] train-rmse:0.824926 test-rmse:4.125626 \n#> [56] train-rmse:0.807213 test-rmse:4.130817 \n#> [57] train-rmse:0.795934 test-rmse:4.133379 \n#> [58] train-rmse:0.782519 test-rmse:4.129859 \n#> [59] train-rmse:0.767906 test-rmse:4.130241 \n#> [60] train-rmse:0.756362 test-rmse:4.126689 \n#> [61] train-rmse:0.750580 test-rmse:4.125184 \n#> [62] train-rmse:0.739081 test-rmse:4.125837 \n#> [63] train-rmse:0.734751 test-rmse:4.124346 \n#> [64] train-rmse:0.727189 test-rmse:4.116006 \n#> [65] train-rmse:0.719912 test-rmse:4.119719 \n#> [66] train-rmse:0.711155 test-rmse:4.123465 \n#> [67] train-rmse:0.692546 test-rmse:4.119317 \n#> [68] train-rmse:0.681082 test-rmse:4.113232 \n#> [69] train-rmse:0.677283 test-rmse:4.110573 \n#> [70] train-rmse:0.661992 test-rmse:4.113574\n#> [1] 3.449206\nwarnings() # no warnings for individual XGBoost function"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"building-weighted-ensembles-to-model-numerical-data","chapter":"4 Building weighted ensembles to model numerical data","heading":"4 Building weighted ensembles to model numerical data","text":"last chapter learned make 23 individual models,\nincluding calculating error rate (root mean squared error), \npredictions holdout data (test validation).chapter show use results make weighted\nensembles can used model numerical data.Let’s start end. Let’s imagine finished product. list \nensembles individual models, error rates sorted decreasing\norder (best result top list)Therefore ’re going need weighted ensemble. weight \nuse?turns excellent answer available virtually \nwork part (great!). model mean error score.\nweight use reciprocal error.Let’s say two models. One error rate 5.0 \nerror rate 2.0. Clearly model error rate 2.0\nsuperior model error rate 5.0.building ensemble multiply values \nensemble 1/(error rate). give higher weights models \nhigher accuracy.Let’s see works extremely simple ensemble.section making weighted ensemble\nusing 17 models numerical data, using ensemble measure\naccuracy models holdout (test) data.","code":"\nlibrary(tree) # Allows us to use tree models\nlibrary(MASS) # For the Boston Housing data set library(Metrics)\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ✖ dplyr::select() masks MASS::select()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Set initial values to 0\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_RMSE <- 0\nlinear_test_predict_value <- 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_RMSE <- 0\ntree_holdout_RMSE <- 0\ntree_test_predict_value <- 0\n\nensemble_linear_RMSE <- 0\nensemble_linear_RMSE_mean <- 0\nensemble_tree_RMSE <- 0\nensemble_tree_RMSE_mean <- 0\n\nnumerical_1 <- function(data, colnum, train_amount, test_amount, numresamples){\n\n# Move target column to far right\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Set up resampling\nfor (i in 1:numresamples) {\n  idx <- sample(seq(1, 2), size = nrow(data), replace = TRUE, prob = c(train_amount, test_amount))\n  train <- data[idx == 1, ]\n  test <- data[idx == 2, ]\n\n# Fit linear model on the training data, make predictions on the test data\nlinear_model <- lm(y ~ ., data = train)\nlinear_predictions <- predict(object = linear_model, newdata = test)\nlinear_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = linear_predictions)\nlinear_RMSE_mean <- mean(linear_RMSE)\n\n# Fit tree model on the training data, make predictions on the test data\ntree_model <- tree(y ~ ., data = train)\ntree_predictions <- predict(object = tree_model, newdata = test)\ntree_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = tree_predictions)\ntree_RMSE_mean <- mean(tree_RMSE)\n\n# Make the weighted ensemble\nensemble <- data.frame(\n  'linear' = linear_predictions / linear_RMSE_mean,\n  'tree' = tree_predictions / tree_RMSE_mean,\n  'y_ensemble' = test$y)\n\n# Split ensemble between train and test\nensemble_idx <- sample(seq(1, 2), size = nrow(ensemble), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble[ensemble_idx == 1, ]\nensemble_test <- ensemble[ensemble_idx == 2, ]\n\n# Fit the ensemble data on the ensemble training data, predict on ensemble test data\nensemble_linear_model <- lm(y_ensemble ~ ., data = ensemble_train)\n\nensemble_linear_predictions <- predict(object = ensemble_linear_model, newdata = ensemble_test)\n\nensemble_linear_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_linear_predictions)\n\nensemble_linear_RMSE_mean <- mean(ensemble_linear_RMSE)\n\n# Fit the tree model on the ensemble training data, predict on ensemble test data\nensemble_tree_model <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree_model, newdata = ensemble_test) \n\nensemble_tree_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_tree_predictions)\n\nensemble_tree_RMSE_mean <- mean(ensemble_tree_RMSE)\n\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble_Linear', 'Ensemble_tree'),\n  'Error_Rate' = c(linear_RMSE_mean, tree_RMSE_mean, ensemble_linear_RMSE_mean, ensemble_tree_RMSE_mean)\n)\n\nresults <- results %>% arrange(Error_Rate)\n\n} # Closing brace for numresamples\nreturn(list(results))\n\n} # Closing brace for the function\n\nnumerical_1(data = MASS::Boston, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25)\n#> [[1]]\n#>             Model Error_Rate\n#> 1 Ensemble_Linear   4.127265\n#> 2            Tree   4.750828\n#> 3   Ensemble_tree   4.812045\n#> 4          Linear   4.865660\n\nwarnings()"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"think-before-you-do-something.-this-will-help-when-we-start-at-the-end-and-work-backwards-toward-the-beginning.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.1 Think before you do something. This will help when we start at the end and work backwards toward the beginning.","text":"going make ensemble. ensemble going made \npredictions numerical models. ’ve already seen couple \nensembles. one extremely similar, involve \nmodels.Starting end, want error rate (root mean squared error)\nensemble prediction models.means ’ll need make ensemble. means ’ll need\nindividual model predictions, ’s ensemble made. \nensemble made individual model predictions, means ’ll\nneed individual models. already know , \nlast chapter.’re going build simple ensemble seven models, use\nensemble four different methods. Ensembles package\nactually works total 40 different models. process \nexactly , whether working seven individual models \n23 individual models, five ensemble models 17 ensemble models. \nstructre methods .let’s get started!seven individual models building :Linear (tuned)Linear (tuned)BayesglmBayesglmBayesrnnBayesrnnGradient BoostedGradient BoostedRandomForestRandomForestTreesTreesIt’s important understand many options possible. \nencouraged add least one modeling method ensemble,\nsee impacts results.","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"one-of-your-own-add-one-model-to-the-list-of-seven-individual-models-see-how-it-impacts-results.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.2 One of your own: Add one model to the list of seven individual models, see how it impacts results.","text":"","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"plan-ahead-as-much-as-you-can-that-makes-the-entire-model-building-process-much-easier.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.3 Plan ahead as much as you can, that makes the entire model building process much easier.","text":"code build ensembles. strongly recommend \n, checking every 5-10 lines make sure \nerrors.","code":"\n\n#Load packages we will need\n\nlibrary(arm) # Allows us to run bayesglm\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\nlibrary(brnn) # Allows us to run brnn\n#> Loading required package: Formula\n#> Loading required package: truncnorm\nlibrary(e1071) # Allows us to run several tuned model, such as linear and KNN\nlibrary(randomForest) # Allows us to run random forest models\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\nlibrary(tree) # Allows us to run tree models"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"a-few-other-packages-we-will-need-to-keep-everything-running-smoothly","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.3.1 A few other packages we will need to keep everything running smoothly","text":"","code":"\nlibrary(tidyverse) # Amazing set of tools for data science\nlibrary(MASS) # Gives us the Boston Housing data set\nlibrary(Metrics) # Allows us to calculate accuracy or error rates"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"build-the-function-that-will-build-the-individual-and-ensemble-models","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.3.2 Build the function that will build the individual and ensemble models","text":"’s cool part setting way. \ntotally different data set, need put information\nfunction, everything runs. Check :","code":"\n\nnumerical <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Make the target column the right most column, change the column name to y:\n\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right df <- df[sample(nrow(df)), ]\n\n# Set initial values to 0 for both individual and ensemble methods:\n\nbayesglm_train_RMSE <- 0\nbayesglm_test_RMSE <- 0\nbayesglm_validation_RMSE <- 0\nbayesglm_sd <- 0\nbayesglm_overfitting <- 0\nbayesglm_duration <- 0\nbayesglm_duration_mean <- 0\nbayesglm_holdout_mean <- 0\nbayesglm_holdout_RMSE <- 0\nbayesglm_holdout_RMSE_mean <- 0\n\nbayesrnn_train_RMSE <- 0\nbayesrnn_test_RMSE <- 0\nbayesrnn_validation_RMSE <- 0\nbayesrnn_sd <- 0\nbayesrnn_overfitting <- 0\nbayesrnn_duration <- 0\nbayesrnn_duration_mean <- 0\nbayesrnn_holdout_mean <- 0\nbayesrnn_holdout_RMSE <- 0\nbayesrnn_holdout_RMSE_mean <- 0\n\ngb_train_RMSE <- 0\ngb_test_RMSE <- 0\ngb_validation_RMSE <- 0\ngb_sd <- 0\ngb_overfitting <- 0\ngb_duration <- 0\ngb_duration_mean <- 0\ngb_holdout_mean <- 0\ngb_holdout_RMSE <- 0\ngb_holdout_RMSE_mean <- 0\n\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_validation_RMSE <- 0\nlinear_sd <- 0\nlinear_overfitting <- 0\nlinear_duration <- 0\nlinear_holdout_RMSE <- 0\nlinear_holdout_RMSE_mean <- 0\n\nrf_train_RMSE <- 0\nrf_test_RMSE <- 0\nrf_validation_RMSE <- 0\nrf_sd <- 0\nrf_overfitting <- 0\nrf_duration <- 0\nrf_duration_mean <- 0\nrf_holdout_mean <- 0\nrf_holdout_RMSE <- 0\nrf_holdout_RMSE_mean <- 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_validation_RMSE <- 0\ntree_sd <- 0\ntree_overfitting <- 0\ntree_duration <- 0\ntree_duration_mean <- 0\ntree_holdout_mean <- 0\ntree_holdout_RMSE <- 0\ntree_holdout_RMSE_mean <- 0\n\nensemble_bayesglm_train_RMSE <- 0\nensemble_bayesglm_test_RMSE <- 0\nensemble_bayesglm_validation_RMSE <- 0\nensemble_bayesglm_sd <- 0\nensemble_bayesglm_overfitting <- 0\nensemble_bayesglm_duration <- 0\nensemble_bayesglm_holdout_RMSE <- 0\nensemble_bayesglm_holdout_RMSE_mean <- 0\nensemble_bayesglm_predict_value_mean <- 0\n\nensemble_bayesrnn_train_RMSE <- 0\nensemble_bayesrnn_test_RMSE <- 0\nensemble_bayesrnn_validation_RMSE <- 0\nensemble_bayesrnn_sd <- 0\nensemble_bayesrnn_overfitting <- 0\nensemble_bayesrnn_duration <- 0\nensemble_bayesrnn_holdout_RMSE <- 0\nensemble_bayesrnn_holdout_RMSE_mean <- 0\nensemble_bayesrnn_predict_value_mean <- 0\n\nensemble_gb_train_RMSE <- 0\nensemble_gb_test_RMSE <- 0\nensemble_gb_validation_RMSE <- 0\nensemble_gb_sd <- 0\nensemble_gb_overfitting <- 0\nensemble_gb_duration <- 0\nensemble_gb_holdout_RMSE <- 0\nensemble_gb_holdout_RMSE_mean <- 0\nensemble_gb_predict_value_mean <- 0\n\nensemble_linear_train_RMSE <- 0\nensemble_linear_test_RMSE <- 0\nensemble_linear_validation_RMSE <- 0\nensemble_linear_sd <- 0\nensemble_linear_overfitting <- 0\nensemble_linear_duration <- 0\nensemble_linear_holdout_RMSE <- 0\nensemble_linear_holdout_RMSE_mean <- 0\n\nensemble_rf_train_RMSE <- 0\nensemble_rf_test_RMSE <- 0\nensemble_rf_test_RMSE_mean <- 0\nensemble_rf_validation_RMSE <- 0\nensemble_rf_sd <- 0\nensemble_rf_overfitting <- 0\nensemble_rf_duration <- 0\nensemble_rf_holdout_RMSE <- 0\nensemble_rf_holdout_RMSE_mean <- 0\n\nensemble_tree_train_RMSE <- 0\nensemble_tree_test_RMSE <- 0\nensemble_tree_validation_RMSE <- 0\nensemble_tree_sd <- 0\nensemble_tree_overfitting <- 0\nensemble_tree_duration <- 0\nensemble_tree_holdout_RMSE <- 0\nensemble_tree_holdout_RMSE_mean <- 0\n\n#Let's build the function that does all the resampling and puts everything together:\n\nfor (i in 1:numresamples) {\n\n# Randomly split the data between train and test\nidx <- sample(seq(1, 2), size = nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\ntrain <- df[idx == 1, ]\ntest <- df[idx == 2, ]\n\n# Bayesglm\n\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = gaussian(link = \"identity\"))\nbayesglm_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesglm_train_fit, newdata = train))\nbayesglm_train_RMSE_mean <- mean(bayesglm_train_RMSE)\nbayesglm_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesglm_train_fit, newdata = test))\nbayesglm_test_RMSE_mean <- mean(bayesglm_test_RMSE)\nbayesglm_holdout_RMSE[i] <- mean(bayesglm_test_RMSE_mean)\nbayesglm_holdout_RMSE_mean <- mean(bayesglm_holdout_RMSE)\nbayesglm_test_predict_value <- as.numeric(predict(object = bayesglm_train_fit, newdata = test))\ny_hat_bayesglm <- c(bayesglm_test_predict_value)\n\n# Bayesrnn\n\nbayesrnn_train_fit <- brnn::brnn(x = as.matrix(train), y = train$y)\nbayesrnn_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = bayesrnn_train_fit, newdata = train))\nbayesrnn_train_RMSE_mean <- mean(bayesrnn_train_RMSE)\nbayesrnn_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = bayesrnn_train_fit, newdata = test))\nbayesrnn_test_RMSE_mean <- mean(bayesrnn_test_RMSE)\nbayesrnn_holdout_RMSE[i] <- mean(c(bayesrnn_test_RMSE_mean))\nbayesrnn_holdout_RMSE_mean <- mean(bayesrnn_holdout_RMSE)\nbayesrnn_train_predict_value <- as.numeric(predict(object = bayesrnn_train_fit, newdata = train))\nbayesrnn_test_predict_value <- as.numeric(predict(object = bayesrnn_train_fit, newdata = test))\nbayesrnn_predict_value_mean <- mean(c(bayesrnn_test_predict_value))\ny_hat_bayesrnn <- c(bayesrnn_test_predict_value)\n\n# Gradient boosted\n\ngb_train_fit <- gbm::gbm(train$y ~ ., data = train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\ngb_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = gb_train_fit, newdata = train))\ngb_train_RMSE_mean <- mean(gb_train_RMSE)\ngb_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = gb_train_fit, newdata = test))\ngb_test_RMSE_mean <- mean(gb_test_RMSE)\ngb_holdout_RMSE[i] <- mean(c(gb_test_RMSE_mean))\ngb_holdout_RMSE_mean <- mean(gb_holdout_RMSE)\ngb_train_predict_value <- as.numeric(predict(object = gb_train_fit, newdata = train))\ngb_test_predict_value <- as.numeric(predict(object = gb_train_fit, newdata = test)) \ngb_predict_value_mean <- mean(c(gb_test_predict_value))\ny_hat_gb <- c(gb_test_predict_value)\n\n# Tuned linear models\n\nlinear_train_fit <- e1071::tune.rpart(formula = y ~ ., data = train)\nlinear_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = linear_train_fit$best.model, newdata = train))\nlinear_train_RMSE_mean <- mean(linear_train_RMSE)\nlinear_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = linear_train_fit$best.model, newdata = test))\nlinear_test_RMSE_mean <- mean(linear_test_RMSE)\nlinear_holdout_RMSE[i] <- mean(c(linear_test_RMSE_mean))\nlinear_holdout_RMSE_mean <- mean(linear_holdout_RMSE)\nlinear_train_predict_value <- as.numeric(predict(object = linear_train_fit$best.model, newdata = train))\nlinear_test_predict_value <- as.numeric(predict(object = linear_train_fit$best.model, newdata = test))\ny_hat_linear <- c(linear_test_predict_value)\n\n# RandomForest\n\nrf_train_fit <- e1071::tune.randomForest(x = train, y = train$y, data = train)\nrf_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = rf_train_fit$best.model, newdata = train))\nrf_train_RMSE_mean <- mean(rf_train_RMSE)\nrf_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = rf_train_fit$best.model, newdata = test))\nrf_test_RMSE_mean <- mean(rf_test_RMSE)\nrf_holdout_RMSE[i] <- mean(c(rf_test_RMSE_mean))\nrf_holdout_RMSE_mean <- mean(rf_holdout_RMSE)\nrf_train_predict_value <- predict(object = rf_train_fit$best.model, newdata = train)\nrf_test_predict_value <- predict(object = rf_train_fit$best.model, newdata = test)\ny_hat_rf <- c(rf_test_predict_value)\n\n# Trees\n\ntree_train_fit <- tree::tree(train$y ~ ., data = train)\ntree_train_RMSE[i] <- Metrics::rmse(actual = train$y, predicted = predict(object = tree_train_fit, newdata = train))\ntree_train_RMSE_mean <- mean(tree_train_RMSE)\ntree_test_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = predict(object = tree_train_fit, newdata = test))\ntree_test_RMSE_mean <- mean(tree_test_RMSE)\ntree_holdout_RMSE[i] <- mean(c(tree_test_RMSE_mean))\ntree_holdout_RMSE_mean <- mean(tree_holdout_RMSE)\ntree_train_predict_value <- as.numeric(predict(object = tree::tree(y ~ ., data = train), newdata = train))\ntree_test_predict_value <- as.numeric(predict(object = tree::tree(y ~ ., data = train), newdata = test))\ny_hat_tree <- c(tree_test_predict_value)\n\n# Make the weighted ensemble:\n\nensemble <- data.frame(\n  \"BayesGLM\" = y_hat_bayesglm * 1 / bayesglm_holdout_RMSE_mean,\n  \"BayesRNN\" = y_hat_bayesrnn * 1 / bayesrnn_holdout_RMSE_mean,\n  \"GBM\" = y_hat_gb * 1 / gb_holdout_RMSE_mean,\n  \"Linear\" = y_hat_linear * 1 / linear_holdout_RMSE_mean,\n  \"RandomForest\" = y_hat_rf * 1 / rf_holdout_RMSE_mean,\n  \"Tree\" = y_hat_tree * 1 / tree_holdout_RMSE_mean\n  )\n\nensemble$Row_mean <- rowMeans(ensemble)\n  ensemble$y_ensemble <- c(test$y)\n  y_ensemble <- c(test$y)\n\n# Split the ensemble into train and test, according to user choices:\n\nensemble_idx <- sample(seq(1, 2), size = nrow(ensemble), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble[ensemble_idx == 1, ]\nensemble_test <- ensemble[ensemble_idx == 2, ]\n\n# Ensemble BayesGLM\n\nensemble_bayesglm_train_fit <- arm::bayesglm(y_ensemble ~ ., data = ensemble_train, family = gaussian(link = \"identity\"))\nensemble_bayesglm_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_bayesglm_train_fit, newdata = ensemble_train))\nensemble_bayesglm_train_RMSE_mean <- mean(ensemble_bayesglm_train_RMSE)\nensemble_bayesglm_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_bayesglm_train_fit, newdata = ensemble_test))\nensemble_bayesglm_test_RMSE_mean <- mean(ensemble_bayesglm_test_RMSE)\nensemble_bayesglm_holdout_RMSE[i] <- mean(c(ensemble_bayesglm_test_RMSE_mean))\nensemble_bayesglm_holdout_RMSE_mean <- mean(ensemble_bayesglm_holdout_RMSE)\n\n# Ensemble BayesRNN\n\nensemble_bayesrnn_train_fit <- brnn::brnn(x = as.matrix(ensemble_train), y = ensemble_train$y_ensemble)\nensemble_bayesrnn_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_bayesrnn_train_fit, newdata = ensemble_train))\nensemble_bayesrnn_train_RMSE_mean <- mean(ensemble_bayesrnn_train_RMSE)\nensemble_bayesrnn_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_bayesrnn_train_fit, newdata = ensemble_test))\nensemble_bayesrnn_test_RMSE_mean <- mean(ensemble_bayesrnn_test_RMSE)\nensemble_bayesrnn_holdout_RMSE[i] <- mean(c(ensemble_bayesrnn_test_RMSE_mean))\nensemble_bayesrnn_holdout_RMSE_mean <- mean(ensemble_bayesrnn_holdout_RMSE)\n\n# Ensemble Graident Boosted\n\nensemble_gb_train_fit <- gbm::gbm(ensemble_train$y_ensemble ~ ., data = ensemble_train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\nensemble_gb_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_gb_train_fit, newdata = ensemble_train))\nensemble_gb_train_RMSE_mean <- mean(ensemble_gb_train_RMSE)\nensemble_gb_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_gb_train_fit, newdata = ensemble_test))\nensemble_gb_test_RMSE_mean <- mean(ensemble_gb_test_RMSE)\nensemble_gb_holdout_RMSE[i] <- mean(c(ensemble_gb_test_RMSE_mean))\nensemble_gb_holdout_RMSE_mean <- mean(ensemble_gb_holdout_RMSE)\n\n# Ensemble using Tuned Random Forest\n\nensemble_rf_train_fit <- e1071::tune.randomForest(x = ensemble_train, y = ensemble_train$y_ensemble, data = ensemble_train)\nensemble_rf_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_rf_train_fit$best.model, newdata = ensemble_train))\nensemble_rf_train_RMSE_mean <- mean(ensemble_rf_train_RMSE)\nensemble_rf_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_rf_train_fit$best.model, newdata = ensemble_test))\nensemble_rf_test_RMSE_mean <- mean(ensemble_rf_test_RMSE)\nensemble_rf_holdout_RMSE[i] <- mean(c(ensemble_rf_test_RMSE_mean))\nensemble_rf_holdout_RMSE_mean <- mean(ensemble_rf_holdout_RMSE)\n\n# Trees\n\nensemble_tree_train_fit <- tree::tree(ensemble_train$y_ensemble ~ ., data = ensemble_train)\nensemble_tree_train_RMSE[i] <- Metrics::rmse(actual = ensemble_train$y_ensemble, predicted = predict(object = ensemble_tree_train_fit, newdata = ensemble_train))\nensemble_tree_train_RMSE_mean <- mean(ensemble_tree_train_RMSE)\nensemble_tree_test_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y_ensemble, predicted = predict(object = ensemble_tree_train_fit, newdata = ensemble_test))\nensemble_tree_test_RMSE_mean <- mean(ensemble_tree_test_RMSE)\nensemble_tree_holdout_RMSE[i] <- mean(c(ensemble_tree_test_RMSE_mean))\nensemble_tree_holdout_RMSE_mean <- mean(ensemble_tree_holdout_RMSE)\n\nsummary_results <- data.frame(\n  'Model' = c('BayesGLM', 'BayesRNN', 'Gradient_Boosted', 'Linear', 'Random_Forest', 'Trees', 'Ensemble_BayesGLM', 'Ensemble_BayesRNN', 'Ensemble_Gradient_Boosted', 'Ensemble_Random_Forest', 'Ensemble_Trees'), \n  'Error' = c(bayesglm_holdout_RMSE_mean, bayesrnn_holdout_RMSE_mean, gb_holdout_RMSE_mean, linear_holdout_RMSE_mean, rf_holdout_RMSE_mean, tree_holdout_RMSE_mean, ensemble_bayesglm_holdout_RMSE_mean, ensemble_bayesrnn_holdout_RMSE_mean, ensemble_gb_holdout_RMSE_mean, ensemble_rf_holdout_RMSE_mean, ensemble_tree_holdout_RMSE_mean) )\n\nsummary_results <- summary_results %>% arrange(Error)\n\n} # closing brace for numresamples\nreturn(summary_results)\n\n} # closing brace for numerical function\n\nnumerical(data = MASS::Boston, colnum = 14, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015569 \n#> gamma= 31.1545    alpha= 5.1034   beta= 14788.74\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7042319 \n#> gamma= 14.5179    alpha= 2.1332   beta= 7029.778\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015619 \n#> gamma= 31.0596    alpha= 4.216    beta= 30890.58\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7036303 \n#> gamma= 13.6581    alpha= 2.1458   beta= 6053.645\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7015874 \n#> gamma= 31.4948    alpha= 5.4893   beta= 14943\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7041593 \n#> gamma= 13.4907    alpha= 1.8086   beta= 6770.96\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7016085 \n#> gamma= 31.1358    alpha= 5.6648   beta= 16396.45\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7037423 \n#> gamma= 13.7725    alpha= 1.7388   beta= 11998.36\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 32 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7017105 \n#> gamma= 30.4814    alpha= 4.7672   beta= 16756.22\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7036577 \n#> gamma= 12.6686    alpha= 1.8284   beta= 8985.648\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#>                        Model     Error\n#> 1          Ensemble_BayesGLM 0.1379912\n#> 2                   BayesRNN 0.1507615\n#> 3          Ensemble_BayesRNN 0.1855423\n#> 4     Ensemble_Random_Forest 0.9268246\n#> 5              Random_Forest 1.7842346\n#> 6             Ensemble_Trees 2.0855036\n#> 7  Ensemble_Gradient_Boosted 2.2402316\n#> 8           Gradient_Boosted 3.5698647\n#> 9                      Trees 4.8366919\n#> 10                  BayesGLM 4.9877972\n#> 11                    Linear 5.1775478\n\nwarnings()\n\nnumerical(data = ISLR::Auto[, 1:ncol(ISLR::Auto)-1], colnum = 1, numresamples = 25, train_amount = 0.50, test_amount = 0.50)\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7026564 \n#> gamma= 17.0741    alpha= 2.6627   beta= 8713.611\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7048205 \n#> gamma= 11.5889    alpha= 2.0474   beta= 4629.483\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025856 \n#> gamma= 18.5517    alpha= 2.648    beta= 28556.25\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7050725 \n#> gamma= 13.0326    alpha= 2.8908   beta= 4353.741\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023942 \n#> gamma= 19.3003    alpha= 3.4712   beta= 10171.52\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7048689 \n#> gamma= 12.1648    alpha= 2.5587   beta= 4977.586\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023593 \n#> gamma= 19.5728    alpha= 3.8344   beta= 9479.29\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7055993 \n#> gamma= 13.403     alpha= 2.3741   beta= 4139.822\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023708 \n#> gamma= 19.2389    alpha= 3.4001   beta= 11594.67\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7058001 \n#> gamma= 14.2831    alpha= 1.8106   beta= 6708.337\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024673 \n#> gamma= 18.8419    alpha= 2.8555   beta= 11166.83\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7052367 \n#> gamma= 13.9749    alpha= 2.3209   beta= 4007.501\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7026276 \n#> gamma= 18.6506    alpha= 3.2347   beta= 9230.53\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7045071 \n#> gamma= 13.6663    alpha= 2.3144   beta= 5343.38\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023254 \n#> gamma= 19.1902    alpha= 3.5465   beta= 9493.475\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7068673 \n#> gamma= 11.9701    alpha= 2.4945   beta= 3145.46\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7026135 \n#> gamma= 18.831     alpha= 3.0418   beta= 11733.43\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7050725 \n#> gamma= 13.0237    alpha= 2.2562   beta= 5798.846\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024061 \n#> gamma= 18.7375    alpha= 3.5705   beta= 10077.21\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7050725 \n#> gamma= 12.8282    alpha= 2.2718   beta= 4613.55\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7026135 \n#> gamma= 19.3209    alpha= 3.5508   beta= 8750.865\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7049686 \n#> gamma= 12.9429    alpha= 2.1176   beta= 5934.885\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024425 \n#> gamma= 18.7662    alpha= 2.9538   beta= 22385.57\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7051261 \n#> gamma= 12.0317    alpha= 2.2065   beta= 5723.925\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024061 \n#> gamma= 18.5628    alpha= 1.7018   beta= 84334.34\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7049686 \n#> gamma= 12.1488    alpha= 2.1287   beta= 4850.201\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025449 \n#> gamma= 19.4594    alpha= 3.7409   beta= 9004.504\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7045493 \n#> gamma= 12.953     alpha= 2.0884   beta= 5940.986\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024181 \n#> gamma= 19.0158    alpha= 3.2166   beta= 13737.31\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7051261 \n#> gamma= 15.683     alpha= 2.2925   beta= 9456.172\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023593 \n#> gamma= 18.689     alpha= 2.4452   beta= 9383.395\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.705473 \n#> gamma= 12.9215    alpha= 1.0122   beta= 4870.5\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025055 \n#> gamma= 19.0227    alpha= 3.4188   beta= 9603.117\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7052367 \n#> gamma= 13.1365    alpha= 2.3345   beta= 4404.568\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7024799 \n#> gamma= 18.5332    alpha= 3.4972   beta= 11495.23\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7055993 \n#> gamma= 12.307     alpha= 1.39     beta= 78137.77\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025055 \n#> gamma= 18.427     alpha= 2.1005   beta= 32867.57\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7052367 \n#> gamma= 12.2244    alpha= 2.6623   beta= 4237.805\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7026564 \n#> gamma= 18.691     alpha= 2.8504   beta= 17454.78\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.704124 \n#> gamma= 13.4225    alpha= 3.0357   beta= 5413.544\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025584 \n#> gamma= 18.9272    alpha= 3.1235   beta= 9202.181\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7051261 \n#> gamma= 12.8962    alpha= 2.4382   beta= 4970.101\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7023708 \n#> gamma= 18.9841    alpha= 3.0874   beta= 9927.468\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7051261 \n#> gamma= 12.1086    alpha= 2.2211   beta= 5083.125\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7027941 \n#> gamma= 19.3697    alpha= 3.8956   beta= 7783.408\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.704124 \n#> gamma= 13.9015    alpha= 2.4818   beta= 5403.593\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7025449 \n#> gamma= 13.3282    alpha= 0.9683   beta= 9062.74\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7043456 \n#> gamma= 14.4531    alpha= 2.326    beta= 8345.213\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7021989 \n#> gamma= 18.0495    alpha= 3.4913   beta= 10339.51\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#> Number of parameters (weights and biases) to estimate: 20 \n#> Nguyen-Widrow method\n#> Scaling factor= 0.7057316 \n#> gamma= 12.4934    alpha= 2.3563   beta= 4167.19\n#> Using 100 trees...\n#> \n#> Using 100 trees...\n#>                        Model     Error\n#> 1          Ensemble_BayesGLM 0.1270331\n#> 2                   BayesRNN 0.1411679\n#> 3          Ensemble_BayesRNN 0.1906438\n#> 4     Ensemble_Random_Forest 0.9333546\n#> 5              Random_Forest 1.6440270\n#> 6  Ensemble_Gradient_Boosted 1.7233343\n#> 7             Ensemble_Trees 1.7852984\n#> 8           Gradient_Boosted 2.9255353\n#> 9                   BayesGLM 3.4321977\n#> 10                    Linear 3.6335479\n#> 11                     Trees 3.6335709"},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"one-of-your-own-add-a-model-to-the-individual-models-and-a-model-to-the-ensemble-of-models","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.4 One of your own: Add a model to the individual models, and a model to the ensemble of models","text":"One : Change data, run , comment results","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"post-your-results-on-social-media-in-a-way-that-a-non-technical-person-can-understand-them.-for-example","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.5 Post your results on social media in a way that a non-technical person can understand them. For example:","text":"“Just ran six individual six ensemble models, easy , \nerrors warnings. plan ensembles data sets soon.\n#AIEnsembles","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"exercises-to-help-you-improve-your-skills","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.6 Exercises to help you improve your skills:","text":"Build individual numerical model using following model\nmethods (’s perfectly OK check prior sections book, \nexample delayed repetition):Gradient Boosted (gmb library)Rpart (rpart library)Support Vector Machines (tuned e1071 library)One model method choosingBuild ensemble using four methods, test using Boston\nHousing data set. Compare results ensemble one made\ntext chapter.Apply function made different numerical data set. can\ndone one line code, ensemble set .","code":""},{"path":"building-weighted-ensembles-to-model-numerical-data.html","id":"post-the-results-of-your-new-ensemble-on-social-media-in-a-way-that-helps-others-understand-the-results-or-methods.","chapter":"4 Building weighted ensembles to model numerical data","heading":"4.7 Post the results of your new ensemble on social media in a way that helps others understand the results or methods.","text":"","code":""},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"classification-data-how-to-make-14-individual-classification-models","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5 Classification data: How to make 14 individual classification models","text":"Ensembles numerical data give results often superior individual model. ’ve seen results ensembles numerical data beat nearly individual models, measures lowest error rate.Now going classification data. build 15 individual models classification data chapter.basic series steps numerical data. follow steps, complete process models classification data.Load librarySet initial values 0Create functionBreak data train test setsSet random resamplingFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setAll models structured way close identical possible. high level consistency makes easier spot errors.","code":""},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"what-is-classification-data","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.1 What is classification data?","text":"Classification models set models identify class specific observation. example, Carseats data set, Shelve Location example data type:look ShelveLoc column, set numbers, one three locations shelf: Bad, Medium Good. Classification models statistical models predict class data.Classification models similar numerical models. follow basic steps numerical models, use classification models instead.One big difference accuracy measured classification data. numerical models used root mean squared error. measure classification simply accuracy result. can measured directly matrix values. example:C50 test results:accuracy determined calculating number correct responses divided total number responses. correct responses along main diagonal.example, model correct predicted 83 bad responses, however also predicted response bad actually Good, also predicted Bad actually Medium. correct responses main diagonal, responses errors. use calculation accuracy model results.case, (83 + 75 + 272) / (83 + 1 + 105 + 4 + 75 + 56 + 106 + 90 + 272) = 0.5429293. create function calculate result automatically classification model. Clearly higher accuracy, better results. best possible accuracy result 1.00.","code":"\nlibrary(ISLR)\nhead(Carseats)\n#>   Sales CompPrice Income Advertising Population Price\n#> 1  9.50       138     73          11        276   120\n#> 2 11.22       111     48          16        260    83\n#> 3 10.06       113     35          10        269    80\n#> 4  7.40       117    100           4        466    97\n#> 5  4.15       141     64           3        340   128\n#> 6 10.81       124    113          13        501    72\n#>   ShelveLoc Age Education Urban  US\n#> 1       Bad  42        17   Yes Yes\n#> 2      Good  65        10   Yes Yes\n#> 3    Medium  59        12   Yes Yes\n#> 4    Medium  55        14   Yes Yes\n#> 5       Bad  38        13   Yes  No\n#> 6       Bad  78        16    No Yes"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"build-our-first-classification-model-from-the-structure","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.2 Build our first classification model from the structure:","text":"Now structure, let’s build model:Now see one individual classification model made, let’s make 11 (total 12).","code":"\n\n# Load libraries\n\n# Set initial values to 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\n\n# Set up resamples\n\n# Randomize the rows\n\n# Split data into train and test sets\n\n# Fit the model on the training data\n\n# Check accuracy and make predictions from the model, applied to the test data\n\n# Calculate overall model accuracy\n\n# Calculate table\n# Print table results\n\n# Return accuracy results\n\n# Closing braces for numresamples loop\n# Closing brace for classification1 function\n\n# Test the function\n\n# Check for errors\n\n# Load libraries\nlibrary(C50)\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Set initial values to 0:\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_validation_accuracy <- 0\nC50_overfitting <- 0\nC50_holdout <- 0\nC50_table_total <- 0\n\n# Build the function:\nC50_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Changes target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\nC50_train_pred <- predict(C50_train_fit, train)\nC50_train_table <- table(C50_train_pred, y_train)\nC50_train_accuracy[i] <- sum(diag(C50_train_table)) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\nC50_train_mean <- mean(diag(C50_train_table)) / mean(C50_train_table)\n\n# Check accuracy and make predictions from the model, applied to the test data\nC50_test_pred <- predict(C50_train_fit, test)\nC50_test_table <- table(C50_test_pred, y_test)\nC50_test_accuracy[i] <- sum(diag(C50_test_table)) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\nC50_test_mean <- mean(diag(C50_test_table)) / mean(C50_test_table)\n\n# Calculate accuracy\nC50_holdout[i] <- mean(c(C50_test_accuracy_mean))\nC50_holdout_mean <- mean(C50_holdout)\n\n# Calculate table\nC50_table <- C50_test_table\nC50_table_total <- C50_table_total + C50_table\n\nprint(C50_table_total)\n\n# Return accuracy result\nreturn(c(C50_holdout_mean))\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\nC50_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>              y_test\n#> C50_test_pred Bad Good Medium\n#>        Bad     11    2     10\n#>        Good     1   12     10\n#>        Medium  15    8     39\n#> [1] 0.5740741"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"adabag-for-classification-data","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.3 Adabag for classification data","text":"","code":"\n\n# Load libraries\nlibrary(ipred)\n\n# Set initial values to 0\nadabag_train_accuracy <- 0\nadabag_test_accuracy <- 0\nadabag_validation_accuracy <- 0\nadabag_holdout <- 0\nadabag_table_total <- 0\n\n# Build the function\nadabag_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nadabag_train_fit <- ipred::bagging(formula = y ~ ., data = train01)\nadabag_train_pred <- predict(object = adabag_train_fit, newdata = train)\nadabag_train_table <- table(adabag_train_pred, y_train)\nadabag_train_accuracy[i] <- sum(diag(adabag_train_table)) / sum(adabag_train_table)\nadabag_train_accuracy_mean <- mean(adabag_train_accuracy)\n  \n# Check accuracy and make predictions from the model, applied to the test data\nadabag_test_pred <- predict(object = adabag_train_fit, newdata = test01)\nadabag_test_table <- table(adabag_test_pred, y_test)\nadabag_test_accuracy[i] <- sum(diag(adabag_test_table)) / sum(adabag_test_table)\nadabag_test_accuracy_mean <- mean(adabag_test_accuracy)\nadabag_test_mean <- mean(diag(adabag_test_table)) / mean(adabag_test_table)\n\n# Calculate accuracy\nadabag_holdout[i] <- mean(c(adabag_test_accuracy_mean))\nadabag_holdout_mean <- mean(adabag_holdout)\n\n# Calculate table\nadabag_table <- adabag_test_table\nadabag_table_total <- adabag_table_total + adabag_table\n\nprint(adabag_table_total)\n\n# Return accuracy results\nreturn(adabag_holdout_mean)\n\n} # Closing braces for numresamples loop\n\n} # Closing brace for classification1 function\n\n# Test the function\nadabag_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> adabag_test_pred Bad Good Medium\n#>           Bad     12    1      6\n#>           Good     0   11      5\n#>           Medium  14    9     44\n#> [1] 0.6568627\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"bagged-random-forest-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.4 Bagged Random Forest","text":"","code":"\n\n# Load libraries\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\n\n# Set initial values to 0\nbag_rf_train_accuracy <- 0\nbag_rf_test_accuracy <- 0\nbag_rf_validation_accuracy <- 0\nbag_rf_overfitting <- 0\nbag_rf_holdout <- 0\nbag_rf_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nbag_rf_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nbag_rf_train_fit <- randomForest::randomForest(y ~ ., data = train01, mtry = ncol(train))\nbag_rf_train_pred <- predict(bag_rf_train_fit, train, type = \"class\")\nbag_rf_train_table <- table(bag_rf_train_pred, y_train)\nbag_rf_train_accuracy[i] <- sum(diag(bag_rf_train_table)) / sum(bag_rf_train_table)\nbag_rf_train_accuracy_mean <- mean(bag_rf_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nbag_rf_test_pred <- predict(bag_rf_train_fit, test, type = \"class\")\nbag_rf_test_table <- table(bag_rf_test_pred, y_test)\nbag_rf_test_accuracy[i] <- sum(diag(bag_rf_test_table)) / sum(bag_rf_test_table)\nbag_rf_test_accuracy_mean <- mean(bag_rf_test_accuracy)\n\n# Calculate model accuracy\nbag_rf_holdout[i] <- mean(c(bag_rf_test_accuracy_mean))\nbag_rf_holdout_mean <- mean(bag_rf_holdout)\n\n# Calculate table\nbag_rf_table <- bag_rf_test_table\nbag_rf_table_total <- bag_rf_table_total + bag_rf_table\n\n# Print table results\nprint(bag_rf_table_total)\n\n# Return accuracy results\nreturn(bag_rf_holdout_mean)\n\n}# Closing braces for numresamples loop\n}# Closing brace for classification1 function\n\n# Test the function\nbag_rf_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> bag_rf_test_pred Bad Good Medium\n#>           Bad     11    0      2\n#>           Good     0   11      5\n#>           Medium  19    9     37\n#> [1] 0.6276596\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"linear-model","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.5 Linear model","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n#> \n#> Attaching package: 'MachineShop'\n#> The following object is masked from 'package:purrr':\n#> \n#>     lift\n#> The following object is masked from 'package:stats':\n#> \n#>     ppr\n\n# Set initial values to 0\nlinear_train_accuracy <- 0\nlinear_validation_accuracy <- 0\nlinear_test_accuracy <- 0\nlinear_test_accuracy_mean <- 0\nlinear_holdout <- 0\nlinear_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nlinear1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nlinear_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"LMModel\")\nlinear_train_pred <- predict(object = linear_train_fit, newdata = train01)\nlinear_train_table <- table(linear_train_pred, y_train)\nlinear_train_accuracy[i] <- sum(diag(linear_train_table)) / sum(linear_train_table)\nlinear_train_accuracy_mean <- mean(linear_train_accuracy)\nlinear_train_mean <- mean(diag(linear_train_table)) / mean(linear_train_table)\n\n# Check accuracy and make predictions from the model, applied to the test data\nlinear_test_pred <- predict(object = linear_train_fit, newdata = test01)\nlinear_test_table <- table(linear_test_pred, y_test)\nlinear_test_accuracy[i] <- sum(diag(linear_test_table)) / sum(linear_test_table)\nlinear_test_accuracy_mean <- mean(linear_test_accuracy)\n\n# Calculate overall model accuracy\nlinear_holdout[i] <- mean(c(linear_test_accuracy_mean))\nlinear_holdout_mean <- mean(linear_holdout)\n\n# Calculate table\nlinear_table <- linear_test_table\nlinear_table_total <- linear_table_total + linear_table\n\n# Print table results\nprint(linear_table_total)\n\n# Return accuracy results\nreturn(linear_holdout_mean)\n\n}# Closing braces for numresamples loop\n}# Closing brace for classification1 function\n\n# Test the function\nlinear1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> linear_test_pred Bad Good Medium\n#>           Bad      5    0      0\n#>           Good     0   11      1\n#>           Medium  30    9     45\n#> [1] 0.6039604\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"naive-bayes-model","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.6 Naive Bayes model","text":"","code":"\n\n# Load libraries\nlibrary(e1071)\n\n# Set initial values to 0\nn_bayes_train_accuracy <- 0\nn_bayes_test_accuracy <- 0\nn_bayes_accuracy <- 0\nn_bayes_test_accuracy_mean <- 0\nn_bayes_holdout <- 0\nn_bayes_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nn_bayes_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nn_bayes_train_fit <- e1071::naiveBayes(y_train ~ ., data = train)\nn_bayes_train_pred <- predict(n_bayes_train_fit, train)\nn_bayes_train_table <- table(n_bayes_train_pred, y_train)\nn_bayes_train_accuracy[i] <- sum(diag(n_bayes_train_table)) / sum(n_bayes_train_table)\nn_bayes_train_accuracy_mean <- mean(n_bayes_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nn_bayes_test_pred <- predict(n_bayes_train_fit, test)\nn_bayes_test_table <- table(n_bayes_test_pred, y_test)\nn_bayes_test_accuracy[i] <- sum(diag(n_bayes_test_table)) / sum(n_bayes_test_table)\nn_bayes_test_accuracy_mean <- mean(n_bayes_test_accuracy)\n\n# Calculate overall model accuracy\nn_bayes_holdout[i] <- mean(c(n_bayes_test_accuracy_mean))\nn_bayes_holdout_mean <- mean(n_bayes_holdout)\n\n# Calculate table\nn_bayes_table <- n_bayes_test_table\nn_bayes_table_total <- n_bayes_table_total + n_bayes_table\n\n# Print table results\nprint(n_bayes_table_total)\n\n# Return accuracy results\nreturn(n_bayes_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nn_bayes_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                  y_test\n#> n_bayes_test_pred Bad Good Medium\n#>            Bad      5    0      3\n#>            Good     0    6      5\n#>            Medium  21   11     39\n#> [1] 0.5555556\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"partial-least-squares-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.7 Partial Least Squares","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\npls_train_accuracy <- 0\npls_test_accuracy <- 0\npls_accuracy <- 0\npls_test_accuracy_mean <- 0\npls_holdout <- 0\npls_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\npls_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\npls_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"PLSModel\")\npls_train_predict <- predict(object = pls_train_fit, newdata = train01)\npls_train_table <- table(pls_train_predict, y_train)\npls_train_accuracy[i] <- sum(diag(pls_train_table)) / sum(pls_train_table)\npls_train_accuracy_mean <- mean(pls_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\npls_test_predict <- predict(object = pls_train_fit, newdata = test01)\npls_test_table <- table(pls_test_predict, y_test)\npls_test_accuracy[i] <- sum(diag(pls_test_table)) / sum(pls_test_table)\npls_test_accuracy_mean <- mean(pls_test_accuracy)\npls_test_pred <- pls_test_predict\n\n# Calculate overall model accuracy\npls_holdout[i] <- mean(c(pls_test_accuracy_mean))\npls_holdout_mean <- mean(pls_holdout)\n\n# Calculate table\npls_table <- pls_test_table\npls_table_total <- pls_table_total + pls_table\n\n# Print table results\nprint(pls_table_total)\n\n# Return accuracy results\nreturn(pls_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\npls_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> pls_test_predict Bad Good Medium\n#>           Bad      0    0      0\n#>           Good     0    0      0\n#>           Medium  24   18     47\n#> [1] 0.5280899\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"penalized-discriminant-analysis-model","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.8 Penalized Discriminant Analysis Model","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\npda_train_accuracy <- 0\npda_test_accuracy <- 0\npda_accuracy <- 0\npda_test_accuracy_mean <- 0\npda_holdout <- 0\npda_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\npda_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\npda_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"PDAModel\")\npda_train_predict <- predict(object = pda_train_fit, newdata = train01)\npda_train_table <- table(pda_train_predict, y_train)\npda_train_accuracy[i] <- sum(diag(pda_train_table)) / sum(pda_train_table)\npda_train_accuracy_mean <- mean(pda_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\npda_test_predict <- predict(object = pda_train_fit, newdata = test01)\npda_test_table <- table(pda_test_predict, y_test)\npda_test_accuracy[i] <- sum(diag(pda_test_table)) / sum(pda_test_table)\npda_test_accuracy_mean <- mean(pda_test_accuracy)\npda_test_pred <- pda_test_predict\n\n# Calculate overall model accuracy\npda_holdout[i] <- mean(c(pda_test_accuracy_mean))\npda_holdout_mean <- mean(pda_holdout)\n\n# Calculate table\npda_table <- pda_test_table\npda_table_total <- pda_table_total + pda_table\n\n# Print table results\nprint(pda_table_total)\n\n# Return accuracy results\nreturn(pda_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\npda_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                 y_test\n#> pda_test_predict Bad Good Medium\n#>           Bad     12    0      6\n#>           Good     0   17      4\n#>           Medium   7    2     42\n#> [1] 0.7888889\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"random-forest-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.9 Random Forest","text":"","code":"\n\n# Load libraries\nlibrary(randomForest)\n\n# Set initial values to 0\nrf_train_accuracy <- 0\nrf_test_accuracy <- 0\nrf_accuracy <- 0\nrf_test_accuracy_mean <- 0\nrf_holdout <- 0\nrf_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nrf_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nrf_train_fit <- randomForest::randomForest(x = train, y = y_train, data = df)\nrf_train_pred <- predict(rf_train_fit, train, type = \"class\")\nrf_train_table <- table(rf_train_pred, y_train)\nrf_train_accuracy[i] <- sum(diag(rf_train_table)) / sum(rf_train_table)\nrf_train_accuracy_mean <- mean(rf_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nrf_test_pred <- predict(rf_train_fit, test, type = \"class\")\nrf_test_table <- table(rf_test_pred, y_test)\nrf_test_accuracy[i] <- sum(diag(rf_test_table)) / sum(rf_test_table)\nrf_test_accuracy_mean <- mean(rf_test_accuracy)\n\n# Calculate overall model accuracy\nrf_holdout[i] <- mean(c(rf_test_accuracy_mean))\nrf_holdout_mean <- mean(rf_holdout)\n\n# Calculate table\nrf_table <- rf_test_table\nrf_table_total <- rf_table_total + rf_table\n\n# Print table results\nprint(rf_table_total)\n\n# Return accuracy results\nreturn(rf_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nrf_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>             y_test\n#> rf_test_pred Bad Good Medium\n#>       Bad      5    0      2\n#>       Good     1   12      0\n#>       Medium  24   10     41\n#> [1] 0.6105263\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"ranger","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.10 Ranger","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\nranger_train_accuracy <- 0\nranger_test_accuracy <- 0\nranger_accuracy <- 0\nranger_test_accuracy_mean <- 0\nranger_holdout <- 0\nranger_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nranger_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n  \n# Fit the model on the training data\nranger_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"RangerModel\")\nranger_train_predict <- predict(object = ranger_train_fit, newdata = train01)\nranger_train_table <- table(ranger_train_predict, y_train)\nranger_train_accuracy[i] <- sum(diag(ranger_train_table)) / sum(ranger_train_table)\nranger_train_accuracy_mean <- mean(ranger_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nranger_test_predict <- predict(object = ranger_train_fit, newdata = test01)\nranger_test_table <- table(ranger_test_predict, y_test)\nranger_test_accuracy[i] <- sum(diag(ranger_test_table)) / sum(ranger_test_table)\nranger_test_accuracy_mean <- mean(ranger_test_accuracy)\nranger_test_pred <- ranger_test_predict\n\n# Calculate overall model accuracy\nranger_holdout[i] <- mean(c(ranger_test_accuracy_mean))\nranger_holdout_mean <- mean(ranger_holdout)\n\n# Calculate table\nranger_table <- ranger_test_table\nranger_table_total <- ranger_table_total + ranger_table\n\n# Print table results\nprint(ranger_table_total)\n\n# Return accuracy results\nreturn(ranger_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nranger_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                    y_test\n#> ranger_test_predict Bad Good Medium\n#>              Bad      8    0      2\n#>              Good     0    9      4\n#>              Medium  21   19     54\n#> [1] 0.6068376\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"regularized-discriminant-analysis","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.11 Regularized Discriminant Analysis","text":"","code":"\n\n# Load libraries\nlibrary(klaR)\n#> Loading required package: MASS\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\n\n# Set initial values to 0\nrda_train_accuracy <- 0\nrda_test_accuracy <- 0\nrda_accuracy <- 0\nrda_test_accuracy_mean <- 0\nrda_holdout <- 0\nrda_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nrda_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nrda_train_fit <- klaR::rda(y_train ~ ., data = train)\nrda_train_pred <- predict(object = rda_train_fit, newdata = train)\nrda_train_table <- table(rda_train_pred$class, y_train)\nrda_train_accuracy[i] <- sum(diag(rda_train_table)) / sum(rda_train_table)\nrda_train_accuracy_mean <- mean(rda_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nrda_test_pred <- predict(object = rda_train_fit, newdata = test)\nrda_test_table <- table(rda_test_pred$class, y_test)\nrda_test_accuracy[i] <- sum(diag(rda_test_table)) / sum(rda_test_table)\nrda_test_accuracy_mean <- mean(rda_test_accuracy)\n\n# Calculate overall model accuracy\nrda_holdout[i] <- mean(c(rda_test_accuracy_mean))\nrda_holdout_mean <- mean(rda_holdout)\n\n# Calculate table\nrda_table <- rda_test_table\nrda_table_total <- rda_table_total + rda_table\n\n# Print table results\nprint(rda_table_total)\n\n# Return accuracy results\nreturn(rda_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nrda_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>         y_test\n#>          Bad Good Medium\n#>   Bad      0    0      0\n#>   Good     0    0      0\n#>   Medium  19   20     48\n#> [1] 0.5517241\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"rpart-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.12 Rpart","text":"","code":"\n\n# Load libraries\nlibrary(MachineShop)\n\n# Set initial values to 0\nrpart_train_accuracy <- 0\nrpart_test_accuracy <- 0\nrpart_accuracy <- 0\nrpart_test_accuracy_mean <- 0\nrpart_holdout <- 0\nrpart_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nrpart_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nrpart_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"RPartModel\")\nrpart_train_predict <- predict(object = rpart_train_fit, newdata = train01)\nrpart_train_table <- table(rpart_train_predict, y_train)\nrpart_train_accuracy[i] <- sum(diag(rpart_train_table)) / sum(rpart_train_table)\nrpart_train_accuracy_mean <- mean(rpart_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nrpart_test_predict <- predict(object = rpart_train_fit, newdata = test01)\nrpart_test_table <- table(rpart_test_predict, y_test)\nrpart_test_accuracy[i] <- sum(diag(rpart_test_table)) / sum(rpart_test_table)\nrpart_test_accuracy_mean <- mean(rpart_test_accuracy)\nrpart_test_pred <- rpart_test_predict\n\n# Calculate overall model accuracy\nrpart_holdout[i] <- mean(c(rpart_test_accuracy_mean))\nrpart_holdout_mean <- mean(rpart_holdout)\n\n# Calculate table\nrpart_table <- rpart_test_table\nrpart_table_total <- rpart_table_total + rpart_table\n  \n# Print table results\nprint(rpart_table_total)\n  \n# Return accuracy results\nreturn(rpart_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nrpart_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>                   y_test\n#> rpart_test_predict Bad Good Medium\n#>             Bad     12    0      9\n#>             Good     0   13      6\n#>             Medium  12   17     38\n#> [1] 0.588785\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"support-vector-machines-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.13 Support Vector Machines","text":"","code":"\n\n# Load libraries\nlibrary(e1071)\n\n# Set initial values to 0\nsvm_train_accuracy <- 0\nsvm_test_accuracy <- 0\nsvm_accuracy <- 0\nsvm_test_accuracy_mean <- 0\nsvm_holdout <- 0\nsvm_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nsvm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nsvm_train_fit <- e1071::svm(y_train ~ ., data = train, kernel = \"radial\", gamma = 1, cost = 1)\nsvm_train_pred <- predict(svm_train_fit, train, type = \"class\")\nsvm_train_table <- table(svm_train_pred, y_train)\nsvm_train_accuracy[i] <- sum(diag(svm_train_table)) / sum(svm_train_table)\nsvm_train_accuracy_mean <- mean(svm_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nsvm_test_pred <- predict(svm_train_fit, test, type = \"class\")\nsvm_test_table <- table(svm_test_pred, y_test)\nsvm_test_accuracy[i] <- sum(diag(svm_test_table)) / sum(svm_test_table)\nsvm_test_accuracy_mean <- mean(svm_test_accuracy)\n\n# Calculate overall model accuracy\nsvm_holdout[i] <- mean(c(svm_test_accuracy_mean))\nsvm_holdout_mean <- mean(svm_holdout)\n\n# Calculate table\nsvm_table <- svm_test_table\nsvm_table_total <- svm_table_total + svm_table\n\n# Print table results\nprint(svm_table_total)\n\n# Return accuracy results\nreturn(svm_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nsvm_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>              y_test\n#> svm_test_pred Bad Good Medium\n#>        Bad      0    0      0\n#>        Good     0    0      0\n#>        Medium  20   29     59\n#> [1] 0.5462963\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"trees-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.14 Trees","text":"","code":"\n\n# Load libraries\nlibrary(tree)\n\n# Set initial values to 0\ntree_train_accuracy <- 0\ntree_test_accuracy <- 0\ntree_accuracy <- 0\ntree_test_accuracy_mean <- 0\ntree_holdout <- 0\ntree_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\ntree_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\ntree_train_fit <- tree::tree(y_train ~ ., data = train)\ntree_train_pred <- predict(tree_train_fit, train, type = \"class\")\ntree_train_table <- table(tree_train_pred, y_train)\ntree_train_accuracy[i] <- sum(diag(tree_train_table)) / sum(tree_train_table)\ntree_train_accuracy_mean <- mean(tree_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\ntree_test_pred <- predict(tree_train_fit, test, type = \"class\")\ntree_test_table <- table(tree_test_pred, y_test)\ntree_test_accuracy[i] <- sum(diag(tree_test_table)) / sum(tree_test_table)\ntree_test_accuracy_mean <- mean(tree_test_accuracy)\n\n# Calculate overall model accuracy\ntree_holdout[i] <- mean(c(tree_test_accuracy_mean))\ntree_holdout_mean <- mean(tree_holdout)\n\n# Calculate table\ntree_table <- tree_test_table\ntree_table_total <- tree_table_total + tree_table  \n\n# Print table results\nprint(tree_table_total)\n\n# Return accuracy results\nreturn(tree_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\ntree_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>               y_test\n#> tree_test_pred Bad Good Medium\n#>         Bad      8    0     10\n#>         Good     0   10      6\n#>         Medium  15   13     44\n#> [1] 0.5849057\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"xgboost-1","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.15 XGBoost","text":"","code":"\n\n# Load libraries\nlibrary(xgboost)\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\n\n# Set initial values to 0\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_accuracy <- 0\nxgb_test_accuracy_mean <- 0\nxgb_holdout <- 0\nxgb_table_total <- 0\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nxgb_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_train), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_train + 1]\nxgb_train_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_train_accuracy[i] <- sum(diag(xgb_train_table)) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_test + 1]\nxgb_test_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_test_accuracy[i] <- sum(diag(xgb_test_table)) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\n# Calculate overall model accuracy\nxgb_holdout[i] <- mean(c(xgb_test_accuracy_mean))\nxgb_holdout_mean <- mean(xgb_holdout)\n\n# Calculate table\nxgb_table <- xgb_test_table\nxgb_table_total <- xgb_table_total + xgb_table\n\n# Print table results\nprint(xgb_table_total)\n\n# Return accuracy results\nreturn(xgb_holdout_mean)\n\n} # Closing braces for numresamples loop\n} # Closing brace for classification1 function\n\n# Test the function\nxgb_1(data = ISLR::Carseats, colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.20)\n#>         \n#>          Bad Good Medium\n#>   Bad      7    0      3\n#>   Good     1   10      5\n#>   Medium  20   15     42\n#> [1] 0.5728155\n\n# Check for errors\nwarnings()"},{"path":"classification-data-how-to-make-14-individual-classification-models.html","id":"post-your-results","chapter":"5 Classification data: How to make 14 individual classification models","heading":"5.0.16 Post your results","text":"","code":""},{"path":"building-ensembles-of-classification-models.html","id":"building-ensembles-of-classification-models","chapter":"6 Building ensembles of classification models","heading":"6 Building ensembles of classification models","text":"section building two ensembles classification models. use six classification models, six ensembles, total 12 results.","code":""},{"path":"building-ensembles-of-classification-models.html","id":"lets-start-at-the-end-and-work-backwards","chapter":"6 Building ensembles of classification models","heading":"6.0.1 Let’s start at the end and work backwards","text":"know want finish predictions ensemble classification models. Therefore need ensemble classification models. Therefore need classification models. Let’s choose five classification models individual models, use five ensemble. Note may use modeling method wish ensemble, since ’s data.final result look something like :Predictions holdout data classification model 1Predictions holdout data classification model 2Predictions holdout data classification model 3Predictions holdout data classification model 4Predictions holdout data classification model 5Use predictions make ensembleUse ensemble models make predictions ensemble holdout dataReport resultsLet’s come list five classification models use:Bagged Random ForestC50RangerSupport Vector MachinesXGBoostNote nothing special using five models. number models may used, set good start.solution also add mean duration model finished report.Since basic outline, know want end, ready begin.","code":"\n\n# Load libraries\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::combine()  masks randomForest::combine()\n#> ✖ dplyr::filter()   masks stats::filter()\n#> ✖ dplyr::lag()      masks stats::lag()\n#> ✖ ggplot2::margin() masks randomForest::margin()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(tree)\n\n# Set initial values to 0\nbag_rf_train_accuracy <- 0\nbag_rf_test_accuracy <- 0\nbag_rf_validation_accuracy <- 0\nbag_rf_overfitting <- 0\nbag_rf_holdout <- 0\nbag_rf_table_total <- 0\nbag_rf_duration <- 0\n\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_validation_accuracy <- 0\nC50_overfitting <- 0\nC50_holdout <- 0\nC50_table_total <- 0\nC50_duration <- 0\n\nranger_train_accuracy <- 0\nranger_test_accuracy <- 0\nranger_accuracy <- 0\nranger_test_accuracy_mean <- 0\nranger_holdout <- 0\nranger_table_total <- 0\nranger_duration <- 0\n\nsvm_train_accuracy <- 0\nsvm_test_accuracy <- 0\nsvm_accuracy <- 0\nsvm_test_accuracy_mean <- 0\nsvm_holdout <- 0\nsvm_table_total <- 0\nsvm_duration <- 0\n\nensemble_bag_rf_train_accuracy <- 0\nensemble_bag_rf_test_accuracy <- 0\nensemble_bag_rf_validation_accuracy <- 0\nensemble_bag_rf_overfitting <- 0\nensemble_bag_rf_holdout <- 0\nensemble_bag_rf_table_total <- 0\nensemble_bag_rf_duration <- 0\n\nensemble_C50_train_accuracy <- 0\nensemble_C50_test_accuracy <- 0\nensemble_C50_validation_accuracy <- 0\nensemble_C50_overfitting <- 0\nensemble_C50_holdout <- 0\nensemble_C50_table_total <- 0\nensemble_C50_duration <- 0\n\nensemble_ranger_train_accuracy <- 0\nensemble_ranger_test_accuracy <- 0\nensemble_ranger_validation_accuracy <- 0\nensemble_ranger_overfitting <- 0\nensemble_ranger_holdout <- 0\nensemble_ranger_table_total <- 0\nensemble_ranger_duration <- 0\n\nensemble_rf_train_accuracy <- 0\nensemble_rf_test_accuracy <- 0\nensemble_rf_validation_accuracy <- 0\nensemble_rf_overfitting <- 0\nensemble_rf_holdout <- 0\nensemble_rf_table_total <- 0\nensemble_rf_duration <- 0\n\nensemble_svm_train_accuracy <- 0\nensemble_svm_test_accuracy <- 0\nensemble_svm_validation_accuracy <- 0\nensemble_svm_overfitting <- 0\nensemble_svm_holdout <- 0\nensemble_svm_table_total <- 0\nensemble_svm_duration <- 0\n\n\n# Build the function, functionname <- function(data, colnum, numresamples, train_amount, test_amount){\nclassification_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n\n# Change target column name to y\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n# \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\ndf <- df[sample(nrow(df)), ]\n\n# Set up resamples:\nfor (i in 1:numresamples) {\n \n# Randomize the rows  \ndf <- df[sample(nrow(df)), ]\n\n# Split the data into train and test sets\nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ntrain01 <- train\ntest01 <- test\n\ny_train <- train$y\ny_test <- test$y\n\ntrain <- df[index == 1, ] %>% dplyr::select(-y)\ntest <- df[index == 2, ] %>% dplyr::select(-y)\n\n# Fit the model on the training data\nbag_rf_start <- Sys.time()\nbag_rf_train_fit <- randomForest::randomForest(y ~ ., data = train01, mtry = ncol(train))\nbag_rf_train_pred <- predict(bag_rf_train_fit, train, type = \"class\")\nbag_rf_train_table <- table(bag_rf_train_pred, y_train)\nbag_rf_train_accuracy[i] <- sum(diag(bag_rf_train_table)) / sum(bag_rf_train_table)\nbag_rf_train_accuracy_mean <- mean(bag_rf_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nbag_rf_test_pred <- predict(bag_rf_train_fit, test, type = \"class\")\nbag_rf_test_table <- table(bag_rf_test_pred, y_test)\nbag_rf_test_accuracy[i] <- sum(diag(bag_rf_test_table)) / sum(bag_rf_test_table)\nbag_rf_test_accuracy_mean <- mean(bag_rf_test_accuracy)\n\n# Calculate model accuracy\nbag_rf_holdout[i] <- mean(c(bag_rf_test_accuracy_mean))\nbag_rf_holdout_mean <- mean(bag_rf_holdout)\n\n# Calculate table\nbag_rf_table <- bag_rf_test_table\nbag_rf_table_total <- bag_rf_table_total + bag_rf_table\n\nbag_rf_end <- Sys.time()\nbag_rf_duration[i] <- bag_rf_end - bag_rf_start\nbag_rf_duration_mean <- mean(bag_rf_duration)\n\n# C50 model\nC50_start <- Sys.time()\n# Fit the model on the training data\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\nC50_train_pred <- predict(C50_train_fit, train)\nC50_train_table <- table(C50_train_pred, y_train)\nC50_train_accuracy[i] <- sum(diag(C50_train_table)) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\nC50_train_mean <- mean(diag(C50_train_table)) / mean(C50_train_table)\n\n# Check accuracy and make predictions from the model, applied to the test data\nC50_test_pred <- predict(C50_train_fit, test)\nC50_test_table <- table(C50_test_pred, y_test)\nC50_test_accuracy[i] <- sum(diag(C50_test_table)) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\nC50_test_mean <- mean(diag(C50_test_table)) / mean(C50_test_table)\n\n# Calculate accuracy\nC50_holdout[i] <- mean(c(C50_test_accuracy_mean))\nC50_holdout_mean <- mean(C50_holdout)\n\nC50_end <- Sys.time()\nC50_duration[i] <- C50_end - C50_start\nC50_duration_mean <- mean(C50_duration)\n\n# Ranger model\n\nranger_start <- Sys.time()\n\n# Fit the model on the training data\nranger_train_fit <- MachineShop::fit(y ~ ., data = train01, model = \"RangerModel\")\nranger_train_predict <- predict(object = ranger_train_fit, newdata = train01)\nranger_train_table <- table(ranger_train_predict, y_train)\nranger_train_accuracy[i] <- sum(diag(ranger_train_table)) / sum(ranger_train_table)\nranger_train_accuracy_mean <- mean(ranger_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nranger_test_predict <- predict(object = ranger_train_fit, newdata = test01)\nranger_test_table <- table(ranger_test_predict, y_test)\nranger_test_accuracy[i] <- sum(diag(ranger_test_table)) / sum(ranger_test_table)\nranger_test_accuracy_mean <- mean(ranger_test_accuracy)\nranger_test_pred <- ranger_test_predict\n\n# Calculate overall model accuracy\nranger_holdout[i] <- mean(c(ranger_test_accuracy_mean))\nranger_holdout_mean <- mean(ranger_holdout)\n\nranger_end <- Sys.time()\nranger_duration[i] <- ranger_end - ranger_start\nranger_duration_mean <- mean(ranger_duration)\n\n# Support vector machines\nsvm_start <- Sys.time()\n\nsvm_train_fit <- e1071::svm(y_train ~ ., data = train, kernel = \"radial\", gamma = 1, cost = 1)\nsvm_train_pred <- predict(svm_train_fit, train, type = \"class\")\nsvm_train_table <- table(svm_train_pred, y_train)\nsvm_train_accuracy[i] <- sum(diag(svm_train_table)) / sum(svm_train_table)\nsvm_train_accuracy_mean <- mean(svm_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\nsvm_test_pred <- predict(svm_train_fit, test, type = \"class\")\nsvm_test_table <- table(svm_test_pred, y_test)\nsvm_test_accuracy[i] <- sum(diag(svm_test_table)) / sum(svm_test_table)\nsvm_test_accuracy_mean <- mean(svm_test_accuracy)\n\n# Calculate overall model accuracy\nsvm_holdout[i] <- mean(c(svm_test_accuracy_mean))\nsvm_holdout_mean <- mean(svm_holdout)\n\nsvm_end <- Sys.time()\nsvm_duration[i] <- svm_end - svm_start\nsvm_duration_mean <- mean(svm_duration)\n\n# XGBoost\nxgb_start <- Sys.time()\n\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_accuracy <- 0\nxgb_test_accuracy_mean <- 0\nxgb_holdout <- 0\nxgb_table_total <- 0\nxgb_duration <- 0\n\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_train), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_train + 1]\nxgb_train_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_train_accuracy[i] <- sum(diag(xgb_train_table)) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\n# Check accuracy and make predictions from the model, applied to the test data\ny_train <- as.integer(train01$y) - 1\ny_test <- as.integer(test01$y) - 1\nX_train <- train %>% dplyr::select(dplyr::where(is.numeric))\nX_test <- test %>% dplyr::select(dplyr::where(is.numeric))\n\nxgb_train <- xgboost::xgb.DMatrix(data = as.matrix(X_train), label = y_train)\nxgb_test <- xgboost::xgb.DMatrix(data = as.matrix(X_test), label = y_test)\nxgb_params <- list(\n    booster = \"gbtree\",\n    eta = 0.01,\n    max_depth = 8,\n    gamma = 4,\n    subsample = 0.75,\n    colsample_bytree = 1,\n    objective = \"multi:softprob\",\n    eval_metric = \"mlogloss\",\n    num_class = length(levels(df$y))\n  )\nxgb_model <- xgboost::xgb.train(\n    params = xgb_params,\n    data = xgb_train,\n    nrounds = 5000,\n    verbose = 1\n  )\n\nxgb_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)\nxgb_preds <- as.data.frame(xgb_preds)\ncolnames(xgb_preds) <- levels(df$y)\n\nxgb_preds$PredictedClass <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])\nxgb_preds$ActualClass <- levels(df$y)[y_test + 1]\nxgb_test_table <- table(xgb_preds$PredictedClass, xgb_preds$ActualClass)\nxgb_test_accuracy[i] <- sum(diag(xgb_test_table)) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\n# Calculate overall model accuracy\nxgb_holdout[i] <- mean(c(xgb_test_accuracy_mean))\nxgb_holdout_mean <- mean(xgb_holdout)\n\nxgb_end <- Sys.time()\nxgb_duration[i] <- xgb_end - xgb_start\nxgb_duration_mean <- mean(xgb_duration)\n\n# Build the ensemble of predictions\n\nensemble1 <- data.frame(\n  'Bag_rf' = bag_rf_test_pred,\n  'C50' = C50_test_pred,\n  'Ranger' = ranger_test_predict,\n  'SVM' = svm_test_pred,\n  'XGBoost' = xgb_preds\n)\n\nensemble_row_numbers <- as.numeric(row.names(ensemble1))\nensemble1$y <- df[ensemble_row_numbers, \"y\"]\n\nensemble_index <- sample(c(1:2), nrow(ensemble1), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble1[ensemble_index == 1, ]\nensemble_test <- ensemble1[ensemble_index == 2, ]\nensemble_y_train <- ensemble_train$y\nensemble_y_test <- ensemble_test$y\n\n\n# Ensemble bagged random forest\nensemble_bag_rf_start <- Sys.time()\n\nensemble_bag_train_rf <- randomForest::randomForest(ensemble_y_train ~ ., data = ensemble_train, mtry = ncol(ensemble_train) - 1)\nensemble_bag_rf_train_pred <- predict(ensemble_bag_train_rf, ensemble_train, type = \"class\")\nensemble_bag_rf_train_table <- table(ensemble_bag_rf_train_pred, ensemble_train$y)\nensemble_bag_rf_train_accuracy[i] <- sum(diag(ensemble_bag_rf_train_table)) / sum(ensemble_bag_rf_train_table)\nensemble_bag_rf_train_accuracy_mean <- mean(ensemble_bag_rf_train_accuracy)\n\nensemble_bag_rf_test_pred <- predict(ensemble_bag_train_rf, ensemble_test, type = \"class\")\nensemble_bag_rf_test_table <- table(ensemble_bag_rf_test_pred, ensemble_test$y)\nensemble_bag_rf_test_accuracy[i] <- sum(diag(ensemble_bag_rf_test_table)) / sum(ensemble_bag_rf_test_table)\nensemble_bag_rf_test_accuracy_mean <- mean(ensemble_bag_rf_test_accuracy)\n\nensemble_bag_rf_holdout[i] <- mean(c(ensemble_bag_rf_test_accuracy_mean))\nensemble_bag_rf_holdout_mean <- mean(ensemble_bag_rf_holdout)\n\nensemble_bag_rf_end <- Sys.time()\nensemble_bag_rf_duration[i] <- ensemble_bag_rf_end - ensemble_bag_rf_start\nensemble_bag_rf_duration_mean <- mean(ensemble_bag_rf_duration)\n\n# Ensemble C50\n\nensemble_C50_start <- Sys.time()\n\nensemble_C50_train_fit <- C50::C5.0(ensemble_y_train ~ ., data = ensemble_train)\nensemble_C50_train_pred <- predict(ensemble_C50_train_fit, ensemble_train)\nensemble_C50_train_table <- table(ensemble_C50_train_pred, ensemble_y_train)\nensemble_C50_train_accuracy[i] <- sum(diag(ensemble_C50_train_table)) / sum(ensemble_C50_train_table)\nensemble_C50_train_accuracy_mean <- mean(ensemble_C50_train_accuracy)\n\nensemble_C50_test_pred <- predict(ensemble_C50_train_fit, ensemble_test)\nensemble_C50_test_table <- table(ensemble_C50_test_pred, ensemble_y_test)\nensemble_C50_test_accuracy[i] <- sum(diag(ensemble_C50_test_table)) / sum(ensemble_C50_test_table)\nensemble_C50_test_accuracy_mean <- mean(ensemble_C50_test_accuracy)\n\nensemble_C50_holdout[i] <- mean(c(ensemble_C50_test_accuracy_mean))\nensemble_C50_holdout_mean <- mean(ensemble_C50_holdout)\n\nensemble_C50_end <- Sys.time()\nensemble_C50_duration[i] <- ensemble_C50_end - ensemble_C50_start\nensemble_C50_duration_mean <- mean(ensemble_C50_duration)\n\n# Ensemble using Ranger\n\nensemble_ranger_start <- Sys.time()\n\nensemble_ranger_train_fit <- MachineShop::fit(y ~ ., data = ensemble_train, model = \"RangerModel\")\nensemble_ranger_train_pred <- predict(ensemble_ranger_train_fit, newdata = ensemble_train)\nensemble_ranger_train_table <- table(ensemble_ranger_train_pred, ensemble_y_train)\nensemble_ranger_train_accuracy[i] <- sum(diag(ensemble_ranger_train_table)) / sum(ensemble_ranger_train_table)\nensemble_ranger_train_accuracy_mean <- mean(ensemble_ranger_train_accuracy)\n\nensemble_ranger_test_fit <- MachineShop::fit(y ~ ., data = ensemble_train, model = \"RangerModel\")\nensemble_ranger_test_pred <- predict(ensemble_ranger_test_fit, newdata = ensemble_test)\nensemble_ranger_test_table <- table(ensemble_ranger_test_pred, ensemble_y_test)\nensemble_ranger_test_accuracy[i] <- sum(diag(ensemble_ranger_test_table)) / sum(ensemble_ranger_test_table)\nensemble_ranger_test_accuracy_mean <- mean(ensemble_ranger_test_accuracy)\n\nensemble_ranger_holdout[i] <- mean(c(ensemble_ranger_test_accuracy_mean))\nensemble_ranger_holdout_mean <- mean(ensemble_ranger_holdout)\n\nensemble_ranger_end <- Sys.time()\nensemble_ranger_duration[i] <- ensemble_ranger_end - ensemble_ranger_start\nensemble_ranger_duration_mean <- mean(ensemble_ranger_duration)\n\n# Ensemble Random Forest\n\nensemble_rf_start <- Sys.time()\n\nensemble_train_rf_fit <- randomForest::randomForest(x = ensemble_train, y = ensemble_y_train)\nensemble_rf_train_pred <- predict(ensemble_train_rf_fit, ensemble_train, type = \"class\")\nensemble_rf_train_table <- table(ensemble_rf_train_pred, ensemble_y_train)\nensemble_rf_train_accuracy[i] <- sum(diag(ensemble_rf_train_table)) / sum(ensemble_rf_train_table)\nensemble_rf_train_accuracy_mean <- mean(ensemble_rf_train_accuracy)\n\nensemble_rf_test_pred <- predict(ensemble_train_rf_fit, ensemble_test, type = \"class\")\nensemble_rf_test_table <- table(ensemble_rf_test_pred, ensemble_y_test)\nensemble_rf_test_accuracy[i] <- sum(diag(ensemble_rf_test_table)) / sum(ensemble_rf_test_table)\nensemble_rf_test_accuracy_mean <- mean(ensemble_rf_test_accuracy)\n\nensemble_rf_holdout[i] <- mean(c(ensemble_rf_test_accuracy_mean))\nensemble_rf_holdout_mean <- mean(ensemble_rf_holdout)\n\nensemble_rf_end <- Sys.time()\nensemble_rf_duration[i] <- ensemble_rf_end -ensemble_rf_start\nensemble_rf_duration_mean <- mean(ensemble_rf_duration)\n\n\n# Ensemble Support Vector Machines\n\nensemble_svm_start <- Sys.time()\n\nensemble_svm_train_fit <- e1071::svm(ensemble_y_train ~ ., data = ensemble_train, kernel = \"radial\", gamma = 1, cost = 1)\nensemble_svm_train_pred <- predict(ensemble_svm_train_fit, ensemble_train, type = \"class\")\nensemble_svm_train_table <- table(ensemble_svm_train_pred, ensemble_y_train)\nensemble_svm_train_accuracy[i] <- sum(diag(ensemble_svm_train_table)) / sum(ensemble_svm_train_table)\nensemble_svm_train_accuracy_mean <- mean(ensemble_svm_train_accuracy)\n\nensemble_svm_test_fit <- e1071::svm(ensemble_y_train ~ ., data = ensemble_train, kernel = \"radial\", gamma = 1, cost = 1)\nensemble_svm_test_pred <- predict(ensemble_svm_test_fit, ensemble_test, type = \"class\")\nensemble_svm_test_table <- table(ensemble_svm_test_pred, ensemble_y_test)\nensemble_svm_test_accuracy[i] <- sum(diag(ensemble_svm_test_table)) / sum(ensemble_svm_test_table)\nensemble_svm_test_accuracy_mean <- mean(ensemble_svm_test_accuracy)\n\nensemble_svm_holdout[i] <- mean(c(ensemble_svm_test_accuracy_mean))\nensemble_svm_holdout_mean <- mean(ensemble_svm_holdout)\n\nensemble_svm_end <- Sys.time()\nensemble_svm_duration[i] <-  ensemble_svm_end - ensemble_svm_start\nensemble_svm_duration_mean <- mean(ensemble_svm_duration)\n\n# Return accuracy results\n\nresults <- data.frame(\n  'Model' = c('Bagged_Random_Forest', 'C50', 'Ranger', 'Support_Vector_Machines', 'XGBoost', 'Ensemble_Bag_RF', 'Ensemble_C50', 'Ensemble_Ranger', 'Ensemble_RF', 'Ensemble_SVM'),\n  'Accuracy' = c(bag_rf_holdout_mean, C50_holdout_mean, ranger_holdout_mean, svm_holdout_mean, xgb_holdout_mean, ensemble_bag_rf_holdout_mean, ensemble_C50_holdout_mean, ensemble_ranger_holdout_mean, ensemble_rf_holdout_mean, ensemble_svm_holdout_mean),\n  'Duration' = c(bag_rf_duration_mean, C50_duration_mean, ranger_duration_mean, svm_duration_mean, xgb_duration_mean, ensemble_bag_rf_duration_mean, ensemble_C50_duration_mean, ensemble_ranger_duration_mean, ensemble_rf_duration_mean, ensemble_svm_duration_mean)\n)\n\nresults <- results %>% arrange(desc(Accuracy))\n\nreturn(results)\n\n}# Closing braces for numresamples loop\n}# Closing brace for classification1 function\n\nclassification_1(data = ISLR::Carseats,colnum = 7, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#>                      Model  Accuracy    Duration\n#> 1          Ensemble_Bag_RF 1.0000000 0.141160011\n#> 2             Ensemble_C50 1.0000000 0.007802963\n#> 3              Ensemble_RF 0.9558824 0.017611980\n#> 4             Ensemble_SVM 0.7647059 0.006072044\n#> 5                  XGBoost 0.6689655 8.002203941\n#> 6                   Ranger 0.6551724 0.820406914\n#> 7     Bagged_Random_Forest 0.6413793 0.111821890\n#> 8  Support_Vector_Machines 0.5793103 0.015836954\n#> 9                      C50 0.5517241 0.493202925\n#> 10         Ensemble_Ranger 0.4264706 0.036972046\n\nwarnings()\n\ndf1 <- Ensembles::dry_beans_small\n#> Registered S3 method overwritten by 'tsibble':\n#>   method          from\n#>   format.interval inum\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n\nclassification_1(data = df1, colnum = 17, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#>                      Model  Accuracy    Duration\n#> 1          Ensemble_Bag_RF 1.0000000  0.13550806\n#> 2             Ensemble_C50 1.0000000  0.01427794\n#> 3     Bagged_Random_Forest 0.9407666  0.16405606\n#> 4                  XGBoost 0.9337979 23.16261911\n#> 5                   Ranger 0.9198606  0.05157804\n#> 6  Support_Vector_Machines 0.9163763  0.01886702\n#> 7                      C50 0.8815331  0.02872300\n#> 8             Ensemble_SVM 0.8141593  0.01312304\n#> 9              Ensemble_RF 0.7699115  0.17488503\n#> 10         Ensemble_Ranger 0.1858407  0.05227113\nwarnings()"},{"path":"individual-logistic-models.html","id":"individual-logistic-models","chapter":"7 Individual logistic models","heading":"7 Individual logistic models","text":"Logistic data sets extremely powerful. chapter ’ll use evaluate risk type 2 diabetes Pima Indian women, logistic ensembles chapter ’ll use make recommendations improve performance Lebron James.raises good question: can two fields far apart scientific research (Pima Indians) sports analytics (Lebron) connected? ’s structure data , ’s structure data makes easy use.Logistic regression rooted idea logical variable (hence name). variable logical variable specific number options, usually two options. many possible names come result. Names might include true false, presence absence condition (diabetes), success failure making basket.logistic modeling values converted 1 0 (converted already). Let’s start getting Pima Indians data set Kaggle web site:https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-databaseDownload data set, open system. example,can clearly see eight features used predict ninth feature, Outcome. final logistic model form : Outcome ~ ., data = df.far common way using Generalized Linear Models, begin . follow well established method building model, used numerical classification data:Load librarySet initial values 0Create functionSet random resamplingBreak data train testFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setFor set examples also going add results ROC curve, ROC curve printed automatically.results consistent similar results using Generalized Linear Models data set.","code":"\n\ndiabetes <- read.csv('/Users/russconte/diabetes.csv')\nhead(diabetes)\n#>   Pregnancies Glucose BloodPressure SkinThickness Insulin\n#> 1           6     148            72            35       0\n#> 2           1      85            66            29       0\n#> 3           8     183            64             0       0\n#> 4           1      89            66            23      94\n#> 5           0     137            40            35     168\n#> 6           5     116            74             0       0\n#>    BMI DiabetesPedigreeFunction Age Outcome\n#> 1 33.6                    0.627  50       1\n#> 2 26.6                    0.351  31       0\n#> 3 23.3                    0.672  32       1\n#> 4 28.1                    0.167  21       0\n#> 5 43.1                    2.288  33       1\n#> 6 25.6                    0.201  30       0\n\n# Load the library\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(pROC)\n#> Type 'citation(\"pROC\")' for a citation.\n#> \n#> Attaching package: 'pROC'\n#> \n#> The following objects are masked from 'package:stats':\n#> \n#>     cov, smooth, var\n\n# Set initial values to 0\n\nglm_train_accuracy <- 0\nglm_test_accuracy <- 0\nglm_holdout_accuracy <- 0\nglm_duration <- 0\nglm_table_total <- 0\n\n# Create the function\n\nglm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \ncolnames(data)[colnum] <- \"y\"\n\ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n\ndf <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n# Set up random resampling\n\nfor (i in 1:numresamples) {\n  \nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n\ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n\ny_train <- train$y\ny_test <- test$y\n\n# Fit the model to the training data, make predictions on the holdout data\n\nglm_train_fit <- stats::glm(y ~ ., data = train, family = binomial(link = \"logit\"))\n\nglm_train_pred <- stats::predict(glm_train_fit, train, type = \"response\")\nglm_train_predictions <- ifelse(glm_train_pred > 0.5, 1, 0)\nglm_train_table <- table(glm_train_predictions, y_train)\nglm_train_accuracy[i] <- (glm_train_table[1, 1] + glm_train_table[2, 2]) / sum(glm_train_table)\nglm_train_accuracy_mean <- mean(glm_train_accuracy)\n\nglm_test_pred <- stats::predict(glm_train_fit, test, type = \"response\")\nglm_test_predictions <- ifelse(glm_test_pred > 0.5, 1, 0)\nglm_test_table <- table(glm_test_predictions, y_test)\nglm_test_accuracy[i] <- (glm_test_table[1, 1] + glm_test_table[2, 2]) / sum(glm_test_table)\nglm_test_accuracy_mean <- mean(glm_test_accuracy)\n\nglm_holdout_accuracy_mean <- mean(glm_test_accuracy)\n\nglm_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(glm_test_pred))\nglm_auc <- round((pROC::auc(c(test$y), as.numeric(c(glm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(glm_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Generalized Linear Models \", \"(AUC = \", glm_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n\nreturn(glm_holdout_accuracy_mean)\n\n} # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nglm_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7979798\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"how-to-use-non-glm-models-in-logistic-analysis","chapter":"7 Individual logistic models","heading":"7.0.1 How to use non-GLM models in logistic analysis","text":"authors excellent book, Introduction Statistical Learning, describe demonstrate non-GLM methods may used logistic analysis. investigated Linear Discriminant Analysis, Quadratic Discriminant Analysis K-Nearest Neighbors. look total ten methods, though many possible.","code":""},{"path":"individual-logistic-models.html","id":"eight-individual-models-for-logistic-data","chapter":"7 Individual logistic models","heading":"7.0.2 Eight individual models for logistic data","text":"","code":""},{"path":"individual-logistic-models.html","id":"adaboost","chapter":"7 Individual logistic models","heading":"7.0.3 Adaboost","text":"","code":"\n\n# Load the library\nlibrary(MachineShop)\n#> \n#> Attaching package: 'MachineShop'\n#> The following object is masked from 'package:pROC':\n#> \n#>     auc\n#> The following object is masked from 'package:purrr':\n#> \n#>     lift\n#> The following object is masked from 'package:stats':\n#> \n#>     ppr\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nadaboost_train_accuracy <- 0\nadaboost_test_accuracy <- 0\nadaboost_holdout_accuracy <- 0\nadaboost_duration <- 0\nadaboost_table_total <- 0\n\n# Create the function\n\nadaboost_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nadaboost_train_fit <- MachineShop::fit(formula = as.factor(y) ~ ., data = train, model = \"AdaBoostModel\")\n\nadaboost_train_pred <- stats::predict(adaboost_train_fit, train, type = \"prob\")\nadaboost_train_predictions <- ifelse(adaboost_train_pred > 0.5, 1, 0)\nadaboost_train_table <- table(adaboost_train_predictions, y_train)\nadaboost_train_accuracy[i] <- (adaboost_train_table[1, 1] + adaboost_train_table[2, 2]) / sum(adaboost_train_table)\nadaboost_train_accuracy_mean <- mean(adaboost_train_accuracy)\n    \nadaboost_test_pred <- stats::predict(adaboost_train_fit, test, type = \"prob\")\nadaboost_test_predictions <- ifelse(adaboost_test_pred > 0.5, 1, 0)\nadaboost_test_table <- table(adaboost_test_predictions, y_test)\nadaboost_test_accuracy[i] <- (adaboost_test_table[1, 1] + adaboost_test_table[2, 2]) / sum(adaboost_test_table)\nadaboost_test_accuracy_mean <- mean(adaboost_test_accuracy)\n\nadaboost_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(adaboost_test_pred))\nadaboost_auc <- round((pROC::auc(c(test$y), as.numeric(c(adaboost_test_pred)) - 1)), 4)\nprint(pROC::ggroc(adaboost_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"ADAboost Models \", \"(AUC = \", adaboost_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(adaboost_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nadaboost_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Warning in rgl.init(initValue, onlyNULL): RGL: unable to\n#> open X11 display\n#> Warning: 'rgl.init' failed, running with 'rgl.useNULL =\n#> TRUE'.\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.715655\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"bayesglm-1","chapter":"7 Individual logistic models","heading":"7.0.4 BayesGLM","text":"","code":"\n\n# Load the library\nlibrary(arm)\n#> Loading required package: MASS\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nbayesglm_train_accuracy <- 0\nbayesglm_test_accuracy <- 0\nbayesglm_holdout_accuracy <- 0\nbayesglm_duration <- 0\nbayesglm_table_total <- 0\n\n# Create the function\n\nbayesglm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = binomial)\n    \nbayesglm_train_pred <- stats::predict(bayesglm_train_fit, train, type = \"response\")\nbayesglm_train_predictions <- ifelse(bayesglm_train_pred > 0.5, 1, 0)\nbayesglm_train_table <- table(bayesglm_train_predictions, y_train)\nbayesglm_train_accuracy[i] <- (bayesglm_train_table[1, 1] + bayesglm_train_table[2, 2]) / sum(bayesglm_train_table)\nbayesglm_train_accuracy_mean <- mean(bayesglm_train_accuracy)\n\nbayesglm_test_pred <- stats::predict(bayesglm_train_fit, test, type = \"response\")\nbayesglm_test_predictions <- ifelse(bayesglm_test_pred > 0.5, 1, 0)\nbayesglm_test_table <- table(bayesglm_test_predictions, y_test)\n\nbayesglm_test_accuracy[i] <- (bayesglm_test_table[1, 1] + bayesglm_test_table[2, 2]) / sum(bayesglm_test_table)\nbayesglm_test_accuracy_mean <- mean(bayesglm_test_accuracy)\n\nbayesglm_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(bayesglm_test_pred))\nbayesglm_auc <- round((pROC::auc(c(test$y), as.numeric(c(bayesglm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(bayesglm_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"Bayesglm Models \", \"(AUC = \", bayesglm_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(bayesglm_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nbayesglm_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7639344\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"c50","chapter":"7 Individual logistic models","heading":"7.0.5 C50","text":"","code":"\n\n# Load the library\nlibrary(C50)\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_holdout_accuracy <- 0\nC50_duration <- 0\nC50_table_total <- 0\n\n# Create the function\n\nC50_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\n\nC50_train_pred <- stats::predict(C50_train_fit, train, type = \"prob\")\nC50_train_predictions <- ifelse(C50_train_pred[, 2] > 0.5, 1, 0)\nC50_train_table <- table(C50_train_predictions, y_train)\nC50_train_accuracy[i] <- (C50_train_table[1, 1] + C50_train_table[2, 2]) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\n\nC50_test_pred <- stats::predict(C50_train_fit, test, type = \"prob\")\nC50_test_predictions <- ifelse(C50_test_pred[, 2] > 0.5, 1, 0)\nC50_test_table <- table(C50_test_predictions, y_test)\nC50_test_accuracy[i] <- (C50_test_table[1, 1] + C50_test_table[2, 2]) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\n\nC50_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(C50_test_predictions)))\nC50_auc <- round((pROC::auc(c(test$y), as.numeric(c(C50_test_predictions)) - 1)), 4)\nprint(pROC::ggroc(C50_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"C50 ROC curve \", \"(AUC = \", C50_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(C50_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nC50_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 1\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"cubist-1","chapter":"7 Individual logistic models","heading":"7.0.6 Cubist","text":"","code":"\n\n# Load the library\nlibrary(Cubist)\n#> Loading required package: lattice\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\ncubist_train_accuracy <- 0\ncubist_test_accuracy <- 0\ncubist_holdout_accuracy <- 0\ncubist_duration <- 0\ncubist_table_total <- 0\n\n# Create the function\n\ncubist_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\ncubist_train_fit <- Cubist::cubist(x = as.data.frame(train), y = train$y)\n    \ncubist_train_pred <- stats::predict(cubist_train_fit, train, type = \"prob\")\ncubist_train_table <- table(cubist_train_pred, y_train)\ncubist_train_accuracy[i] <- (cubist_train_table[1, 1] + cubist_train_table[2, 2]) / sum(cubist_train_table)\ncubist_train_accuracy_mean <- mean(cubist_train_accuracy)\n\ncubist_test_pred <- stats::predict(cubist_train_fit, test, type = \"prob\")\ncubist_test_table <- table(cubist_test_pred, y_test)\ncubist_test_accuracy[i] <- (cubist_test_table[1, 1] + cubist_test_table[2, 2]) / sum(cubist_test_table)\ncubist_test_accuracy_mean <- mean(cubist_test_accuracy)\n\n\ncubist_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(cubist_test_pred)))\ncubist_auc <- round((pROC::auc(c(test$y), as.numeric(c(cubist_test_pred)) - 1)), 4)\nprint(pROC::ggroc(cubist_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Cubist ROC curve \", \"(AUC = \", cubist_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(cubist_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\ncubist_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 1\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"gradient-boosted-1","chapter":"7 Individual logistic models","heading":"7.0.7 Gradient Boosted","text":"","code":"\n\n# Load the library\nlibrary(gbm)\n#> Loaded gbm 2.1.9\n#> This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\ngb_train_accuracy <- 0\ngb_test_accuracy <- 0\ngb_holdout_accuracy <- 0\ngb_duration <- 0\ngb_table_total <- 0\n\n# Create the function\n\ngb_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\ngb_train_fit <- gbm::gbm(train$y ~ ., data = train, distribution = \"gaussian\", n.trees = 100, shrinkage = 0.1, interaction.depth = 10)\n    \ngb_train_pred <- stats::predict(gb_train_fit, train, type = \"response\")\ngb_train_predictions <- ifelse(gb_train_pred > 0.5, 1, 0)\ngb_train_table <- table(gb_train_predictions, y_train)\ngb_train_accuracy[i] <- (gb_train_table[1, 1] + gb_train_table[2, 2]) / sum(gb_train_table)\ngb_train_accuracy_mean <- mean(gb_train_accuracy)\n\ngb_test_pred <- stats::predict(gb_train_fit, test, type = \"response\")\ngb_test_predictions <- ifelse(gb_test_pred > 0.5, 1, 0)\ngb_test_table <- table(gb_test_predictions, y_test)\ngb_test_accuracy[i] <- (gb_test_table[1, 1] + gb_test_table[2, 2]) / sum(gb_test_table)\ngb_test_accuracy_mean <- mean(gb_test_accuracy)\n\ngb_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(gb_test_pred)))\ngb_auc <- round((pROC::auc(c(test$y), as.numeric(c(gb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(gb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Gradient Boosted ROC curve \", \"(AUC = \", gb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(gb_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\ngb_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Using 100 trees...\n#> Using 100 trees...\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7227414\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"random-forest-2","chapter":"7 Individual logistic models","heading":"7.0.8 Random Forest","text":"","code":"\n\n# Load the library\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nrf_train_accuracy <- 0\nrf_test_accuracy <- 0\nrf_holdout_accuracy <- 0\nrf_duration <- 0\nrf_table_total <- 0\n\n# Create the function\n\nrf_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nrf_train_fit <- randomForest(x = train, y = as.factor(y_train), data = df)\n    \nrf_train_pred <- stats::predict(rf_train_fit, train, type = \"prob\")\nrf_train_probabilities <- ifelse(rf_train_pred > 0.50, 1, 0)[, 2]\nrf_train_table <- table(rf_train_probabilities, y_train)\nrf_train_accuracy[i] <- (rf_train_table[1, 1] + rf_train_table[2, 2]) / sum(rf_train_table)\nrf_train_accuracy_mean <- mean(rf_train_accuracy)\n\nrf_test_pred <- stats::predict(rf_train_fit, test, type = \"prob\")\nrf_test_probabilities <- ifelse(rf_test_pred > 0.50, 1, 0)[, 2]\nrf_test_table <- table(rf_test_probabilities, y_test)\nrf_test_accuracy[i] <- (rf_test_table[1, 1] + rf_test_table[2, 2]) / sum(rf_test_table)\nrf_test_accuracy_mean <- mean(rf_test_accuracy)\n\nrf_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(rf_test_probabilities)))\nrf_auc <- round((pROC::auc(c(test$y), as.numeric(c(rf_test_probabilities)) - 1)), 4)\nprint(pROC::ggroc(rf_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Random Forest \", \"(AUC = \", rf_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(rf_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nrf_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 1\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"support-vector-machines-2","chapter":"7 Individual logistic models","heading":"7.0.9 Support Vector Machines","text":"","code":"\n\n# Load the library\nlibrary(e1071)\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nsvm_train_accuracy <- 0\nsvm_test_accuracy <- 0\nsvm_holdout_accuracy <- 0\nsvm_duration <- 0\nsvm_table_total <- 0\n\n# Create the function\n\nsvm_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n    # Fit the model to the training data, make predictions on the holdout data\nsvm_train_fit <- e1071::svm(as.factor(y) ~ ., data = train)\n    \nsvm_train_pred <- stats::predict(svm_train_fit, train, type = \"prob\")\nsvm_train_table <- table(svm_train_pred, y_train)\nsvm_train_accuracy[i] <- (svm_train_table[1, 1] + svm_train_table[2, 2]) / sum(svm_train_table)\nsvm_train_accuracy_mean <- mean(svm_train_accuracy)\n\nsvm_test_pred <- stats::predict(svm_train_fit, test, type = \"prob\")\nsvm_test_table <- table(svm_test_pred, y_test)\nsvm_test_accuracy[i] <- (svm_test_table[1, 1] + svm_test_table[2, 2]) / sum(svm_test_table)\nsvm_test_accuracy_mean <- mean(svm_test_accuracy)\n\nsvm_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(svm_test_pred)))\nsvm_auc <- round((pROC::auc(c(test$y), as.numeric(c(svm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(svm_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Random Forest \", \"(AUC = \", svm_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(svm_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nsvm_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.754902\n\n# Check for any errors\nwarnings()"},{"path":"individual-logistic-models.html","id":"xgboost-2","chapter":"7 Individual logistic models","heading":"7.0.10 XGBoost","text":"","code":"\n\n# Load the library\nlibrary(xgboost)\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\nlibrary(tidyverse)\nlibrary(pROC)\n\n# Set initial values to 0\n\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_holdout_accuracy <- 0\nxgb_duration <- 0\nxgb_table_total <- 0\n\n# Create the function\n\nxgb_1 <- function(data, colnum, numresamples, train_amount, test_amount){\n  \n  colnames(data)[colnum] <- \"y\"\n  \n  df <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \n  df <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n  # Set up random resampling\n  \n  for (i in 1:numresamples) {\n    \n    index <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \n    train <- df[index == 1, ]\n    test <- df[index == 2, ]\n    \n    y_train <- train$y\n    y_test <- test$y\n    \n# Fit the model to the training data, make predictions on the holdout data\ntrain_x <- data.matrix(train[, -ncol(train)])\ntrain_y <- train[, ncol(train)]\n    \n# define predictor and response variables in test set\ntest_x <- data.matrix(test[, -ncol(test)])\ntest_y <- test[, ncol(test)]\n    \n# define final train and test sets\nxgb_train <- xgboost::xgb.DMatrix(data = train_x, label = train_y)\nxgb_test <- xgboost::xgb.DMatrix(data = test_x, label = test_y)\n\n# define watchlist\nwatchlist <- list(train = xgb_train)\nwatchlist_test <- list(train = xgb_train, test = xgb_test)\n\nxgb_model <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_test, nrounds = 70)\n    \nxgb_min <- which.min(xgb_model$evaluation_log$validation_rmse)\n    \nxgb_train_pred <- stats::predict(object = xgb_model, newdata = train_x, type = \"prob\")\nxgb_train_predictions <- ifelse(xgb_train_pred > 0.5, 1, 0)\nxgb_train_table <- table(xgb_train_predictions, y_train)\nxgb_train_accuracy[i] <- (xgb_train_table[1, 1] + xgb_train_table[2, 2]) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\nxgb_test_pred <- stats::predict(object = xgb_model, newdata = test_x, type = \"prob\")\nxgb_test_predictions <- ifelse(xgb_test_pred > 0.5, 1, 0)\nxgb_test_table <- table(xgb_test_predictions, y_test)\nxgb_test_accuracy[i] <- (xgb_test_table[1, 1] + xgb_test_table[2, 2]) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\nxgb_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(xgb_test_pred)))\nxgb_auc <- round((pROC::auc(c(test$y), as.numeric(c(xgb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(xgb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"XGBoost \", \"(AUC = \", xgb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n    \nreturn(xgb_test_accuracy_mean)\n    \n  } # closing brace for numresamples \n  \n} # Closing brace for the function\n\n# Test the function\nxgb_1(data = diabetes, colnum = 9, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> [1]  train-rmse:0.445408 test-rmse:0.451609 \n#> [2]  train-rmse:0.414125 test-rmse:0.424981 \n#> [3]  train-rmse:0.392889 test-rmse:0.412808 \n#> [4]  train-rmse:0.374376 test-rmse:0.400258 \n#> [5]  train-rmse:0.363075 test-rmse:0.397032 \n#> [6]  train-rmse:0.353474 test-rmse:0.393990 \n#> [7]  train-rmse:0.345974 test-rmse:0.392875 \n#> [8]  train-rmse:0.340500 test-rmse:0.390903 \n#> [9]  train-rmse:0.337987 test-rmse:0.389748 \n#> [10] train-rmse:0.334592 test-rmse:0.388716 \n#> [11] train-rmse:0.329688 test-rmse:0.389387 \n#> [12] train-rmse:0.326464 test-rmse:0.388277 \n#> [13] train-rmse:0.324965 test-rmse:0.388538 \n#> [14] train-rmse:0.322499 test-rmse:0.387743 \n#> [15] train-rmse:0.319469 test-rmse:0.387050 \n#> [16] train-rmse:0.313831 test-rmse:0.388981 \n#> [17] train-rmse:0.310254 test-rmse:0.390239 \n#> [18] train-rmse:0.306899 test-rmse:0.392204 \n#> [19] train-rmse:0.301659 test-rmse:0.393161 \n#> [20] train-rmse:0.299553 test-rmse:0.394776 \n#> [21] train-rmse:0.298237 test-rmse:0.394929 \n#> [22] train-rmse:0.296551 test-rmse:0.394339 \n#> [23] train-rmse:0.291534 test-rmse:0.395549 \n#> [24] train-rmse:0.289980 test-rmse:0.395576 \n#> [25] train-rmse:0.285314 test-rmse:0.397041 \n#> [26] train-rmse:0.279349 test-rmse:0.395518 \n#> [27] train-rmse:0.278130 test-rmse:0.394680 \n#> [28] train-rmse:0.272842 test-rmse:0.394815 \n#> [29] train-rmse:0.270642 test-rmse:0.395134 \n#> [30] train-rmse:0.267811 test-rmse:0.395081 \n#> [31] train-rmse:0.265555 test-rmse:0.395846 \n#> [32] train-rmse:0.261733 test-rmse:0.395215 \n#> [33] train-rmse:0.258715 test-rmse:0.396640 \n#> [34] train-rmse:0.255219 test-rmse:0.395239 \n#> [35] train-rmse:0.253350 test-rmse:0.395672 \n#> [36] train-rmse:0.252564 test-rmse:0.396012 \n#> [37] train-rmse:0.251803 test-rmse:0.396499 \n#> [38] train-rmse:0.248205 test-rmse:0.397349 \n#> [39] train-rmse:0.246352 test-rmse:0.396809 \n#> [40] train-rmse:0.244044 test-rmse:0.397549 \n#> [41] train-rmse:0.242269 test-rmse:0.396941 \n#> [42] train-rmse:0.240035 test-rmse:0.397491 \n#> [43] train-rmse:0.235618 test-rmse:0.398848 \n#> [44] train-rmse:0.233333 test-rmse:0.400615 \n#> [45] train-rmse:0.231688 test-rmse:0.401996 \n#> [46] train-rmse:0.229347 test-rmse:0.402715 \n#> [47] train-rmse:0.228402 test-rmse:0.402862 \n#> [48] train-rmse:0.227969 test-rmse:0.403321 \n#> [49] train-rmse:0.226776 test-rmse:0.403049 \n#> [50] train-rmse:0.223379 test-rmse:0.403438 \n#> [51] train-rmse:0.220481 test-rmse:0.402767 \n#> [52] train-rmse:0.217631 test-rmse:0.402707 \n#> [53] train-rmse:0.216376 test-rmse:0.403022 \n#> [54] train-rmse:0.212842 test-rmse:0.404942 \n#> [55] train-rmse:0.210298 test-rmse:0.405176 \n#> [56] train-rmse:0.208235 test-rmse:0.406070 \n#> [57] train-rmse:0.205781 test-rmse:0.405437 \n#> [58] train-rmse:0.203231 test-rmse:0.405864 \n#> [59] train-rmse:0.201519 test-rmse:0.407112 \n#> [60] train-rmse:0.200186 test-rmse:0.406963 \n#> [61] train-rmse:0.196523 test-rmse:0.408566 \n#> [62] train-rmse:0.195268 test-rmse:0.409664 \n#> [63] train-rmse:0.193201 test-rmse:0.410803 \n#> [64] train-rmse:0.191854 test-rmse:0.412022 \n#> [65] train-rmse:0.189905 test-rmse:0.412147 \n#> [66] train-rmse:0.187798 test-rmse:0.413002 \n#> [67] train-rmse:0.187329 test-rmse:0.412837 \n#> [68] train-rmse:0.186555 test-rmse:0.413153 \n#> [69] train-rmse:0.185096 test-rmse:0.413731 \n#> [70] train-rmse:0.182819 test-rmse:0.414756\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1] 0.7804878\n\n# Check for any errors\nwarnings()"},{"path":"advice-to-lebron-james-and-everyone-who-does-talent-analytics-logistic-ensembles.html","id":"advice-to-lebron-james-and-everyone-who-does-talent-analytics-logistic-ensembles","chapter":"8 Advice to Lebron James (and everyone who does talent analytics): Logistic ensembles","heading":"8 Advice to Lebron James (and everyone who does talent analytics): Logistic ensembles","text":"section ’re going take lessons previous chapter move making ensembles models. process extremely similar, follows steps:Load librarySet initial values 0Create functionSet random resamplingBreak data train testFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setLogistic ensembles can used extremely wide range fields. Previously modeled diabetes Pima Indian women. chapter’s example performance court Lebron James.’s image Lebron play.LOT data sports HR analytics (extremely similar ways). lot data set logistic data. example, data set performance Lebron James. main column interest “result”, either 1 0. Thus perfectly fits requirements logistic analysis.Let’s look structure data:see numbers. might easier “qtr” “opponent” changed factors, ’ll first.Now ’re ready create ensemble models make predictions Lebron’s future performance. new skill chapter saving trained models Environment. allow us look trained models, use make strongest evidence based recommendations.use following individual models ensemble models:Individual models:AdaBoostBayesGLMC50CubistGeneralized Linear Models (GLM)Random ForestXGBoostWe make ensemble predictions five models, use ensemble model predictions Lebron’s performance.also show ROC curves results, save trained models end.","code":"\n\nlibrary(Ensembles)\n#> Loading required package: arm\n#> Loading required package: MASS\n#> Loading required package: Matrix\n#> Loading required package: lme4\n#> \n#> arm (Version 1.14-4, built: 2024-4-1)\n#> Working directory is /Users/russconte/Library/Mobile Documents/com~apple~CloudDocs/Documents/Machine Learning templates in R/EnsemblesBook\n#> Loading required package: brnn\n#> Loading required package: Formula\n#> Loading required package: truncnorm\n#> Loading required package: broom\n#> Loading required package: C50\n#> Loading required package: caret\n#> Loading required package: ggplot2\n#> Loading required package: lattice\n#> Loading required package: class\n#> Loading required package: corrplot\n#> corrplot 0.92 loaded\n#> \n#> Attaching package: 'corrplot'\n#> The following object is masked from 'package:arm':\n#> \n#>     corrplot\n#> Loading required package: Cubist\n#> Loading required package: doParallel\n#> Loading required package: foreach\n#> Loading required package: iterators\n#> Loading required package: parallel\n#> Loading required package: dplyr\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:MASS':\n#> \n#>     select\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n#> Loading required package: e1071\n#> Loading required package: fable\n#> Loading required package: fabletools\n#> Registered S3 method overwritten by 'tsibble':\n#>   method          from\n#>   format.interval inum\n#> \n#> Attaching package: 'fabletools'\n#> The following object is masked from 'package:e1071':\n#> \n#>     interpolate\n#> The following objects are masked from 'package:caret':\n#> \n#>     MAE, RMSE\n#> The following object is masked from 'package:lme4':\n#> \n#>     refit\n#> Loading required package: fable.prophet\n#> Loading required package: Rcpp\n#> Loading required package: feasts\n#> Loading required package: gam\n#> Loading required package: splines\n#> Loaded gam 1.22-3\n#> Loading required package: gbm\n#> Loaded gbm 2.1.9\n#> This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n#> Loading required package: GGally\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n#> Loading required package: glmnet\n#> Loaded glmnet 4.1-8\n#> Loading required package: gridExtra\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> Loading required package: gt\n#> Loading required package: gtExtras\n#> \n#> Attaching package: 'gtExtras'\n#> The following object is masked from 'package:MASS':\n#> \n#>     select\n#> Loading required package: ipred\n#> Loading required package: kernlab\n#> \n#> Attaching package: 'kernlab'\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     alpha\n#> Loading required package: klaR\n#> Loading required package: leaps\n#> Loading required package: MachineShop\n#> \n#> Attaching package: 'MachineShop'\n#> The following objects are masked from 'package:fabletools':\n#> \n#>     accuracy, response\n#> The following objects are masked from 'package:caret':\n#> \n#>     calibration, lift, precision, recall, rfe,\n#>     sensitivity, specificity\n#> The following object is masked from 'package:stats':\n#> \n#>     ppr\n#> Loading required package: magrittr\n#> Loading required package: mda\n#> Loaded mda 0.5-4\n#> \n#> Attaching package: 'mda'\n#> The following object is masked from 'package:MachineShop':\n#> \n#>     confusion\n#> Loading required package: Metrics\n#> \n#> Attaching package: 'Metrics'\n#> The following objects are masked from 'package:MachineShop':\n#> \n#>     accuracy, auc, mae, mse, msle, precision, recall,\n#>     rmse, rmsle\n#> The following object is masked from 'package:fabletools':\n#> \n#>     accuracy\n#> The following objects are masked from 'package:caret':\n#> \n#>     precision, recall\n#> Loading required package: neuralnet\n#> \n#> Attaching package: 'neuralnet'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     compute\n#> Loading required package: pls\n#> \n#> Attaching package: 'pls'\n#> The following object is masked from 'package:corrplot':\n#> \n#>     corrplot\n#> The following object is masked from 'package:caret':\n#> \n#>     R2\n#> The following objects are masked from 'package:arm':\n#> \n#>     coefplot, corrplot\n#> The following object is masked from 'package:stats':\n#> \n#>     loadings\n#> Loading required package: pROC\n#> Type 'citation(\"pROC\")' for a citation.\n#> \n#> Attaching package: 'pROC'\n#> The following object is masked from 'package:Metrics':\n#> \n#>     auc\n#> The following object is masked from 'package:MachineShop':\n#> \n#>     auc\n#> The following objects are masked from 'package:stats':\n#> \n#>     cov, smooth, var\n#> Loading required package: purrr\n#> \n#> Attaching package: 'purrr'\n#> The following object is masked from 'package:magrittr':\n#> \n#>     set_names\n#> The following object is masked from 'package:MachineShop':\n#> \n#>     lift\n#> The following object is masked from 'package:kernlab':\n#> \n#>     cross\n#> The following objects are masked from 'package:foreach':\n#> \n#>     accumulate, when\n#> The following object is masked from 'package:caret':\n#> \n#>     lift\n#> Loading required package: randomForest\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:gridExtra':\n#> \n#>     combine\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\n#> Loading required package: reactable\n#> Loading required package: reactablefmtr\n#> \n#> Attaching package: 'reactablefmtr'\n#> The following object is masked from 'package:randomForest':\n#> \n#>     margin\n#> The following objects are masked from 'package:gt':\n#> \n#>     google_font, html\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\n#> Loading required package: readr\n#> Loading required package: rpart\n#> Loading required package: scales\n#> \n#> Attaching package: 'scales'\n#> The following object is masked from 'package:readr':\n#> \n#>     col_factor\n#> The following object is masked from 'package:purrr':\n#> \n#>     discard\n#> The following object is masked from 'package:kernlab':\n#> \n#>     alpha\n#> The following object is masked from 'package:arm':\n#> \n#>     rescale\n#> Loading required package: tibble\n#> Loading required package: tidyr\n#> \n#> Attaching package: 'tidyr'\n#> The following object is masked from 'package:magrittr':\n#> \n#>     extract\n#> The following objects are masked from 'package:Matrix':\n#> \n#>     expand, pack, unpack\n#> Loading required package: tree\n#> Loading required package: tsibble\n#> \n#> Attaching package: 'tsibble'\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, union\n#> Loading required package: xgboost\n#> \n#> Attaching package: 'xgboost'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     slice\nhead(lebron, n = 20)\n#>    top left  date qtr time_remaining result shot_type\n#> 1  310  203 19283   2            566      0         3\n#> 2  213  259 19283   2            518      0         2\n#> 3  143  171 19283   2            490      0         2\n#> 4   68  215 19283   2            324      1         2\n#> 5   66  470 19283   2             62      0         3\n#> 6   63  239 19283   4            690      1         2\n#> 7  230   54 19283   4            630      0         3\n#> 8   53  224 19283   4            605      1         2\n#> 9  241   67 19283   4            570      0         3\n#> 10 273  113 19283   4            535      0         3\n#> 11  62  224 19283   4            426      0         2\n#> 12  63  249 19283   4            233      1         2\n#> 13 103  236 19283   4            154      0         2\n#> 14  54  249 19283   4            108      1         2\n#> 15  53  240 19283   4             58      0         2\n#> 16 230   71 19283   5            649      1         3\n#> 17 231  358 19283   5            540      0         2\n#> 18  61  240 19283   5            524      1         2\n#> 19  59  235 19283   5             71      1         2\n#> 20 299  188 19283   5              6      1         3\n#>    distance_ft lead lebron_team_score opponent_team_score\n#> 1           26    0                 2                   2\n#> 2           16    0                 4                   5\n#> 3           11    0                 4                   7\n#> 4            3    0                12                  19\n#> 5           23    0                22                  23\n#> 6            1    0                24                  25\n#> 7           26    0                24                  27\n#> 8            2    0                26                  27\n#> 9           26    0                26                  29\n#> 10          25    0                26                  32\n#> 11           2    0                31                  39\n#> 12           1    0                39                  49\n#> 13           5    0                39                  51\n#> 14           1    0                44                  53\n#> 15           0    0                46                  55\n#> 16          25    0                58                  63\n#> 17          21    0                60                  70\n#> 18           1    0                62                  70\n#> 19           1    0                68                  91\n#> 20          25    0                71                  91\n#>    opponent\n#> 1         9\n#> 2         9\n#> 3         9\n#> 4         9\n#> 5         9\n#> 6         9\n#> 7         9\n#> 8         9\n#> 9         9\n#> 10        9\n#> 11        9\n#> 12        9\n#> 13        9\n#> 14        9\n#> 15        9\n#> 16        9\n#> 17        9\n#> 18        9\n#> 19        9\n#> 20        9\nlebron <- Ensembles::lebron\nstr(Ensembles::lebron)\n#> 'data.frame':    1533 obs. of  12 variables:\n#>  $ top                : int  310 213 143 68 66 63 230 53 241 273 ...\n#>  $ left               : int  203 259 171 215 470 239 54 224 67 113 ...\n#>  $ date               : num  19283 19283 19283 19283 19283 ...\n#>  $ qtr                : num  2 2 2 2 2 4 4 4 4 4 ...\n#>  $ time_remaining     : num  566 518 490 324 62 690 630 605 570 535 ...\n#>  $ result             : num  0 0 0 1 0 1 0 1 0 0 ...\n#>  $ shot_type          : int  3 2 2 2 3 2 3 2 3 3 ...\n#>  $ distance_ft        : int  26 16 11 3 23 1 26 2 26 25 ...\n#>  $ lead               : num  0 0 0 0 0 0 0 0 0 0 ...\n#>  $ lebron_team_score  : int  2 4 4 12 22 24 24 26 26 26 ...\n#>  $ opponent_team_score: int  2 5 7 19 23 25 27 27 29 32 ...\n#>  $ opponent           : num  9 9 9 9 9 9 9 9 9 9 ...\n\nlebron$qtr <- as.factor(lebron$qtr)\nlebron$opponent <- as.factor(lebron$opponent)\n\n# Load libraries - note these will work with individual and ensemble models\nlibrary(arm) # to use with BayesGLM\nlibrary(C50) # To use with C50\nlibrary(Cubist) # To use with Cubist modeling\nlibrary(MachineShop)# To use with ADABoost\nlibrary(randomForest) # Random Forest models\nlibrary(tidyverse) # My favorite tool for data science!\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ lubridate 1.9.3     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ purrr::accumulate()     masks foreach::accumulate()\n#> ✖ scales::alpha()         masks kernlab::alpha(), ggplot2::alpha()\n#> ✖ scales::col_factor()    masks readr::col_factor()\n#> ✖ randomForest::combine() masks gridExtra::combine(), dplyr::combine()\n#> ✖ neuralnet::compute()    masks dplyr::compute()\n#> ✖ purrr::cross()          masks kernlab::cross()\n#> ✖ scales::discard()       masks purrr::discard()\n#> ✖ tidyr::expand()         masks Matrix::expand()\n#> ✖ tidyr::extract()        masks magrittr::extract()\n#> ✖ dplyr::filter()         masks stats::filter()\n#> ✖ lubridate::interval()   masks tsibble::interval()\n#> ✖ dplyr::lag()            masks stats::lag()\n#> ✖ purrr::lift()           masks MachineShop::lift(), caret::lift()\n#> ✖ reactablefmtr::margin() masks randomForest::margin(), ggplot2::margin()\n#> ✖ tidyr::pack()           masks Matrix::pack()\n#> ✖ gtExtras::select()      masks dplyr::select(), MASS::select()\n#> ✖ purrr::set_names()      masks magrittr::set_names()\n#> ✖ xgboost::slice()        masks dplyr::slice()\n#> ✖ tidyr::unpack()         masks Matrix::unpack()\n#> ✖ purrr::when()           masks foreach::when()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(pROC) # To print ROC curves\n\n# Set initial values to 0\nadaboost_train_accuracy <- 0\nadaboost_test_accuracy <- 0\nadaboost_holdout_accuracy <- 0\nadaboost_duration <- 0\nadaboost_table_total <- 0\n\nbayesglm_train_accuracy <- 0\nbayesglm_test_accuracy <- 0\nbayesglm_holdout_accuracy <- 0\nbayesglm_duration <- 0\nbayesglm_table_total <- 0\n\nC50_train_accuracy <- 0\nC50_test_accuracy <- 0\nC50_holdout_accuracy <- 0\nC50_duration <- 0\nC50_table_total <- 0\n\ncubist_train_accuracy <- 0\ncubist_test_accuracy <- 0\ncubist_holdout_accuracy <- 0\ncubist_duration <- 0\ncubist_table_total <- 0\n\nrf_train_accuracy <- 0\nrf_test_accuracy <- 0\nrf_holdout_accuracy <- 0\nrf_duration <- 0\nrf_table_total <- 0\n\nxgb_train_accuracy <- 0\nxgb_test_accuracy <- 0\nxgb_holdout_accuracy <- 0\nxgb_duration <- 0\nxgb_table_total <- 0\n\n\nensemble_adaboost_train_accuracy <- 0\nensemble_adaboost_test_accuracy <- 0\nensemble_adaboost_holdout_accuracy <- 0\nensemble_adaboost_duration <- 0\nensemble_adaboost_table_total <- 0\nensemble_adaboost_train_pred <- 0\n\nensemble_bayesglm_train_accuracy <- 0\nensemble_bayesglm_test_accuracy <- 0\nensemble_bayesglm_holdout_accuracy <- 0\nensemble_bayesglm_duration <- 0\nensemble_bayesglm_table_total <- 0\n\nensemble_C50_train_accuracy <- 0\nensemble_C50_test_accuracy <- 0\nensemble_C50_holdout_accuracy <- 0\nensemble_C50_duration <- 0\nensemble_C50_table_total <- 0\n\nensemble_rf_train_accuracy <- 0\nensemble_rf_test_accuracy <- 0\nensemble_rf_holdout_accuracy <- 0\nensemble_rf_duration <- 0\nensemble_rf_table_total <- 0\n\nensemble_xgb_train_accuracy <- 0\nensemble_xgb_test_accuracy <- 0\nensemble_xgb_holdout_accuracy <- 0\nensemble_xgb_duration <- 0\nensemble_xgb_table_total <- 0\n\n# Create the function\n\nlogistic_1 <- function(data, colnum, numresamples, train_amount, test_amount){\ncolnames(data)[colnum] <- \"y\"\n  \ndf <- data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n  \ndf <- df[sample(1:nrow(df)), ] # randomizes the rows\n  \n# Set up random resampling\n  \nfor (i in 1:numresamples) {\n    \nindex <- sample(c(1:2), nrow(df), replace = TRUE, prob = c(train_amount, test_amount))\n    \ntrain <- df[index == 1, ]\ntest <- df[index == 2, ]\n    \ny_train <- train$y\ny_test <- test$y\n  \n  \n# ADABoost model\nadaboost_train_fit <- MachineShop::fit(formula = as.factor(y) ~ ., data = train, model = \"AdaBoostModel\")\n\nadaboost_train_pred <- stats::predict(adaboost_train_fit, train, type = \"prob\")\nadaboost_train_predictions <- ifelse(adaboost_train_pred > 0.5, 1, 0)\nadaboost_train_table <- table(adaboost_train_predictions, y_train)\nadaboost_train_accuracy[i] <- (adaboost_train_table[1, 1] + adaboost_train_table[2, 2]) / sum(adaboost_train_table)\nadaboost_train_accuracy_mean <- mean(adaboost_train_accuracy)\n\nadaboost_test_pred <- stats::predict(adaboost_train_fit, test, type = \"prob\")\nadaboost_test_predictions <- ifelse(adaboost_test_pred > 0.5, 1, 0)\nadaboost_test_table <- table(adaboost_test_predictions, y_test)\nadaboost_test_accuracy[i] <- (adaboost_test_table[1, 1] + adaboost_test_table[2, 2]) / sum(adaboost_test_table)\nadaboost_test_accuracy_mean <- mean(adaboost_test_accuracy)\n\nadaboost_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(adaboost_test_pred))\nadaboost_auc <- round((pROC::auc(c(test$y), as.numeric(c(adaboost_test_pred)) - 1)), 4)\nprint(pROC::ggroc(adaboost_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"ADAboost Models \", \"(AUC = \", adaboost_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n\n# BayesGLM\nbayesglm_train_fit <- arm::bayesglm(y ~ ., data = train, family = binomial)\n    \nbayesglm_train_pred <- stats::predict(bayesglm_train_fit, train, type = \"response\")\nbayesglm_train_predictions <- ifelse(bayesglm_train_pred > 0.5, 1, 0)\nbayesglm_train_table <- table(bayesglm_train_predictions, y_train)\nbayesglm_train_accuracy[i] <- (bayesglm_train_table[1, 1] + bayesglm_train_table[2, 2]) / sum(bayesglm_train_table)\nbayesglm_train_accuracy_mean <- mean(bayesglm_train_accuracy)\n\nbayesglm_test_pred <- stats::predict(bayesglm_train_fit, test, type = \"response\")\nbayesglm_test_predictions <- ifelse(bayesglm_test_pred > 0.5, 1, 0)\nbayesglm_test_table <- table(bayesglm_test_predictions, y_test)\n\nbayesglm_test_accuracy[i] <- (bayesglm_test_table[1, 1] + bayesglm_test_table[2, 2]) / sum(bayesglm_test_table)\nbayesglm_test_accuracy_mean <- mean(bayesglm_test_accuracy)\n\nbayesglm_roc_obj <- pROC::roc(as.numeric(c(test$y)), c(bayesglm_test_pred))\nbayesglm_auc <- round((pROC::auc(c(test$y), as.numeric(c(bayesglm_test_pred)) - 1)), 4)\nprint(pROC::ggroc(bayesglm_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"Bayesglm Models \", \"(AUC = \", bayesglm_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# C50 model\n\nC50_train_fit <- C50::C5.0(as.factor(y_train) ~ ., data = train)\n\nC50_train_pred <- stats::predict(C50_train_fit, train, type = \"prob\")\nC50_train_predictions <- ifelse(C50_train_pred[, 2] > 0.5, 1, 0)\nC50_train_table <- table(C50_train_predictions, y_train)\nC50_train_accuracy[i] <- (C50_train_table[1, 1] + C50_train_table[2, 2]) / sum(C50_train_table)\nC50_train_accuracy_mean <- mean(C50_train_accuracy)\n\nC50_test_pred <- stats::predict(C50_train_fit, test, type = \"prob\")\nC50_test_predictions <- ifelse(C50_test_pred[, 2] > 0.5, 1, 0)\nC50_test_table <- table(C50_test_predictions, y_test)\nC50_test_accuracy[i] <- (C50_test_table[1, 1] + C50_test_table[2, 2]) / sum(C50_test_table)\nC50_test_accuracy_mean <- mean(C50_test_accuracy)\n\nC50_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(C50_test_predictions)))\nC50_auc <- round((pROC::auc(c(test$y), as.numeric(c(C50_test_predictions)) - 1)), 4)\nprint(pROC::ggroc(C50_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"C50 ROC curve \", \"(AUC = \", C50_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# Cubist\ncubist_train_fit <- Cubist::cubist(x = as.data.frame(train), y = train$y)\n    \ncubist_train_pred <- stats::predict(cubist_train_fit, train, type = \"prob\")\ncubist_train_table <- table(cubist_train_pred, y_train)\ncubist_train_accuracy[i] <- (cubist_train_table[1, 1] + cubist_train_table[2, 2]) / sum(cubist_train_table)\ncubist_train_accuracy_mean <- mean(cubist_train_accuracy)\n\ncubist_test_pred <- stats::predict(cubist_train_fit, test, type = \"prob\")\ncubist_test_table <- table(cubist_test_pred, y_test)\ncubist_test_accuracy[i] <- (cubist_test_table[1, 1] + cubist_test_table[2, 2]) / sum(cubist_test_table)\ncubist_test_accuracy_mean <- mean(cubist_test_accuracy)\n\n\ncubist_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(cubist_test_pred)))\ncubist_auc <- round((pROC::auc(c(test$y), as.numeric(c(cubist_test_pred)) - 1)), 4)\nprint(pROC::ggroc(cubist_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Cubist ROC curve \", \"(AUC = \", cubist_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# Random Forest\nrf_train_fit <- randomForest(x = train, y = as.factor(y_train), data = df)\n    \nrf_train_pred <- stats::predict(rf_train_fit, train, type = \"prob\")\nrf_train_probabilities <- ifelse(rf_train_pred > 0.50, 1, 0)[, 2]\nrf_train_table <- table(rf_train_probabilities, y_train)\nrf_train_accuracy[i] <- (rf_train_table[1, 1] + rf_train_table[2, 2]) / sum(rf_train_table)\nrf_train_accuracy_mean <- mean(rf_train_accuracy)\n\nrf_test_pred <- stats::predict(rf_train_fit, test, type = \"prob\")\nrf_test_probabilities <- ifelse(rf_test_pred > 0.50, 1, 0)[, 2]\nrf_test_table <- table(rf_test_probabilities, y_test)\nrf_test_accuracy[i] <- (rf_test_table[1, 1] + rf_test_table[2, 2]) / sum(rf_test_table)\nrf_test_accuracy_mean <- mean(rf_test_accuracy)\n\nrf_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(rf_test_probabilities)))\nrf_auc <- round((pROC::auc(c(test$y), as.numeric(c(rf_test_probabilities)) - 1)), 4)\nprint(pROC::ggroc(rf_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Random Forest \", \"(AUC = \", rf_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# XGBoost\ntrain_x <- data.matrix(train[, -ncol(train)])\ntrain_y <- train[, ncol(train)]\n    \n# define predictor and response variables in test set\ntest_x <- data.matrix(test[, -ncol(test)])\ntest_y <- test[, ncol(test)]\n    \n# define final train and test sets\nxgb_train <- xgboost::xgb.DMatrix(data = train_x, label = train_y)\nxgb_test <- xgboost::xgb.DMatrix(data = test_x, label = test_y)\n\n# define watchlist\nwatchlist <- list(train = xgb_train)\nwatchlist_test <- list(train = xgb_train, test = xgb_test)\n\nxgb_model <- xgboost::xgb.train(data = xgb_train, max.depth = 3, watchlist = watchlist_test, nrounds = 70)\n    \nxgb_min <- which.min(xgb_model$evaluation_log$validation_rmse)\n    \nxgb_train_pred <- stats::predict(object = xgb_model, newdata = train_x, type = \"prob\")\nxgb_train_predictions <- ifelse(xgb_train_pred > 0.5, 1, 0)\nxgb_train_table <- table(xgb_train_predictions, y_train)\nxgb_train_accuracy[i] <- (xgb_train_table[1, 1] + xgb_train_table[2, 2]) / sum(xgb_train_table)\nxgb_train_accuracy_mean <- mean(xgb_train_accuracy)\n\nxgb_test_pred <- stats::predict(object = xgb_model, newdata = test_x, type = \"prob\")\nxgb_test_predictions <- ifelse(xgb_test_pred > 0.5, 1, 0)\nxgb_test_table <- table(xgb_test_predictions, y_test)\nxgb_test_accuracy[i] <- (xgb_test_table[1, 1] + xgb_test_table[2, 2]) / sum(xgb_test_table)\nxgb_test_accuracy_mean <- mean(xgb_test_accuracy)\n\nxgb_roc_obj <- pROC::roc(as.numeric(c(test$y)), as.numeric(c(xgb_test_pred)))\nxgb_auc <- round((pROC::auc(c(test$y), as.numeric(c(xgb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(xgb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"XGBoost \", \"(AUC = \", xgb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n# Ensemble\n\nensemble1 <- data.frame(\n  'ADABoost' = adaboost_test_predictions,\n  'BayesGLM'= bayesglm_test_predictions,\n  'C50' = C50_test_predictions,\n  'Cubist' = cubist_test_pred,\n  'Random_Forest' = rf_test_pred,\n  'XGBoost' = xgb_test_predictions,\n  'y' = test$y\n)\n\nensemble_index <- sample(c(1:2), nrow(ensemble1), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble1[ensemble_index == 1, ]\nensemble_test <- ensemble1[ensemble_index == 2, ]\nensemble_y_train <- ensemble_train$y\nensemble_y_test <- ensemble_test$y\n\n# Ensemble ADABoost\nensemble_adaboost_train_fit <- MachineShop::fit(as.factor(y) ~ ., data = ensemble_train, model = \"AdaBoostModel\")\n    \nensemble_adaboost_train_pred <- stats::predict(ensemble_adaboost_train_fit, ensemble_train, type = \"prob\")\nensemble_adaboost_train_probabilities <- ifelse(ensemble_adaboost_train_pred > 0.5, 1, 0)\nensemble_adaboost_train_table <- table(ensemble_adaboost_train_probabilities, ensemble_y_train)\nensemble_adaboost_train_accuracy[i] <- (ensemble_adaboost_train_table[1, 1] + ensemble_adaboost_train_table[2, 2]) / sum(ensemble_adaboost_train_table)\nensemble_adaboost_train_accuracy_mean <- mean(ensemble_adaboost_train_accuracy)\n    \nensemble_adaboost_test_pred <- stats::predict(ensemble_adaboost_train_fit, ensemble_test, type = \"prob\")\nensemble_adaboost_test_probabilities <- ifelse(ensemble_adaboost_test_pred > 0.5, 1, 0)\nensemble_adaboost_test_table <- table(ensemble_adaboost_test_probabilities, ensemble_y_test)\nensemble_adaboost_test_accuracy[i] <- (ensemble_adaboost_test_table[1, 1] + ensemble_adaboost_test_table[2, 2]) / sum(ensemble_adaboost_test_table)\nensemble_adaboost_test_accuracy_mean <- mean(ensemble_adaboost_test_accuracy)\n    \nensemble_adaboost_holdout_accuracy_mean <- mean(ensemble_adaboost_test_accuracy)\n    \nensemble_adaboost_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_adaboost_test_pred)))\nensemble_adaboost_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_adaboost_test_pred)) - 1)), 4)\nprint(pROC::ggroc(ensemble_adaboost_roc_obj, color = \"steelblue\", size = 2) +\n            ggplot2::ggtitle(paste0(\"Ensemble Adaboostoost \", \"(AUC = \", ensemble_adaboost_auc, \")\")) +\n            ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n            ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n    )\n\n\n# Ensembles using C50\nensemble_C50_train_fit <- C50::C5.0(as.factor(ensemble_y_train) ~ ., data = ensemble_train)\n\nensemble_C50_train_pred <- stats::predict(ensemble_C50_train_fit, ensemble_train, type = \"prob\")\nensemble_C50_train_probabilities <- ifelse(ensemble_C50_train_pred[, 2] > 0.5, 1, 0)\nensemble_C50_train_table <- table(ensemble_C50_train_probabilities, ensemble_y_train)\nensemble_C50_train_accuracy[i] <- (ensemble_C50_train_table[1, 1] + ensemble_C50_train_table[2, 2]) / sum(ensemble_C50_train_table)\nensemble_C50_train_accuracy_mean <- mean(ensemble_C50_train_accuracy)\n\nensemble_C50_test_pred <- stats::predict(ensemble_C50_train_fit, ensemble_test, type = \"prob\")\nensemble_C50_test_probabilities <- ifelse(ensemble_C50_test_pred[, 2] > 0.5, 1, 0)\nensemble_C50_test_table <- table(ensemble_C50_test_probabilities, ensemble_y_test)\nensemble_C50_test_accuracy[i] <- (ensemble_C50_test_table[1, 1] + ensemble_C50_test_table[2, 2]) / sum(ensemble_C50_test_table)\nensemble_C50_test_accuracy_mean <- mean(ensemble_C50_test_accuracy)\n\nensemble_C50_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_C50_test_pred[, 2])))\nensemble_C50_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_C50_test_pred[, 2])) - 1)), 4)\nprint(pROC::ggroc(ensemble_C50_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Ensemble_C50 \", \"(AUC = \", ensemble_C50_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n\n# Ensemble Random Forest\n\nensemble_rf_train_fit <- randomForest(x = ensemble_train, y = as.factor(ensemble_y_train), data = ensemble1)\n\nensemble_rf_train_pred <- stats::predict(ensemble_rf_train_fit, ensemble_train, type = \"prob\")\nensemble_rf_train_predictions <- ifelse(ensemble_rf_train_pred > 0.50, 1, 0)[, 2]\nensemble_rf_train_table <- table(ensemble_rf_train_predictions, ensemble_y_train)\nensemble_rf_train_accuracy[i] <- (ensemble_rf_train_table[1, 1] + ensemble_rf_train_table[2, 2]) / sum(ensemble_rf_train_table)\nensemble_rf_train_accuracy_mean <- mean(ensemble_rf_train_accuracy)\n\nensemble_rf_test_pred <- stats::predict(ensemble_rf_train_fit, ensemble_test, type = \"prob\")\nensemble_rf_test_predictions <- ifelse(ensemble_rf_test_pred > 0.50, 1, 0)[, 2]\nensemble_rf_test_table <- table(ensemble_rf_test_predictions, ensemble_y_test)\nensemble_rf_test_accuracy[i] <- (ensemble_rf_test_table[1, 1] + ensemble_rf_test_table[2, 2]) / sum(ensemble_rf_test_table)\nensemble_rf_test_accuracy_mean <- mean(ensemble_rf_test_accuracy)\n\nensemble_rf_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_rf_test_predictions)))\nensemble_rf_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_rf_test_predictions)) - 1)), 4)\nprint(pROC::ggroc(ensemble_rf_roc_obj, color = \"steelblue\", size = 2) +\n        ggplot2::ggtitle(paste0(\"Ensemble_rf \", \"(AUC = \", ensemble_rf_auc, \")\")) +\n        ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n        ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n\n# Ensemble XGBoost\n\nensemble_train_x <- data.matrix(ensemble_train[, -ncol(ensemble_train)])\nensemble_train_y <- ensemble_train[, ncol(ensemble_train)]\n\n  # define predictor and response variables in test set\nensemble_test_x <- data.matrix(ensemble_test[, -ncol(ensemble_test)])\nensemble_test_y <- ensemble_test[, ncol(ensemble_test)]\n\n# define final train and test sets\nensemble_xgb_train <- xgboost::xgb.DMatrix(data = ensemble_train_x, label = ensemble_train_y)\nensemble_xgb_test <- xgboost::xgb.DMatrix(data = ensemble_test_x, label = ensemble_test_y)\n\n# define watchlist\nensemble_watchlist <- list(train = ensemble_xgb_train)\nensemble_watchlist_test <- list(train = ensemble_xgb_train, test = ensemble_xgb_test)\n\nensemble_xgb_model <- xgboost::xgb.train(data = ensemble_xgb_train, max.depth = 3, watchlist = ensemble_watchlist_test, nrounds = 70)\n\nensemble_xgboost_min <- which.min(ensemble_xgb_model$evaluation_log$validation_rmse)\n\nensemble_xgb_train_pred <- predict(object = ensemble_xgb_model, newdata = ensemble_train_x, type = \"response\")\nensemble_xgb_train_probabilities <- ifelse(ensemble_xgb_train_pred > 0.5, 1, 0)\nensemble_xgb_train_table <- table(ensemble_xgb_train_probabilities, ensemble_y_train)\nensemble_xgb_train_accuracy[i] <- (ensemble_xgb_train_table[1, 1] + ensemble_xgb_train_table[2, 2]) / sum(ensemble_xgb_train_table)\nensemble_xgb_train_accuracy_mean <- mean(ensemble_xgb_train_accuracy)\n\nensemble_xgb_test_pred <- predict(object = ensemble_xgb_model, newdata = ensemble_test_x, type = \"response\")\nensemble_xgb_test_probabilities <- ifelse(ensemble_xgb_test_pred > 0.5, 1, 0)\nensemble_xgb_test_table <- table(ensemble_xgb_test_probabilities, ensemble_y_test)\nensemble_xgb_test_accuracy[i] <- (ensemble_xgb_test_table[1, 1] + ensemble_xgb_test_table[2, 2]) / sum(ensemble_xgb_test_table)\nensemble_xgb_test_accuracy_mean <- mean(ensemble_xgb_test_accuracy)\n\nensemble_xgb_roc_obj <- pROC::roc(as.numeric(c(ensemble_test$y)), as.numeric(c(ensemble_xgb_test_pred)))\nensemble_xgb_auc <- round((pROC::auc(c(ensemble_test$y), as.numeric(c(ensemble_xgb_test_pred)) - 1)), 4)\nprint(pROC::ggroc(ensemble_xgb_roc_obj, color = \"steelblue\", size = 2) +\n  ggplot2::ggtitle(paste0(\"Ensemble XGBoost \", \"(AUC = \", ensemble_xgb_auc, \")\")) +\n  ggplot2::labs(x = \"Specificity\", y = \"Sensitivity\") +\n  ggplot2::annotate(\"segment\", x = 1, xend = 0, y = 0, yend = 1, color = \"grey\")\n)\n  \n  \n# Save all trained models to the Environment\nadaboost_train_fit <<- adaboost_train_fit\nbayesglm_train_fit <<- bayesglm_train_fit\nC50_train_fit<<- C50_train_fit\ncubist_train_fit <<- cubist_train_fit\nrf_train_fit <<- rf_train_fit\nxgb_model <<- xgb_model\nensemble_adaboost_train_fit <<- ensemble_adaboost_train_fit\nensemble_C50_train_fit <<- ensemble_C50_train_fit\nensemble_rf_train_fit <<- ensemble_rf_train_fit\nensemble_xgb_model <<- ensemble_xgb_model\n\nresults <- data.frame(\n  'Model'= c('ADABoost', 'BayesGLM', 'C50', 'Cubist', 'Random_Forest', 'XGBoost', 'Ensemble_ADABoost', 'Ensemble_C50', 'Ensemble_Random_Forest', 'Ensemble_XGBoost'),\n  'Accuracy' = c(adaboost_test_accuracy_mean, bayesglm_test_accuracy_mean, C50_test_accuracy_mean, cubist_test_accuracy_mean, rf_test_accuracy_mean, xgb_test_accuracy_mean, ensemble_adaboost_holdout_accuracy_mean, ensemble_C50_test_accuracy_mean, ensemble_rf_test_accuracy_mean, ensemble_xgb_test_accuracy_mean)\n)\n\nresults <- results %>% arrange(desc(Accuracy), Model)\n\n} # Closing loop for numresamples\nreturn(results)\n\n} # Closing loop for the function\n\nlogistic_1(data = lebron, colnum = 6, numresamples = 5, train_amount = 0.60, test_amount = 0.40)\n#> Warning in rgl.init(initValue, onlyNULL): RGL: unable to\n#> open X11 display\n#> Warning: 'rgl.init' failed, running with 'rgl.useNULL =\n#> TRUE'.\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.475192 test-rmse:0.478079 \n#> [2]  train-rmse:0.460745 test-rmse:0.465107 \n#> [3]  train-rmse:0.451388 test-rmse:0.459080 \n#> [4]  train-rmse:0.445489 test-rmse:0.455906 \n#> [5]  train-rmse:0.438896 test-rmse:0.454753 \n#> [6]  train-rmse:0.435170 test-rmse:0.454940 \n#> [7]  train-rmse:0.431544 test-rmse:0.454370 \n#> [8]  train-rmse:0.427571 test-rmse:0.452858 \n#> [9]  train-rmse:0.424162 test-rmse:0.453291 \n#> [10] train-rmse:0.420299 test-rmse:0.454480 \n#> [11] train-rmse:0.416102 test-rmse:0.456759 \n#> [12] train-rmse:0.411797 test-rmse:0.457650 \n#> [13] train-rmse:0.410482 test-rmse:0.457898 \n#> [14] train-rmse:0.408634 test-rmse:0.459100 \n#> [15] train-rmse:0.406348 test-rmse:0.460187 \n#> [16] train-rmse:0.402368 test-rmse:0.458932 \n#> [17] train-rmse:0.399812 test-rmse:0.458677 \n#> [18] train-rmse:0.397695 test-rmse:0.459359 \n#> [19] train-rmse:0.394028 test-rmse:0.461203 \n#> [20] train-rmse:0.391255 test-rmse:0.462953 \n#> [21] train-rmse:0.390029 test-rmse:0.463780 \n#> [22] train-rmse:0.386454 test-rmse:0.464364 \n#> [23] train-rmse:0.383933 test-rmse:0.464019 \n#> [24] train-rmse:0.380405 test-rmse:0.465352 \n#> [25] train-rmse:0.378997 test-rmse:0.465155 \n#> [26] train-rmse:0.378035 test-rmse:0.465882 \n#> [27] train-rmse:0.376297 test-rmse:0.467501 \n#> [28] train-rmse:0.373678 test-rmse:0.468427 \n#> [29] train-rmse:0.370551 test-rmse:0.469632 \n#> [30] train-rmse:0.368614 test-rmse:0.469647 \n#> [31] train-rmse:0.367087 test-rmse:0.468969 \n#> [32] train-rmse:0.365240 test-rmse:0.469454 \n#> [33] train-rmse:0.364351 test-rmse:0.469551 \n#> [34] train-rmse:0.363537 test-rmse:0.470564 \n#> [35] train-rmse:0.362598 test-rmse:0.470403 \n#> [36] train-rmse:0.360605 test-rmse:0.470439 \n#> [37] train-rmse:0.359448 test-rmse:0.471352 \n#> [38] train-rmse:0.357047 test-rmse:0.472299 \n#> [39] train-rmse:0.356753 test-rmse:0.472468 \n#> [40] train-rmse:0.356337 test-rmse:0.472618 \n#> [41] train-rmse:0.354152 test-rmse:0.472050 \n#> [42] train-rmse:0.353494 test-rmse:0.472129 \n#> [43] train-rmse:0.351278 test-rmse:0.473007 \n#> [44] train-rmse:0.349063 test-rmse:0.474251 \n#> [45] train-rmse:0.347140 test-rmse:0.475184 \n#> [46] train-rmse:0.344705 test-rmse:0.476496 \n#> [47] train-rmse:0.343971 test-rmse:0.476474 \n#> [48] train-rmse:0.342816 test-rmse:0.477032 \n#> [49] train-rmse:0.341562 test-rmse:0.476662 \n#> [50] train-rmse:0.338655 test-rmse:0.477149 \n#> [51] train-rmse:0.336345 test-rmse:0.478140 \n#> [52] train-rmse:0.334830 test-rmse:0.478396 \n#> [53] train-rmse:0.332630 test-rmse:0.479673 \n#> [54] train-rmse:0.330085 test-rmse:0.480057 \n#> [55] train-rmse:0.328973 test-rmse:0.480978 \n#> [56] train-rmse:0.327095 test-rmse:0.480589 \n#> [57] train-rmse:0.326709 test-rmse:0.480824 \n#> [58] train-rmse:0.325985 test-rmse:0.481004 \n#> [59] train-rmse:0.323679 test-rmse:0.480972 \n#> [60] train-rmse:0.322220 test-rmse:0.480484 \n#> [61] train-rmse:0.320199 test-rmse:0.480786 \n#> [62] train-rmse:0.318155 test-rmse:0.481231 \n#> [63] train-rmse:0.317449 test-rmse:0.482030 \n#> [64] train-rmse:0.316382 test-rmse:0.482857 \n#> [65] train-rmse:0.314298 test-rmse:0.484149 \n#> [66] train-rmse:0.313270 test-rmse:0.485328 \n#> [67] train-rmse:0.312403 test-rmse:0.485504 \n#> [68] train-rmse:0.310201 test-rmse:0.485571 \n#> [69] train-rmse:0.309894 test-rmse:0.485966 \n#> [70] train-rmse:0.308192 test-rmse:0.486661\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350836 test-rmse:0.350838 \n#> [2]  train-rmse:0.246171 test-rmse:0.246174 \n#> [3]  train-rmse:0.172731 test-rmse:0.172734 \n#> [4]  train-rmse:0.121201 test-rmse:0.121204 \n#> [5]  train-rmse:0.085043 test-rmse:0.085046 \n#> [6]  train-rmse:0.059672 test-rmse:0.059674 \n#> [7]  train-rmse:0.041870 test-rmse:0.041872 \n#> [8]  train-rmse:0.029379 test-rmse:0.029381 \n#> [9]  train-rmse:0.020615 test-rmse:0.020616 \n#> [10] train-rmse:0.014465 test-rmse:0.014466 \n#> [11] train-rmse:0.010149 test-rmse:0.010150 \n#> [12] train-rmse:0.007122 test-rmse:0.007122 \n#> [13] train-rmse:0.004997 test-rmse:0.004997 \n#> [14] train-rmse:0.003506 test-rmse:0.003507 \n#> [15] train-rmse:0.002460 test-rmse:0.002460 \n#> [16] train-rmse:0.001726 test-rmse:0.001726 \n#> [17] train-rmse:0.001211 test-rmse:0.001211 \n#> [18] train-rmse:0.000850 test-rmse:0.000850 \n#> [19] train-rmse:0.000596 test-rmse:0.000596 \n#> [20] train-rmse:0.000418 test-rmse:0.000419 \n#> [21] train-rmse:0.000294 test-rmse:0.000294 \n#> [22] train-rmse:0.000206 test-rmse:0.000206 \n#> [23] train-rmse:0.000145 test-rmse:0.000145 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.471288 test-rmse:0.480597 \n#> [2]  train-rmse:0.454148 test-rmse:0.471956 \n#> [3]  train-rmse:0.444279 test-rmse:0.468135 \n#> [4]  train-rmse:0.436460 test-rmse:0.466477 \n#> [5]  train-rmse:0.430706 test-rmse:0.465734 \n#> [6]  train-rmse:0.425711 test-rmse:0.466878 \n#> [7]  train-rmse:0.420403 test-rmse:0.466368 \n#> [8]  train-rmse:0.415413 test-rmse:0.466983 \n#> [9]  train-rmse:0.411158 test-rmse:0.468684 \n#> [10] train-rmse:0.408749 test-rmse:0.468528 \n#> [11] train-rmse:0.405317 test-rmse:0.468140 \n#> [12] train-rmse:0.401817 test-rmse:0.468375 \n#> [13] train-rmse:0.399431 test-rmse:0.469036 \n#> [14] train-rmse:0.395495 test-rmse:0.469593 \n#> [15] train-rmse:0.392010 test-rmse:0.470457 \n#> [16] train-rmse:0.388653 test-rmse:0.472501 \n#> [17] train-rmse:0.387409 test-rmse:0.472397 \n#> [18] train-rmse:0.383724 test-rmse:0.473634 \n#> [19] train-rmse:0.382593 test-rmse:0.474546 \n#> [20] train-rmse:0.378619 test-rmse:0.476287 \n#> [21] train-rmse:0.376242 test-rmse:0.476496 \n#> [22] train-rmse:0.374736 test-rmse:0.474994 \n#> [23] train-rmse:0.372731 test-rmse:0.476013 \n#> [24] train-rmse:0.369985 test-rmse:0.476399 \n#> [25] train-rmse:0.367332 test-rmse:0.477766 \n#> [26] train-rmse:0.364767 test-rmse:0.478064 \n#> [27] train-rmse:0.362684 test-rmse:0.478989 \n#> [28] train-rmse:0.360203 test-rmse:0.478573 \n#> [29] train-rmse:0.358111 test-rmse:0.478864 \n#> [30] train-rmse:0.357035 test-rmse:0.479190 \n#> [31] train-rmse:0.354377 test-rmse:0.480162 \n#> [32] train-rmse:0.351908 test-rmse:0.480480 \n#> [33] train-rmse:0.351209 test-rmse:0.480823 \n#> [34] train-rmse:0.348424 test-rmse:0.480542 \n#> [35] train-rmse:0.346924 test-rmse:0.480054 \n#> [36] train-rmse:0.344238 test-rmse:0.481170 \n#> [37] train-rmse:0.343246 test-rmse:0.481543 \n#> [38] train-rmse:0.342540 test-rmse:0.481763 \n#> [39] train-rmse:0.339574 test-rmse:0.481122 \n#> [40] train-rmse:0.337354 test-rmse:0.482510 \n#> [41] train-rmse:0.335757 test-rmse:0.482954 \n#> [42] train-rmse:0.333766 test-rmse:0.484287 \n#> [43] train-rmse:0.331758 test-rmse:0.484819 \n#> [44] train-rmse:0.331201 test-rmse:0.485416 \n#> [45] train-rmse:0.329451 test-rmse:0.485938 \n#> [46] train-rmse:0.327174 test-rmse:0.485647 \n#> [47] train-rmse:0.325412 test-rmse:0.486074 \n#> [48] train-rmse:0.323566 test-rmse:0.486410 \n#> [49] train-rmse:0.322551 test-rmse:0.487852 \n#> [50] train-rmse:0.320895 test-rmse:0.488615 \n#> [51] train-rmse:0.319681 test-rmse:0.489755 \n#> [52] train-rmse:0.318106 test-rmse:0.490562 \n#> [53] train-rmse:0.315893 test-rmse:0.490989 \n#> [54] train-rmse:0.314484 test-rmse:0.491501 \n#> [55] train-rmse:0.313154 test-rmse:0.492249 \n#> [56] train-rmse:0.310960 test-rmse:0.492431 \n#> [57] train-rmse:0.308151 test-rmse:0.492892 \n#> [58] train-rmse:0.305982 test-rmse:0.493646 \n#> [59] train-rmse:0.304090 test-rmse:0.493688 \n#> [60] train-rmse:0.301510 test-rmse:0.493719 \n#> [61] train-rmse:0.300066 test-rmse:0.494916 \n#> [62] train-rmse:0.297987 test-rmse:0.496096 \n#> [63] train-rmse:0.296870 test-rmse:0.496458 \n#> [64] train-rmse:0.296605 test-rmse:0.496525 \n#> [65] train-rmse:0.294505 test-rmse:0.497734 \n#> [66] train-rmse:0.292709 test-rmse:0.498562 \n#> [67] train-rmse:0.291528 test-rmse:0.498973 \n#> [68] train-rmse:0.289977 test-rmse:0.499720 \n#> [69] train-rmse:0.288232 test-rmse:0.500854 \n#> [70] train-rmse:0.286875 test-rmse:0.501465\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350806 test-rmse:0.350810 \n#> [2]  train-rmse:0.246130 test-rmse:0.246136 \n#> [3]  train-rmse:0.172688 test-rmse:0.172694 \n#> [4]  train-rmse:0.121160 test-rmse:0.121166 \n#> [5]  train-rmse:0.085008 test-rmse:0.085012 \n#> [6]  train-rmse:0.059642 test-rmse:0.059646 \n#> [7]  train-rmse:0.041846 test-rmse:0.041849 \n#> [8]  train-rmse:0.029360 test-rmse:0.029362 \n#> [9]  train-rmse:0.020599 test-rmse:0.020601 \n#> [10] train-rmse:0.014453 test-rmse:0.014454 \n#> [11] train-rmse:0.010140 test-rmse:0.010141 \n#> [12] train-rmse:0.007114 test-rmse:0.007115 \n#> [13] train-rmse:0.004992 test-rmse:0.004992 \n#> [14] train-rmse:0.003502 test-rmse:0.003503 \n#> [15] train-rmse:0.002457 test-rmse:0.002458 \n#> [16] train-rmse:0.001724 test-rmse:0.001724 \n#> [17] train-rmse:0.001210 test-rmse:0.001210 \n#> [18] train-rmse:0.000849 test-rmse:0.000849 \n#> [19] train-rmse:0.000595 test-rmse:0.000596 \n#> [20] train-rmse:0.000418 test-rmse:0.000418 \n#> [21] train-rmse:0.000293 test-rmse:0.000293 \n#> [22] train-rmse:0.000206 test-rmse:0.000206 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.473801 test-rmse:0.480014 \n#> [2]  train-rmse:0.458215 test-rmse:0.469864 \n#> [3]  train-rmse:0.449741 test-rmse:0.466168 \n#> [4]  train-rmse:0.442625 test-rmse:0.464386 \n#> [5]  train-rmse:0.437502 test-rmse:0.463313 \n#> [6]  train-rmse:0.433406 test-rmse:0.465118 \n#> [7]  train-rmse:0.429744 test-rmse:0.465226 \n#> [8]  train-rmse:0.427812 test-rmse:0.464808 \n#> [9]  train-rmse:0.425196 test-rmse:0.465825 \n#> [10] train-rmse:0.422256 test-rmse:0.466677 \n#> [11] train-rmse:0.418247 test-rmse:0.467299 \n#> [12] train-rmse:0.414233 test-rmse:0.465835 \n#> [13] train-rmse:0.412075 test-rmse:0.467204 \n#> [14] train-rmse:0.410693 test-rmse:0.467586 \n#> [15] train-rmse:0.409503 test-rmse:0.468581 \n#> [16] train-rmse:0.408755 test-rmse:0.469685 \n#> [17] train-rmse:0.406969 test-rmse:0.469987 \n#> [18] train-rmse:0.404962 test-rmse:0.470371 \n#> [19] train-rmse:0.403384 test-rmse:0.471426 \n#> [20] train-rmse:0.400175 test-rmse:0.473795 \n#> [21] train-rmse:0.399097 test-rmse:0.473812 \n#> [22] train-rmse:0.396074 test-rmse:0.473160 \n#> [23] train-rmse:0.393537 test-rmse:0.472550 \n#> [24] train-rmse:0.390971 test-rmse:0.473480 \n#> [25] train-rmse:0.387452 test-rmse:0.473622 \n#> [26] train-rmse:0.384494 test-rmse:0.473356 \n#> [27] train-rmse:0.381861 test-rmse:0.474089 \n#> [28] train-rmse:0.377919 test-rmse:0.475059 \n#> [29] train-rmse:0.376196 test-rmse:0.474627 \n#> [30] train-rmse:0.375485 test-rmse:0.474749 \n#> [31] train-rmse:0.373193 test-rmse:0.475647 \n#> [32] train-rmse:0.372183 test-rmse:0.475989 \n#> [33] train-rmse:0.370839 test-rmse:0.475017 \n#> [34] train-rmse:0.368011 test-rmse:0.474879 \n#> [35] train-rmse:0.365786 test-rmse:0.476053 \n#> [36] train-rmse:0.361786 test-rmse:0.478188 \n#> [37] train-rmse:0.358594 test-rmse:0.479465 \n#> [38] train-rmse:0.357436 test-rmse:0.480038 \n#> [39] train-rmse:0.355393 test-rmse:0.480862 \n#> [40] train-rmse:0.352473 test-rmse:0.482852 \n#> [41] train-rmse:0.350967 test-rmse:0.483703 \n#> [42] train-rmse:0.349832 test-rmse:0.483414 \n#> [43] train-rmse:0.349549 test-rmse:0.483538 \n#> [44] train-rmse:0.347342 test-rmse:0.483293 \n#> [45] train-rmse:0.345605 test-rmse:0.483945 \n#> [46] train-rmse:0.344805 test-rmse:0.483469 \n#> [47] train-rmse:0.342493 test-rmse:0.483422 \n#> [48] train-rmse:0.339428 test-rmse:0.483950 \n#> [49] train-rmse:0.339030 test-rmse:0.484049 \n#> [50] train-rmse:0.336944 test-rmse:0.483367 \n#> [51] train-rmse:0.333893 test-rmse:0.482459 \n#> [52] train-rmse:0.331244 test-rmse:0.483001 \n#> [53] train-rmse:0.329855 test-rmse:0.482918 \n#> [54] train-rmse:0.329137 test-rmse:0.482874 \n#> [55] train-rmse:0.328487 test-rmse:0.483778 \n#> [56] train-rmse:0.327071 test-rmse:0.484555 \n#> [57] train-rmse:0.324390 test-rmse:0.485547 \n#> [58] train-rmse:0.322206 test-rmse:0.486124 \n#> [59] train-rmse:0.319892 test-rmse:0.487004 \n#> [60] train-rmse:0.317308 test-rmse:0.488051 \n#> [61] train-rmse:0.315291 test-rmse:0.487747 \n#> [62] train-rmse:0.314257 test-rmse:0.487359 \n#> [63] train-rmse:0.313193 test-rmse:0.487545 \n#> [64] train-rmse:0.310592 test-rmse:0.488525 \n#> [65] train-rmse:0.310393 test-rmse:0.488563 \n#> [66] train-rmse:0.308776 test-rmse:0.489347 \n#> [67] train-rmse:0.306880 test-rmse:0.489801 \n#> [68] train-rmse:0.306194 test-rmse:0.489683 \n#> [69] train-rmse:0.303963 test-rmse:0.490333 \n#> [70] train-rmse:0.302760 test-rmse:0.491298\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350779 test-rmse:0.350780 \n#> [2]  train-rmse:0.246092 test-rmse:0.246093 \n#> [3]  train-rmse:0.172648 test-rmse:0.172649 \n#> [4]  train-rmse:0.121123 test-rmse:0.121123 \n#> [5]  train-rmse:0.084975 test-rmse:0.084975 \n#> [6]  train-rmse:0.059615 test-rmse:0.059615 \n#> [7]  train-rmse:0.041823 test-rmse:0.041824 \n#> [8]  train-rmse:0.029341 test-rmse:0.029342 \n#> [9]  train-rmse:0.020585 test-rmse:0.020585 \n#> [10] train-rmse:0.014441 test-rmse:0.014442 \n#> [11] train-rmse:0.010131 test-rmse:0.010132 \n#> [12] train-rmse:0.007108 test-rmse:0.007108 \n#> [13] train-rmse:0.004987 test-rmse:0.004987 \n#> [14] train-rmse:0.003498 test-rmse:0.003498 \n#> [15] train-rmse:0.002454 test-rmse:0.002454 \n#> [16] train-rmse:0.001722 test-rmse:0.001722 \n#> [17] train-rmse:0.001208 test-rmse:0.001208 \n#> [18] train-rmse:0.000847 test-rmse:0.000847 \n#> [19] train-rmse:0.000595 test-rmse:0.000595 \n#> [20] train-rmse:0.000417 test-rmse:0.000417 \n#> [21] train-rmse:0.000293 test-rmse:0.000293 \n#> [22] train-rmse:0.000205 test-rmse:0.000205 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.469345 test-rmse:0.483306 \n#> [2]  train-rmse:0.451129 test-rmse:0.475073 \n#> [3]  train-rmse:0.438859 test-rmse:0.472677 \n#> [4]  train-rmse:0.430394 test-rmse:0.474792 \n#> [5]  train-rmse:0.424651 test-rmse:0.474480 \n#> [6]  train-rmse:0.419686 test-rmse:0.476138 \n#> [7]  train-rmse:0.416104 test-rmse:0.477385 \n#> [8]  train-rmse:0.409485 test-rmse:0.479152 \n#> [9]  train-rmse:0.407226 test-rmse:0.477947 \n#> [10] train-rmse:0.403165 test-rmse:0.478425 \n#> [11] train-rmse:0.401277 test-rmse:0.479130 \n#> [12] train-rmse:0.396918 test-rmse:0.479524 \n#> [13] train-rmse:0.392692 test-rmse:0.481042 \n#> [14] train-rmse:0.389309 test-rmse:0.482407 \n#> [15] train-rmse:0.385995 test-rmse:0.483304 \n#> [16] train-rmse:0.382852 test-rmse:0.484639 \n#> [17] train-rmse:0.381596 test-rmse:0.485118 \n#> [18] train-rmse:0.379741 test-rmse:0.485999 \n#> [19] train-rmse:0.377448 test-rmse:0.486745 \n#> [20] train-rmse:0.373995 test-rmse:0.487018 \n#> [21] train-rmse:0.371236 test-rmse:0.487427 \n#> [22] train-rmse:0.367078 test-rmse:0.487029 \n#> [23] train-rmse:0.364324 test-rmse:0.487024 \n#> [24] train-rmse:0.361429 test-rmse:0.486409 \n#> [25] train-rmse:0.358355 test-rmse:0.486373 \n#> [26] train-rmse:0.356879 test-rmse:0.486452 \n#> [27] train-rmse:0.356182 test-rmse:0.486853 \n#> [28] train-rmse:0.353620 test-rmse:0.487637 \n#> [29] train-rmse:0.352210 test-rmse:0.487617 \n#> [30] train-rmse:0.349787 test-rmse:0.488029 \n#> [31] train-rmse:0.347052 test-rmse:0.488152 \n#> [32] train-rmse:0.344668 test-rmse:0.489566 \n#> [33] train-rmse:0.342204 test-rmse:0.489855 \n#> [34] train-rmse:0.340754 test-rmse:0.489307 \n#> [35] train-rmse:0.340022 test-rmse:0.489012 \n#> [36] train-rmse:0.337948 test-rmse:0.489216 \n#> [37] train-rmse:0.336364 test-rmse:0.489918 \n#> [38] train-rmse:0.335238 test-rmse:0.490907 \n#> [39] train-rmse:0.333505 test-rmse:0.490977 \n#> [40] train-rmse:0.331388 test-rmse:0.491889 \n#> [41] train-rmse:0.328998 test-rmse:0.492251 \n#> [42] train-rmse:0.327470 test-rmse:0.491634 \n#> [43] train-rmse:0.326239 test-rmse:0.492068 \n#> [44] train-rmse:0.324830 test-rmse:0.492400 \n#> [45] train-rmse:0.322450 test-rmse:0.492293 \n#> [46] train-rmse:0.321218 test-rmse:0.492800 \n#> [47] train-rmse:0.318764 test-rmse:0.493141 \n#> [48] train-rmse:0.316828 test-rmse:0.493773 \n#> [49] train-rmse:0.314391 test-rmse:0.493342 \n#> [50] train-rmse:0.312233 test-rmse:0.493425 \n#> [51] train-rmse:0.310441 test-rmse:0.493546 \n#> [52] train-rmse:0.309609 test-rmse:0.493152 \n#> [53] train-rmse:0.308001 test-rmse:0.493657 \n#> [54] train-rmse:0.306697 test-rmse:0.493582 \n#> [55] train-rmse:0.304640 test-rmse:0.493931 \n#> [56] train-rmse:0.303244 test-rmse:0.494351 \n#> [57] train-rmse:0.302286 test-rmse:0.494680 \n#> [58] train-rmse:0.301160 test-rmse:0.494775 \n#> [59] train-rmse:0.300158 test-rmse:0.494663 \n#> [60] train-rmse:0.297504 test-rmse:0.494739 \n#> [61] train-rmse:0.295155 test-rmse:0.495895 \n#> [62] train-rmse:0.293799 test-rmse:0.496655 \n#> [63] train-rmse:0.292051 test-rmse:0.496780 \n#> [64] train-rmse:0.288926 test-rmse:0.497473 \n#> [65] train-rmse:0.287236 test-rmse:0.497843 \n#> [66] train-rmse:0.285369 test-rmse:0.498784 \n#> [67] train-rmse:0.283905 test-rmse:0.499510 \n#> [68] train-rmse:0.283018 test-rmse:0.499894 \n#> [69] train-rmse:0.281572 test-rmse:0.499873 \n#> [70] train-rmse:0.281403 test-rmse:0.499882\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350752 test-rmse:0.350751 \n#> [2]  train-rmse:0.246054 test-rmse:0.246052 \n#> [3]  train-rmse:0.172608 test-rmse:0.172606 \n#> [4]  train-rmse:0.121085 test-rmse:0.121083 \n#> [5]  train-rmse:0.084942 test-rmse:0.084940 \n#> [6]  train-rmse:0.059587 test-rmse:0.059586 \n#> [7]  train-rmse:0.041800 test-rmse:0.041799 \n#> [8]  train-rmse:0.029323 test-rmse:0.029322 \n#> [9]  train-rmse:0.020570 test-rmse:0.020570 \n#> [10] train-rmse:0.014430 test-rmse:0.014430 \n#> [11] train-rmse:0.010123 test-rmse:0.010122 \n#> [12] train-rmse:0.007101 test-rmse:0.007101 \n#> [13] train-rmse:0.004982 test-rmse:0.004981 \n#> [14] train-rmse:0.003495 test-rmse:0.003494 \n#> [15] train-rmse:0.002451 test-rmse:0.002451 \n#> [16] train-rmse:0.001720 test-rmse:0.001720 \n#> [17] train-rmse:0.001206 test-rmse:0.001206 \n#> [18] train-rmse:0.000846 test-rmse:0.000846 \n#> [19] train-rmse:0.000594 test-rmse:0.000594 \n#> [20] train-rmse:0.000416 test-rmse:0.000416 \n#> [21] train-rmse:0.000292 test-rmse:0.000292 \n#> [22] train-rmse:0.000205 test-rmse:0.000205 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000049 \n#> [40] train-rmse:0.000050 test-rmse:0.000049 \n#> [41] train-rmse:0.000050 test-rmse:0.000049 \n#> [42] train-rmse:0.000050 test-rmse:0.000049 \n#> [43] train-rmse:0.000050 test-rmse:0.000049 \n#> [44] train-rmse:0.000050 test-rmse:0.000049 \n#> [45] train-rmse:0.000050 test-rmse:0.000049 \n#> [46] train-rmse:0.000050 test-rmse:0.000049 \n#> [47] train-rmse:0.000050 test-rmse:0.000049 \n#> [48] train-rmse:0.000050 test-rmse:0.000049 \n#> [49] train-rmse:0.000050 test-rmse:0.000049 \n#> [50] train-rmse:0.000050 test-rmse:0.000049 \n#> [51] train-rmse:0.000050 test-rmse:0.000049 \n#> [52] train-rmse:0.000050 test-rmse:0.000049 \n#> [53] train-rmse:0.000050 test-rmse:0.000049 \n#> [54] train-rmse:0.000050 test-rmse:0.000049 \n#> [55] train-rmse:0.000050 test-rmse:0.000049 \n#> [56] train-rmse:0.000050 test-rmse:0.000049 \n#> [57] train-rmse:0.000050 test-rmse:0.000049 \n#> [58] train-rmse:0.000050 test-rmse:0.000049 \n#> [59] train-rmse:0.000050 test-rmse:0.000049 \n#> [60] train-rmse:0.000050 test-rmse:0.000049 \n#> [61] train-rmse:0.000050 test-rmse:0.000049 \n#> [62] train-rmse:0.000050 test-rmse:0.000049 \n#> [63] train-rmse:0.000050 test-rmse:0.000049 \n#> [64] train-rmse:0.000050 test-rmse:0.000049 \n#> [65] train-rmse:0.000050 test-rmse:0.000049 \n#> [66] train-rmse:0.000050 test-rmse:0.000049 \n#> [67] train-rmse:0.000050 test-rmse:0.000049 \n#> [68] train-rmse:0.000050 test-rmse:0.000049 \n#> [69] train-rmse:0.000050 test-rmse:0.000049 \n#> [70] train-rmse:0.000050 test-rmse:0.000049\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.469983 test-rmse:0.481825 \n#> [2]  train-rmse:0.453292 test-rmse:0.474652 \n#> [3]  train-rmse:0.442160 test-rmse:0.470276 \n#> [4]  train-rmse:0.434314 test-rmse:0.469678 \n#> [5]  train-rmse:0.429381 test-rmse:0.468875 \n#> [6]  train-rmse:0.425673 test-rmse:0.470175 \n#> [7]  train-rmse:0.422683 test-rmse:0.469549 \n#> [8]  train-rmse:0.419515 test-rmse:0.471713 \n#> [9]  train-rmse:0.418412 test-rmse:0.471700 \n#> [10] train-rmse:0.412741 test-rmse:0.473357 \n#> [11] train-rmse:0.410944 test-rmse:0.473120 \n#> [12] train-rmse:0.408424 test-rmse:0.473023 \n#> [13] train-rmse:0.405919 test-rmse:0.472233 \n#> [14] train-rmse:0.404462 test-rmse:0.472746 \n#> [15] train-rmse:0.403653 test-rmse:0.472729 \n#> [16] train-rmse:0.399715 test-rmse:0.472551 \n#> [17] train-rmse:0.396624 test-rmse:0.472368 \n#> [18] train-rmse:0.393517 test-rmse:0.471737 \n#> [19] train-rmse:0.392274 test-rmse:0.471105 \n#> [20] train-rmse:0.391304 test-rmse:0.471421 \n#> [21] train-rmse:0.388356 test-rmse:0.472729 \n#> [22] train-rmse:0.386881 test-rmse:0.473676 \n#> [23] train-rmse:0.383574 test-rmse:0.474620 \n#> [24] train-rmse:0.379972 test-rmse:0.475196 \n#> [25] train-rmse:0.379027 test-rmse:0.474874 \n#> [26] train-rmse:0.377911 test-rmse:0.475319 \n#> [27] train-rmse:0.374023 test-rmse:0.477312 \n#> [28] train-rmse:0.371324 test-rmse:0.477651 \n#> [29] train-rmse:0.369694 test-rmse:0.477372 \n#> [30] train-rmse:0.367384 test-rmse:0.476881 \n#> [31] train-rmse:0.365231 test-rmse:0.477780 \n#> [32] train-rmse:0.362514 test-rmse:0.477771 \n#> [33] train-rmse:0.361889 test-rmse:0.477999 \n#> [34] train-rmse:0.358611 test-rmse:0.477956 \n#> [35] train-rmse:0.357341 test-rmse:0.477890 \n#> [36] train-rmse:0.354910 test-rmse:0.477889 \n#> [37] train-rmse:0.352434 test-rmse:0.478723 \n#> [38] train-rmse:0.349446 test-rmse:0.479393 \n#> [39] train-rmse:0.348320 test-rmse:0.479751 \n#> [40] train-rmse:0.346057 test-rmse:0.479579 \n#> [41] train-rmse:0.344301 test-rmse:0.479321 \n#> [42] train-rmse:0.342503 test-rmse:0.479055 \n#> [43] train-rmse:0.341741 test-rmse:0.479281 \n#> [44] train-rmse:0.340278 test-rmse:0.479698 \n#> [45] train-rmse:0.338905 test-rmse:0.479804 \n#> [46] train-rmse:0.338534 test-rmse:0.479388 \n#> [47] train-rmse:0.338150 test-rmse:0.479594 \n#> [48] train-rmse:0.336896 test-rmse:0.479947 \n#> [49] train-rmse:0.334466 test-rmse:0.481629 \n#> [50] train-rmse:0.332821 test-rmse:0.482458 \n#> [51] train-rmse:0.330561 test-rmse:0.483253 \n#> [52] train-rmse:0.328776 test-rmse:0.482987 \n#> [53] train-rmse:0.327713 test-rmse:0.483217 \n#> [54] train-rmse:0.325673 test-rmse:0.483616 \n#> [55] train-rmse:0.323789 test-rmse:0.484530 \n#> [56] train-rmse:0.321965 test-rmse:0.485015 \n#> [57] train-rmse:0.321327 test-rmse:0.484828 \n#> [58] train-rmse:0.320789 test-rmse:0.484955 \n#> [59] train-rmse:0.319977 test-rmse:0.485544 \n#> [60] train-rmse:0.319336 test-rmse:0.485246 \n#> [61] train-rmse:0.317920 test-rmse:0.485508 \n#> [62] train-rmse:0.316384 test-rmse:0.486216 \n#> [63] train-rmse:0.314147 test-rmse:0.486477 \n#> [64] train-rmse:0.313547 test-rmse:0.487216 \n#> [65] train-rmse:0.311705 test-rmse:0.488294 \n#> [66] train-rmse:0.310588 test-rmse:0.488662 \n#> [67] train-rmse:0.308651 test-rmse:0.487913 \n#> [68] train-rmse:0.306966 test-rmse:0.488149 \n#> [69] train-rmse:0.305213 test-rmse:0.489127 \n#> [70] train-rmse:0.303103 test-rmse:0.489845\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#> [1]  train-rmse:0.350759 test-rmse:0.350760 \n#> [2]  train-rmse:0.246064 test-rmse:0.246065 \n#> [3]  train-rmse:0.172619 test-rmse:0.172619 \n#> [4]  train-rmse:0.121095 test-rmse:0.121096 \n#> [5]  train-rmse:0.084951 test-rmse:0.084951 \n#> [6]  train-rmse:0.059595 test-rmse:0.059595 \n#> [7]  train-rmse:0.041807 test-rmse:0.041807 \n#> [8]  train-rmse:0.029328 test-rmse:0.029328 \n#> [9]  train-rmse:0.020574 test-rmse:0.020574 \n#> [10] train-rmse:0.014433 test-rmse:0.014433 \n#> [11] train-rmse:0.010125 test-rmse:0.010125 \n#> [12] train-rmse:0.007103 test-rmse:0.007103 \n#> [13] train-rmse:0.004983 test-rmse:0.004983 \n#> [14] train-rmse:0.003496 test-rmse:0.003496 \n#> [15] train-rmse:0.002452 test-rmse:0.002452 \n#> [16] train-rmse:0.001720 test-rmse:0.001720 \n#> [17] train-rmse:0.001207 test-rmse:0.001207 \n#> [18] train-rmse:0.000847 test-rmse:0.000847 \n#> [19] train-rmse:0.000594 test-rmse:0.000594 \n#> [20] train-rmse:0.000417 test-rmse:0.000417 \n#> [21] train-rmse:0.000292 test-rmse:0.000292 \n#> [22] train-rmse:0.000205 test-rmse:0.000205 \n#> [23] train-rmse:0.000144 test-rmse:0.000144 \n#> [24] train-rmse:0.000101 test-rmse:0.000101 \n#> [25] train-rmse:0.000071 test-rmse:0.000071 \n#> [26] train-rmse:0.000050 test-rmse:0.000050 \n#> [27] train-rmse:0.000050 test-rmse:0.000050 \n#> [28] train-rmse:0.000050 test-rmse:0.000050 \n#> [29] train-rmse:0.000050 test-rmse:0.000050 \n#> [30] train-rmse:0.000050 test-rmse:0.000050 \n#> [31] train-rmse:0.000050 test-rmse:0.000050 \n#> [32] train-rmse:0.000050 test-rmse:0.000050 \n#> [33] train-rmse:0.000050 test-rmse:0.000050 \n#> [34] train-rmse:0.000050 test-rmse:0.000050 \n#> [35] train-rmse:0.000050 test-rmse:0.000050 \n#> [36] train-rmse:0.000050 test-rmse:0.000050 \n#> [37] train-rmse:0.000050 test-rmse:0.000050 \n#> [38] train-rmse:0.000050 test-rmse:0.000050 \n#> [39] train-rmse:0.000050 test-rmse:0.000050 \n#> [40] train-rmse:0.000050 test-rmse:0.000050 \n#> [41] train-rmse:0.000050 test-rmse:0.000050 \n#> [42] train-rmse:0.000050 test-rmse:0.000050 \n#> [43] train-rmse:0.000050 test-rmse:0.000050 \n#> [44] train-rmse:0.000050 test-rmse:0.000050 \n#> [45] train-rmse:0.000050 test-rmse:0.000050 \n#> [46] train-rmse:0.000050 test-rmse:0.000050 \n#> [47] train-rmse:0.000050 test-rmse:0.000050 \n#> [48] train-rmse:0.000050 test-rmse:0.000050 \n#> [49] train-rmse:0.000050 test-rmse:0.000050 \n#> [50] train-rmse:0.000050 test-rmse:0.000050 \n#> [51] train-rmse:0.000050 test-rmse:0.000050 \n#> [52] train-rmse:0.000050 test-rmse:0.000050 \n#> [53] train-rmse:0.000050 test-rmse:0.000050 \n#> [54] train-rmse:0.000050 test-rmse:0.000050 \n#> [55] train-rmse:0.000050 test-rmse:0.000050 \n#> [56] train-rmse:0.000050 test-rmse:0.000050 \n#> [57] train-rmse:0.000050 test-rmse:0.000050 \n#> [58] train-rmse:0.000050 test-rmse:0.000050 \n#> [59] train-rmse:0.000050 test-rmse:0.000050 \n#> [60] train-rmse:0.000050 test-rmse:0.000050 \n#> [61] train-rmse:0.000050 test-rmse:0.000050 \n#> [62] train-rmse:0.000050 test-rmse:0.000050 \n#> [63] train-rmse:0.000050 test-rmse:0.000050 \n#> [64] train-rmse:0.000050 test-rmse:0.000050 \n#> [65] train-rmse:0.000050 test-rmse:0.000050 \n#> [66] train-rmse:0.000050 test-rmse:0.000050 \n#> [67] train-rmse:0.000050 test-rmse:0.000050 \n#> [68] train-rmse:0.000050 test-rmse:0.000050 \n#> [69] train-rmse:0.000050 test-rmse:0.000050 \n#> [70] train-rmse:0.000050 test-rmse:0.000050\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases\n#> Setting levels: control = 0, case = 1\n#> Setting direction: controls < cases#>                     Model  Accuracy\n#> 1                     C50 1.0000000\n#> 2                  Cubist 1.0000000\n#> 3       Ensemble_ADABoost 1.0000000\n#> 4            Ensemble_C50 1.0000000\n#> 5  Ensemble_Random_Forest 1.0000000\n#> 6        Ensemble_XGBoost 1.0000000\n#> 7           Random_Forest 1.0000000\n#> 8                BayesGLM 0.6430284\n#> 9                 XGBoost 0.6361022\n#> 10               ADABoost 0.6176642\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"how-to-make-27-individual-forecasting-models","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9 How to Make 27 Individual Forecasting Models","text":"chapter builds time series models. also known forecasting, professional organization named International Institute Forecasters, website https://forecasters.org. strongly recommend checking IIF, ’ve found good source skills knowledge comes forecasting.chapter going build 16 forecasting models. large groups models, variations within groups. example, use (use) seasonality model making process.’ll follow pattern/process ’ve following previous sections:Load librarySet initial values 0Create functionBreak data train test setsSet random resamplingFit model training data, make predictions measure error test dataReturn resultsCheck errors warningsTest different data setThe first step load library case time series forecasting, library excellent FPP3 library. excellent book available guides learner time series process. book Forecasting Principles Practice. currently third edition, recommend highly. website book :https://otexts.com/fpp3/time series data use important data published regular bases United States federal government: monthly labor report. large set time series data sets Bureau Labor Statistics website:https://www.bls.govThe top picks time series data :https://data.bls.gov/cgi-bin/surveymost?ceFor work looking one data set, ’s far watched result: Total nonfarm employment. data can found :https://data.bls.gov/timeseries/CES0000000001I data stored Github repository, w accessing data, ways data may retrieved. plan use lot, consider registering Application Program Interface (API) time series data. information API directions register available :https://www.bls.gov/developers/","code":""},{"path":"how-to-make-27-individual-forecasting-models.html","id":"individual-time-series-models","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1 12 Individual Time Series Models","text":"","code":""},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima-1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.1 Arima 1","text":"","code":"\n\nlibrary(fpp3)\n#> ── Attaching packages ────────────────────────── fpp3 0.5 ──\n#> ✔ tibble      3.2.1     ✔ tsibble     1.1.4\n#> ✔ dplyr       1.1.4     ✔ tsibbledata 0.4.1\n#> ✔ tidyr       1.3.1     ✔ feasts      0.3.2\n#> ✔ lubridate   1.9.3     ✔ fable       0.3.4\n#> ✔ ggplot2     3.5.1     ✔ fabletools  0.4.2\n#> ── Conflicts ───────────────────────────── fpp3_conflicts ──\n#> ✖ lubridate::date()    masks base::date()\n#> ✖ dplyr::filter()      masks stats::filter()\n#> ✖ tsibble::intersect() masks base::intersect()\n#> ✖ tsibble::interval()  masks lubridate::interval()\n#> ✖ dplyr::lag()         masks stats::lag()\n#> ✖ tsibble::setdiff()   masks base::setdiff()\n#> ✖ tsibble::union()     masks base::union()\n\n# Set initial values to 0\n\n# Set up function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\n# Build the model\nArima1_model = fable::ARIMA(Difference ~ season() + trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n\n# Calculate error rate\nArima1_test_error <- time_series_train %>%\n    fabletools::model(Arima1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n\n# Make predictions on the holdout/test data\nArima1_predictions <- time_series_test %>%\n    fabletools::model(\n      Arima1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n\n# Report the predictions\nArima1_prediction_model <- Arima1_predictions[1]\nArima1_prediction_date<- Arima1_predictions[2]\nArima1_prediction_range <- Arima1_predictions[3]\nArima1_prediction_mean <-Arima1_predictions[4]\n\nresults <- data.frame(\n  'Model' = Arima1_predictions[1],\n  'Error' = Arima1_test_error$RMSE,\n  'Date' = Arima1_predictions[2],\n  'Forecast' = Arima1_predictions[4]\n)\n\nreturn(results)\n\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date     .mean\n#> 1 Arima1_model 58.39161 2024 May  740.0623\n#> 2 Arima1_model 58.39161 2024 Jun 1029.3480\n#> 3 Arima1_model 58.39161 2024 Jul  586.4908\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima-2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.2 Arima 2","text":"","code":"\n\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\nArima2_model <- fable::ARIMA(Difference ~ season(), stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n\n# Calculate error rate\nArima2_test_error <- time_series_train %>%\n  fabletools::model(Arima2_model) %>%\n  fabletools::forecast(h = number) %>%\n  fabletools::accuracy(time_series_test)\n\n# Make predictions on the holdout/test data\nArima2_predictions <- time_series_test %>%\n  fabletools::model(\n    Arima2_model,\n  ) %>%\n  fabletools::forecast(h = number)\n\n# Report the predictions\nArima2_prediction_model <- Arima2_predictions[1]\nArima2_prediction_date<- Arima2_predictions[2]\nArima2_prediction_range <- Arima2_predictions[3]\nArima2_prediction_mean <-Arima2_predictions[4]\n\nresults <- data.frame(\n  'Model' = Arima2_predictions[1],\n  'Error' = Arima2_test_error$RMSE,\n  'Date' = Arima2_predictions[2],\n  'Forecast' = Arima2_predictions[4]\n)\n\nreturn(results)\n\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date    .mean\n#> 1 Arima2_model 54.98322 2024 May 623.5714\n#> 2 Arima2_model 54.98322 2024 Jun 912.8571\n#> 3 Arima2_model 54.98322 2024 Jul 470.0000\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.3 Arima3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\n# Create the model:\nArima3_model <- fable::ARIMA(Difference ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n\n# Calculate the error:\nArima3_test_error <- time_series_train %>%\n    fabletools::model(Arima3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n\n# Calculate the forecast:\nArima3_predictions <- time_series_test %>%\n    fabletools::model(\n      Arima3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n\n# Report the predictions:\nresults <- data.frame(\n  'Model' = Arima3_predictions[1],\n  'Error' = Arima3_test_error$RMSE,\n  'Date' = Arima3_predictions[2],\n  'Forecast' = Arima3_predictions[4]\n)\n\nreturn(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date    .mean\n#> 1 Arima3_model 46.96308 2024 May 196.6640\n#> 2 Arima3_model 46.96308 2024 Jun 197.5579\n#> 3 Arima3_model 46.96308 2024 Jul 198.4518\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"arima4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.4 Arima4","text":"","code":"\n\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nArima4_model <- fable::ARIMA(Difference ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n  \n  # Calculate the error:\n  Arima4_test_error <- time_series_train %>%\n    fabletools::model(Arima4_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Arima4_predictions <- time_series_test %>%\n    fabletools::model(\n      Arima4_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Arima4_predictions[1],\n    'Error' = Arima4_test_error$RMSE,\n    'Date' = Arima4_predictions[2],\n    'Forecast' = Arima4_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date    .mean\n#> 1 Arima4_model 46.96308 2024 May 196.6640\n#> 2 Arima4_model 46.96308 2024 Jun 197.5579\n#> 3 Arima4_model 46.96308 2024 Jul 198.4518\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"deterministic","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.5 Deterministic","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\nDeterministic_model <- fable::ARIMA(Difference ~  1 + pdq(d = 0))\n  \n# Calculate the error:\nDeterministic_test_error <- time_series_train %>%\n    fabletools::model(Deterministic_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nDeterministic_predictions <- time_series_test %>%\n    fabletools::model(\n      Deterministic_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Deterministic_predictions[1],\n    'Error' = Deterministic_test_error$RMSE,\n    'Date' = Deterministic_predictions[2],\n    'Forecast' = Deterministic_predictions[4]\n  )\n  \nreturn(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                .model    Error     Date    .mean\n#> 1 Deterministic_model 42.86143 2024 May 146.3409\n#> 2 Deterministic_model 42.86143 2024 Jun 146.3409\n#> 3 Deterministic_model 42.86143 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"drift","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.6 Drift","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nDrift_model <- fable::SNAIVE(Difference ~ drift())\n\n# Calculate the error:\nDrift_test_error <- time_series_train %>%\n    fabletools::model(Drift_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nDrift_predictions <- time_series_test %>%\n    fabletools::model(\n      Drift_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Drift_predictions[1],\n    'Error' = Drift_test_error$RMSE,\n    'Date' = Drift_predictions[2],\n    'Range' = Drift_predictions[3],\n    'Value' = Drift_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>        .model    Error     Date            Difference\n#> 1 Drift_model 99.09185 2024 May N(287.3684, 12520161)\n#> 2 Drift_model 99.09185 2024 Jun N(191.3684, 12520161)\n#> 3 Drift_model 99.09185 2024 Jul N(193.3684, 12520161)\n#>      .mean\n#> 1 287.3684\n#> 2 191.3684\n#> 3 193.3684\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.7 ETS1","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS1_model <-   fable::ETS(Difference ~ season() + trend())\n\n# Calculate the error:\nETS1_test_error <- time_series_train %>%\n    fabletools::model(ETS1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS1_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS1_predictions[1],\n    'Error' = ETS1_test_error$RMSE,\n    'Date' = ETS1_predictions[2],\n    'Value' = ETS1_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS1_model 42.98491 2024 May 189.8662\n#> 2 ETS1_model 42.98491 2024 Jun 189.8662\n#> 3 ETS1_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.8 ETS2","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS2_model <- fable::ETS(Difference ~ trend())\n\n# Calculate the error:\nETS2_test_error <- time_series_train %>%\n    fabletools::model(ETS2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS2_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS2_predictions[1],\n    'Error' = ETS2_test_error$RMSE,\n    'Date' = ETS2_predictions[2],\n    'Value' = ETS2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS2_model 42.98491 2024 May 189.8662\n#> 2 ETS2_model 42.98491 2024 Jun 189.8662\n#> 3 ETS2_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.9 ETS3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS3_model <- fable::ETS(Difference ~ season())\n\n# Calculate the error:\nETS3_test_error <- time_series_train %>%\n    fabletools::model(ETS3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS3_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS3_predictions[1],\n    'Error' = ETS3_test_error$RMSE,\n    'Date' = ETS3_predictions[2],\n    'Value' = ETS3_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS3_model 42.98491 2024 May 189.8662\n#> 2 ETS3_model 42.98491 2024 Jun 189.8662\n#> 3 ETS3_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"ets4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.10 ETS4","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nETS4_model <- fable::ETS(Difference)\n\n# Calculate the error:\nETS4_test_error <- time_series_train %>%\n    fabletools::model(ETS4_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nETS4_predictions <- time_series_test %>%\n    fabletools::model(\n      ETS4_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = ETS4_predictions[1],\n    'Error' = ETS4_test_error$RMSE,\n    'Date' = ETS4_predictions[2],\n    'Value' = ETS4_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 ETS4_model 42.98491 2024 May 189.8662\n#> 2 ETS4_model 42.98491 2024 Jun 189.8662\n#> 3 ETS4_model 42.98491 2024 Jul 189.8662\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"holt-winters-additive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.11 Holt-Winters Additive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nHolt_Winters_Additive_model <- fable::ETS(Difference ~ error(\"A\") + trend(\"A\") + season(\"A\"))\n\n# Calculate the error:\nHolt_Winters_Additive_test_error <- time_series_train %>%\n    fabletools::model(Holt_Winters_Additive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nHolt_Winters_Additive_predictions <- time_series_test %>%\n    fabletools::model(\n      Holt_Winters_Additive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Holt_Winters_Additive_predictions[1],\n    'Error' = Holt_Winters_Additive_test_error$RMSE,\n    'Date' = Holt_Winters_Additive_predictions[2],\n    'Value' = Holt_Winters_Additive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                        .model    Error     Date    .mean\n#> 1 Holt_Winters_Additive_model 52.74689 2024 May 467.2980\n#> 2 Holt_Winters_Additive_model 52.74689 2024 Jun 802.6268\n#> 3 Holt_Winters_Additive_model 52.74689 2024 Jul 229.5501\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"holt-winters-damped","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.12 Holt-Winters Damped","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nHolt_Winters_Damped_model <- fable::ETS(Difference ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n\n# Calculate the error:\nHolt_Winters_Damped_test_error <- time_series_train %>%\n    fabletools::model(Holt_Winters_Damped_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nHolt_Winters_Damped_predictions <- time_series_test %>%\n    fabletools::model(\n      Holt_Winters_Damped_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Holt_Winters_Damped_predictions[1],\n    'Error' = Holt_Winters_Damped_test_error$RMSE,\n    'Date' = Holt_Winters_Damped_predictions[2],\n    'Value' = Holt_Winters_Damped_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                      .model    Error     Date     .mean\n#> 1 Holt_Winters_Damped_model 481.2281 2024 May 137.36755\n#> 2 Holt_Winters_Damped_model 481.2281 2024 Jun 116.40494\n#> 3 Holt_Winters_Damped_model 481.2281 2024 Jul  97.76673\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"holt-winters-multiplicative","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.13 Holt-Winters Multiplicative","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nHolt_Winters_Multiplicative_model <- fable::ETS(Difference ~ error(\"M\") + trend(\"A\") + season(\"M\"))\n\n# Calculate the error:\nHolt_Winters_Multiplicative_test_error <- time_series_train %>%\n    fabletools::model(Holt_Winters_Multiplicative_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nHolt_Winters_Multiplicative_predictions <- time_series_test %>%\n    fabletools::model(\n      Holt_Winters_Multiplicative_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Holt_Winters_Multiplicative_predictions[1],\n    'Error' = Holt_Winters_Multiplicative_test_error$RMSE,\n    'Date' = Holt_Winters_Multiplicative_predictions[2],\n    'Value' = Holt_Winters_Multiplicative_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                              .model    Error     Date\n#> 1 Holt_Winters_Multiplicative_model 470.7401 2024 May\n#> 2 Holt_Winters_Multiplicative_model 470.7401 2024 Jun\n#> 3 Holt_Winters_Multiplicative_model 470.7401 2024 Jul\n#>      .mean\n#> 1 122.6052\n#> 2 128.6342\n#> 3 108.2321\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.14 Linear 1","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n# Create the model:\nLinear1_model <- fable::ETS(Difference ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n\n# Calculate the error:\nLinear1_test_error <- time_series_train %>%\n    fabletools::model(Linear1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n# Calculate the forecast:\nLinear1_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n# Report the predictions:\nresults <- data.frame(\n    'Model' = Linear1_predictions[1],\n    'Error' = Linear1_test_error$RMSE,\n    'Date' = Linear1_predictions[2],\n    'Value' = Linear1_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date     .mean\n#> 1 Linear1_model 481.2281 2024 May 137.36755\n#> 2 Linear1_model 481.2281 2024 Jun 116.40494\n#> 3 Linear1_model 481.2281 2024 Jul  97.76673\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.15 Linear 2","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Linear2_model <- fable::TSLM(Difference)\n  \n  # Calculate the error:\n  Linear2_test_error <- time_series_train %>%\n    fabletools::model(Linear2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Linear2_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Linear2_predictions[1],\n    'Error' = Linear2_test_error$RMSE,\n    'Date' = Linear2_predictions[2],\n    'Value' = Linear2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date    .mean\n#> 1 Linear2_model 120.6944 2024 May 146.3409\n#> 2 Linear2_model 120.6944 2024 Jun 146.3409\n#> 3 Linear2_model 120.6944 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.16 Linear 3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Linear2_model <- fable::TSLM(Difference)\n  \n  # Calculate the error:\n  Linear2_test_error <- time_series_train %>%\n    fabletools::model(Linear2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Linear2_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Linear2_predictions[1],\n    'Error' = Linear2_test_error$RMSE,\n    'Date' = Linear2_predictions[2],\n    'Value' = Linear2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date    .mean\n#> 1 Linear2_model 120.6944 2024 May 146.3409\n#> 2 Linear2_model 120.6944 2024 Jun 146.3409\n#> 3 Linear2_model 120.6944 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"linear-4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.17 Linear 4","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Linear4_model <- fable::TSLM(Difference ~ trend())\n  \n  # Calculate the error:\n  Linear4_test_error <- time_series_train %>%\n    fabletools::model(Linear4_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Linear4_predictions <- time_series_test %>%\n    fabletools::model(\n      Linear4_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Linear4_predictions[1],\n    'Error' = Linear4_test_error$RMSE,\n    'Date' = Linear4_predictions[2],\n    'Value' = Linear4_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>          .model    Error     Date    .mean\n#> 1 Linear4_model 87.83978 2024 May 313.7312\n#> 2 Linear4_model 87.83978 2024 Jun 317.4928\n#> 3 Linear4_model 87.83978 2024 Jul 321.2543\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"mean","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.18 Mean","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Mean_model <- fable::MEAN(Difference)\n  \n  # Calculate the error:\n  Mean_test_error <- time_series_train %>%\n    fabletools::model(Mean_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Mean_predictions <- time_series_test %>%\n    fabletools::model(\n      Mean_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Mean_predictions[1],\n    'Error' = Mean_test_error$RMSE,\n    'Date' = Mean_predictions[2],\n    'Value' = Mean_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>       .model    Error     Date    .mean\n#> 1 Mean_model 120.6944 2024 May 146.3409\n#> 2 Mean_model 120.6944 2024 Jun 146.3409\n#> 3 Mean_model 120.6944 2024 Jul 146.3409\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"naive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.19 Naive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Naive_model <- fable::NAIVE(Difference)\n  \n  # Calculate the error:\n  Naive_test_error <- time_series_train %>%\n    fabletools::model(Naive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Naive_predictions <- time_series_test %>%\n    fabletools::model(\n      Naive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Naive_predictions[1],\n    'Error' = Naive_test_error$RMSE,\n    'Date' = Naive_predictions[2],\n    'Value' = Naive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>        .model    Error     Date .mean\n#> 1 Naive_model 52.38957 2024 May   175\n#> 2 Naive_model 52.38957 2024 Jun   175\n#> 3 Naive_model 52.38957 2024 Jul   175\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-1","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.20 Neuralnet 1","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet1_model <- fable::NNETAR(Difference ~ season() + trend())\n  \n  # Calculate the error:\n  Neuralnet1_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet1_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet1_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet1_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet1_predictions[1],\n    'Error' = Neuralnet1_test_error$RMSE,\n    'Date' = Neuralnet1_predictions[2],\n    'Value' = Neuralnet1_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date      .mean\n#> 1 Neuralnet1_model 21.67796 2024 May   227.2104\n#> 2 Neuralnet1_model 21.67796 2024 Jun  -245.7613\n#> 3 Neuralnet1_model 21.67796 2024 Jul -1212.2786\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-2","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.21 Neuralnet 2","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet2_model <- fable::NNETAR(Difference ~ trend())\n  \n  # Calculate the error:\n  Neuralnet2_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet2_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet2_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet2_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet2_predictions[1],\n    'Error' = Neuralnet2_test_error$RMSE,\n    'Date' = Neuralnet2_predictions[2],\n    'Value' = Neuralnet2_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date      .mean\n#> 1 Neuralnet2_model 51.42435 2024 May   246.7653\n#> 2 Neuralnet2_model 51.42435 2024 Jun -1056.4116\n#> 3 Neuralnet2_model 51.42435 2024 Jul -5376.7290\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-3","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.22 Neuralnet 3","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet3_model <- fable::NNETAR(Difference ~ season())\n  \n  # Calculate the error:\n  Neuralnet3_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet3_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet3_predictions[1],\n    'Error' = Neuralnet3_test_error$RMSE,\n    'Date' = Neuralnet3_predictions[2],\n    'Value' = Neuralnet3_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date    .mean\n#> 1 Neuralnet3_model 25.75467 2024 May 284.6480\n#> 2 Neuralnet3_model 25.75467 2024 Jun 313.0187\n#> 3 Neuralnet3_model 25.75467 2024 Jul 219.7550\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"neuralnet-4","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.23 Neuralnet 4","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Neuralnet3_model <- fable::NNETAR(Difference ~ season())\n  \n  # Calculate the error:\n  Neuralnet3_test_error <- time_series_train %>%\n    fabletools::model(Neuralnet3_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Neuralnet3_predictions <- time_series_test %>%\n    fabletools::model(\n      Neuralnet3_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Neuralnet3_predictions[1],\n    'Error' = Neuralnet3_test_error$RMSE,\n    'Date' = Neuralnet3_predictions[2],\n    'Value' = Neuralnet3_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date     .mean\n#> 1 Neuralnet3_model 34.71215 2024 May 301.21197\n#> 2 Neuralnet3_model 34.71215 2024 Jun 278.81890\n#> 3 Neuralnet3_model 34.71215 2024 Jul  91.52135\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"prophet-additive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.24 Prophet Additive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Prophet_Additive_model <- fable.prophet::prophet(Difference ~ season(period = 12, type = \"additive\"))\n  \n  # Calculate the error:\n  Prophet_Additive_test_error <- time_series_train %>%\n    fabletools::model(Prophet_Additive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Prophet_Additive_predictions <- time_series_test %>%\n    fabletools::model(\n      Prophet_Additive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Prophet_Additive_predictions[1],\n    'Error' = Prophet_Additive_test_error$RMSE,\n    'Date' = Prophet_Additive_predictions[2],\n    'Value' = Prophet_Additive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                   .model    Error     Date    .mean\n#> 1 Prophet_Additive_model 99.30282 2024 May 2650.549\n#> 2 Prophet_Additive_model 99.30282 2024 Jun 3159.724\n#> 3 Prophet_Additive_model 99.30282 2024 Jul 1010.568\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"prophet-multiplicative","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.25 Prophet Multiplicative","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Prophet_Multiplicative_model <- fable.prophet::prophet(Difference ~ season(period = 12, type = \"multiplicative\"))\n  \n  # Calculate the error:\n  Prophet_Multiplicative_test_error <- time_series_train %>%\n    fabletools::model(Prophet_Multiplicative_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Prophet_Multiplicative_predictions <- time_series_test %>%\n    fabletools::model(\n      Prophet_Multiplicative_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Prophet_Multiplicative_predictions[1],\n    'Error' = Prophet_Multiplicative_test_error$RMSE,\n    'Date' = Prophet_Multiplicative_predictions[2],\n    'Value' = Prophet_Multiplicative_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>                         .model    Error     Date     .mean\n#> 1 Prophet_Multiplicative_model 76.28555 2024 May -32.68779\n#> 2 Prophet_Multiplicative_model 76.28555 2024 Jun -63.67985\n#> 3 Prophet_Multiplicative_model 76.28555 2024 Jul -35.34949\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"seasonal-naive","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.26 Seasonal Naive","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  SNaive_model <- fable::SNAIVE(Difference)\n  \n  # Calculate the error:\n  SNaive_test_error <- time_series_train %>%\n    fabletools::model(SNaive_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  SNaive_predictions <- time_series_test %>%\n    fabletools::model(\n      SNaive_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = SNaive_predictions[1],\n    'Error' = SNaive_test_error$RMSE,\n    'Date' = SNaive_predictions[2],\n    'Value' = SNaive_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>         .model    Error     Date .mean\n#> 1 SNaive_model 98.94106 2024 May   281\n#> 2 SNaive_model 98.94106 2024 Jun   185\n#> 3 SNaive_model 98.94106 2024 Jul   187\nwarnings()"},{"path":"how-to-make-27-individual-forecasting-models.html","id":"stochastic","chapter":"9 How to Make 27 Individual Forecasting Models","heading":"9.1.27 Stochastic","text":"","code":"\nlibrary(fpp3)\n\n# Set up the function:\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n  \n  # Determine if the data is quarterly, monthly or weekly from the input:\n  if (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  if (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n  \n  # Split the data into train and test:\n  time_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\n  time_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n  \n  # Create the model:\n  Stochastic_model <- fable::ARIMA(Difference ~ pdq(d = 1), stepwise = TRUE, greedy = TRUE, approximation = TRUE)\n  \n  # Calculate the error:\n  Stochastic_test_error <- time_series_train %>%\n    fabletools::model(Stochastic_model) %>%\n    fabletools::forecast(h = number) %>%\n    fabletools::accuracy(time_series_test)\n  \n  # Calculate the forecast:\n  Stochastic_predictions <- time_series_test %>%\n    fabletools::model(\n      Stochastic_model,\n    ) %>%\n    fabletools::forecast(h = number)\n  \n  # Report the predictions:\n  results <- data.frame(\n    'Model' = Stochastic_predictions[1],\n    'Error' = Stochastic_test_error$RMSE,\n    'Date' = Stochastic_predictions[2],\n    'Value' = Stochastic_predictions[4]\n  )\n  \n  return(results)\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#>             .model    Error     Date    .mean\n#> 1 Stochastic_model 42.98025 2024 May 239.7312\n#> 2 Stochastic_model 42.98025 2024 Jun 256.0506\n#> 3 Stochastic_model 42.98025 2024 Jul 241.0237\nwarnings()\nsummary_table <- data.frame()"},{"path":"ensembles-of-26-forecasting-models.html","id":"ensembles-of-26-forecasting-models","chapter":"10 Ensembles of 26 Forecasting Models","heading":"10 Ensembles of 26 Forecasting Models","text":"know make 27 individual time series forecasting models, ensemble simply puts 27 models together.","code":"\n\nlibrary(fpp3)\n#> ── Attaching packages ────────────────────────── fpp3 0.5 ──\n#> ✔ tibble      3.2.1     ✔ tsibble     1.1.4\n#> ✔ dplyr       1.1.4     ✔ tsibbledata 0.4.1\n#> ✔ tidyr       1.3.1     ✔ feasts      0.3.2\n#> ✔ lubridate   1.9.3     ✔ fable       0.3.4\n#> ✔ ggplot2     3.5.1     ✔ fabletools  0.4.2\n#> ── Conflicts ───────────────────────────── fpp3_conflicts ──\n#> ✖ lubridate::date()    masks base::date()\n#> ✖ dplyr::filter()      masks stats::filter()\n#> ✖ tsibble::intersect() masks base::intersect()\n#> ✖ tsibble::interval()  masks lubridate::interval()\n#> ✖ dplyr::lag()         masks stats::lag()\n#> ✖ tsibble::setdiff()   masks base::setdiff()\n#> ✖ tsibble::union()     masks base::union()\n\nforecasting <- function(time_series_data, train_amount, number, time_interval = c(\"Q\", \"M\", \"W\")) {\n\n# Determine if the data is quarterly, monthly or weekly from the input:\nif (time_interval == \"Q\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearquarter(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"M\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearmonth(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\nif (time_interval == \"W\") {\n    time_series_data <- time_series_data %>%\n      dplyr::mutate(Date = tsibble::yearweek(Label), Value = Value, Difference = tsibble::difference(Value)) %>%\n      dplyr::select(Date, Value, Difference) %>%\n      tsibble::as_tsibble(index = Date) %>%\n      dplyr::slice(-c(1))\n  }\n\n# Split the data into train and test:\ntime_series_train <- time_series_data[1:round(train_amount*(nrow(time_series_data))),]\ntime_series_test <- time_series_data[(round(train_amount*(nrow(time_series_data))) +1):nrow(time_series_data),]\n\n# Fit the ensemble model on the training data\nEnsembles_model <- time_series_train %>%\n    fabletools::model(\n      Ensemble = (\n      fable::TSLM(Value ~ season() + trend()) +\n      fable::TSLM(Value) + fable::TSLM(Value ~ season()) +\n      fable::TSLM(Value ~ trend()) +\n      fable::ARIMA(Value ~ season() + trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Value ~ season(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Value ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +         fable::ARIMA(Value) + fable::ARIMA(Value ~ pdq(d = 1), stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ETS(Value ~ season() + trend()) + fable::ETS(Value ~ trend()) + fable::ETS(Value ~ season()) +\n      fable::ETS(Value) +\n      fable::ETS(Value ~ error(\"A\") + trend(\"A\") + season(\"A\")) + fable::ETS(Value ~ error(\"M\") + trend(\"A\") + season(\"M\")) +\n      fable::ETS(Value ~ error(\"M\") + trend(\"Ad\") + season(\"M\")) +\n      fable::MEAN(Value) +\n      fable::NAIVE(Value) +\n      fable::SNAIVE(Value) +\n      fable::SNAIVE(Value ~ drift()) +\n      fable.prophet::prophet(Value ~ season(period = 12, type = \"multiplicative\")) +\n      fable.prophet::prophet(Value ~ season(period = 12, type = \"additive\")) +\n      fable::NNETAR(Value ~ season() + trend()) +\n      fable::NNETAR(Value ~ trend()) +\n      fable::NNETAR(Value ~ season()) +\n      fable::NNETAR(Value))/26\n    )\n\n# # Make predicitons:\n# Ensemble_predictions <- time_series_test %>% \n#   model(Ensemble_model) %>%\n#     fabletools::forecast(h = number)\n\nEnsemble_predictions <- time_series_test %>%\n  fabletools::model(\n    Ensemble = (\n      fable::TSLM(Difference ~ season() + trend()) +\n      fable::TSLM(Difference) +\n      fable::TSLM(Difference ~ season()) +\n      fable::TSLM(Difference ~ trend()) +\n      fable::ARIMA(Difference ~ season() + trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Difference ~ season(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Difference ~ trend(),stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ARIMA(Difference) +\n      fable::ARIMA(Difference ~ pdq(d = 1), stepwise = TRUE, greedy = TRUE, approximation = TRUE) +\n      fable::ETS(Difference ~ season() + trend()) +\n      fable::ETS(Difference ~ trend()) +\n      fable::ETS(Difference ~ season()) +\n      fable::ETS(Difference) +\n      fable::ETS(Difference ~ error(\"A\") + trend(\"A\") + season(\"A\")) +\n      fable::ETS(Difference ~ error(\"M\") + trend(\"A\") + season(\"M\")) +\n      fable::ETS(Difference ~ error(\"M\") + trend(\"Ad\") + season(\"M\")) +\n      fable::MEAN(Difference) +\n      fable::NAIVE(Difference) +\n      fable::SNAIVE(Difference) +\n      fable::SNAIVE(Difference ~ drift()) +\n      fable.prophet::prophet(Difference ~ season(period = 12, type = \"multiplicative\")) +\n      fable.prophet::prophet(Difference ~ season(period = 12, type = \"additive\")) +\n      fable::NNETAR(Difference ~ season() + trend()) +\n      fable::NNETAR(Difference ~ trend()) +\n      fable::NNETAR(Difference ~ season()) +\n      fable::NNETAR(Difference)/26\n    )\n  ) %>%\n  fabletools::forecast(h = number)\n\nresults <- data.frame(\n  'Model' = Ensemble_predictions[1],\n  'Date' = Ensemble_predictions[2],\n  'Forecast' = Ensemble_predictions[4]\n)\n\nreturn(results)\n\n}\n\n# Test the function:\ntime_series_data <- read.csv('https://raw.githubusercontent.com/InfiniteCuriosity/forecasting_jobs/main/Total_Nonfarm.csv')\n\nforecasting(time_series_data = time_series_data, train_amount = 0.60, number = 3, time_interval = \"M\")\n#> Warning in sqrt(diag(best$var.coef)): NaNs produced\n#>     .model     Date     .mean\n#> 1 Ensemble 2024 May 9824.6234\n#> 2 Ensemble 2024 Jun 8429.0936\n#> 3 Ensemble 2024 Jul -119.0388\nwarnings()"},{"path":"predicting-on-totally-new-data-with-individual-models-and-ensembles.html","id":"predicting-on-totally-new-data-with-individual-models-and-ensembles","chapter":"11 Predicting on totally new data with individual models and ensembles","heading":"11 Predicting on totally new data with individual models and ensembles","text":"Let’s start simple ensemble cubist, gam linear models:","code":"\nlibrary(tree) # Allows us to use tree models\nlibrary(MASS) # For the Boston Housing data set library(Metrics)\nlibrary(reactable) # For the final report - looks amazing!\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ✖ dplyr::select() masks MASS::select()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Set initial values to 0\nlinear_train_RMSE <- 0\nlinear_test_RMSE <- 0\nlinear_RMSE <- 0\nlinear_test_predict_value <- 0\n\ntree_train_RMSE <- 0\ntree_test_RMSE <- 0\ntree_RMSE <- 0\ntree_holdout_RMSE <- 0\ntree_test_predict_value <- 0\n\nensemble_linear_RMSE <- 0\nensemble_linear_RMSE_mean <- 0\nensemble_tree_RMSE <- 0\nensemble_tree_RMSE_mean <- 0\n\nnumerical_1 <- function(data, colnum, train_amount, test_amount, numresamples, do_you_have_new_data = c(\"Y\", \"N\")){\n\n# Move target column to far right\ny <- 0\ncolnames(data)[colnum] <- \"y\"\n\n# Set up resampling\nfor (i in 1:numresamples) {\n  idx <- sample(seq(1, 2), size = nrow(data), replace = TRUE, prob = c(train_amount, test_amount))\n  train <- data[idx == 1, ]\n  test <- data[idx == 2, ]\n\n# Fit linear model on the training data, make predictions on the test data\nlinear_train_fit <- lm(y ~ ., data = train)\nlinear_predictions <- predict(object = linear_train_fit, newdata = test)\nlinear_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = linear_predictions)\nlinear_RMSE_mean <- mean(linear_RMSE)\n\n# Fit tree model on the training data, make predictions on the test data\ntree_train_fit <- tree(y ~ ., data = train)\ntree_predictions <- predict(object = tree_train_fit, newdata = test)\ntree_RMSE[i] <- Metrics::rmse(actual = test$y, predicted = tree_predictions)\ntree_RMSE_mean <- mean(tree_RMSE)\n\n# Make the weighted ensemble\nensemble <- data.frame(\n  'linear' = linear_predictions / linear_RMSE_mean,\n  'tree' = tree_predictions / tree_RMSE_mean,\n  'y_ensemble' = test$y)\n\n# Split ensemble between train and test\nensemble_idx <- sample(seq(1, 2), size = nrow(ensemble), replace = TRUE, prob = c(train_amount, test_amount))\nensemble_train <- ensemble[ensemble_idx == 1, ]\nensemble_test <- ensemble[ensemble_idx == 2, ]\n\n# Fit the ensemble data on the ensemble training data, predict on ensemble test data\nensemble_linear_train_fit <- lm(y_ensemble ~ ., data = ensemble_train)\n\nensemble_linear_predictions <- predict(object = ensemble_linear_train_fit, newdata = ensemble_test)\n\nensemble_linear_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_linear_predictions)\n\nensemble_linear_RMSE_mean <- mean(ensemble_linear_RMSE)\n\n# Fit the tree model on the ensemble training data, predict on ensemble test data\nensemble_tree_train_fit <- tree(y_ensemble ~ ., data = ensemble_train)\n\nensemble_tree_predictions <- predict(object = ensemble_tree_train_fit, newdata = ensemble_test) \n\nensemble_tree_RMSE[i] <- Metrics::rmse(actual = ensemble_test$y, predicted = ensemble_tree_predictions)\n\nensemble_tree_RMSE_mean <- mean(ensemble_tree_RMSE)\n\nresults <- data.frame(\n  'Model' = c('Linear', 'Tree', 'Ensemble_Linear', 'Ensemble_tree'),\n  'Error_Rate' = c(linear_RMSE_mean, tree_RMSE_mean, ensemble_linear_RMSE_mean, ensemble_tree_RMSE_mean)\n)\n\nresults <- results %>% arrange(Error_Rate)\n\n} # Closing brace for numresamples\n\nif (do_you_have_new_data == \"Y\") {\n  new_data <- read.csv('/Users/russconte/NewBoston.csv', header = TRUE, sep = ',')\n\n  y <- 0\n  colnames(new_data)[colnum] <- \"y\"\n\n  new_data <- new_data %>% dplyr::relocate(y, .after = last_col()) # Moves the target column to the last column on the right\n}\n  \n  new_linear <- predict(object = linear_train_fit, newdata = new_data)\n  new_tree <- predict(object = tree_train_fit, newdata = new_data)\n\n  new_ensemble <- data.frame(\n    \"linear\" = new_linear / linear_RMSE_mean,\n    \"tree\" = new_tree / tree_RMSE_mean\n    )\n\n  new_ensemble$Row_mean <- rowMeans(new_ensemble)\n  new_ensemble$y_ensemble <- new_data$y\n\n  new_ensemble_linear <- predict(object = ensemble_linear_train_fit, newdata = new_ensemble)\n  new_ensemble_tree <- predict(object = ensemble_tree_train_fit, newdata = new_ensemble)\n\n  new_data_results <-\n    data.frame(\n      \"True_Value\" = new_ensemble$y_ensemble,\n      \"Linear\" = round(new_linear, 4),\n      \"Tree\" = round(new_tree, 4),\n      \"Ensemble_Linear\" = round(new_ensemble_linear, 4),\n      \"Ensemble_Tree\" = round(new_ensemble_tree, 4)\n    )\n\n  df1 <- t(new_data_results)\n\n  predictions_of_new_data <- reactable::reactable(\n    data = df1, searchable = TRUE, pagination = FALSE, wrap = TRUE, rownames = TRUE, fullWidth = TRUE, filterable = TRUE, bordered = TRUE,\n    striped = TRUE, highlight = TRUE, resizable = TRUE\n  ) %>%\n    \n    reactablefmtr::add_title(\"Predictions of new data\")\n  \n  results <- reactable::reactable(\n    data = results, searchable = TRUE, pagination = FALSE, wrap = TRUE, rownames = TRUE, fullWidth = TRUE, filterable = TRUE, bordered = TRUE, striped = TRUE, highlight = TRUE, resizable = TRUE\n  ) %>% \n    reactablefmtr::add_title(\"Model and error rates\")\n\nreturn(list(results, predictions_of_new_data))\n\n} # Closing brace for the function\n\nnumerical_1(data = Ensembles::Boston_Housing, colnum = 14, train_amount = 0.60, test_amount = 0.40, numresamples = 25, do_you_have_new_data = \"Y\")\n#> Registered S3 method overwritten by 'tsibble':\n#>   method          from\n#>   format.interval inum\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n#> [[1]]\n#> \n#> [[2]]\n\n# Note these results show up in the Viewer."},{"path":"how-to-communicate-your-results.html","id":"how-to-communicate-your-results","chapter":"12 How to communicate your results","heading":"12 How to communicate your results","text":"chapter, going discuss, presenting results people various levels organization. include people ranging management, analysts, people C-Suite. elements common presenting results, elements going specific, depending person’s responsibilities include.’ve opportunity manage multi-million dollar accounts fortune 1000 company, ’ve also run two volunteer nonprofits, chapter Amnesty International, Chicago Apple User Group.work also includes several nonprofit social service organizations. ’ve also run vacation rental business, ’ve done lot volunteer work. Therefore, can speak wide range experience business needs.","code":""},{"path":"how-to-communicate-your-results.html","id":"a-very-basic-introduction-to-financial-reporting","chapter":"12 How to communicate your results","heading":"12.1 A very basic introduction to financial reporting","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"always-give-the-best-service-you-can-possibly-do-the-ritz-carlton-method","chapter":"12 How to communicate your results","heading":"12.1.1 Always give the best service you can possibly do: The Ritz Carlton method","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"help-your-manager-make-the-best-possible-decisions","chapter":"12 How to communicate your results","heading":"12.2 Help your manager make the best possible decisions","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"have-empathy-for-your-managersand-customers-situations","chapter":"12 How to communicate your results","heading":"12.3 Have empathy for your manager’s—and customer’s— situations","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"dont-need-them-in-any-way","chapter":"12 How to communicate your results","heading":"12.3.1 don’t need them in any way","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"have-a-great-story-to-tellhow-to-create-a-great-story","chapter":"12 How to communicate your results","heading":"12.3.2 Have a great story to tell—How to create a great story","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"br-prepared-with-strong-counterexamples","chapter":"12 How to communicate your results","heading":"12.3.3 Br prepared with strong counterexamples","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"include-uncertainty","chapter":"12 How to communicate your results","heading":"12.3.4 include uncertainty","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"tell-the-hard-truth-prepare-them-for-criticism","chapter":"12 How to communicate your results","heading":"12.3.5 tell the (hard) truth, prepare them for criticism","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"communicating-results-with-staff-who-have-profit-and-loss-responsibility","chapter":"12 How to communicate your results","heading":"12.4 Communicating results with staff who have Profit and Loss responsibility","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"vision-mission-values---what-is-the-central-question","chapter":"12 How to communicate your results","heading":"12.4.1 Vision, mission, values - what is the central question?","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"financials","chapter":"12 How to communicate your results","heading":"12.4.2 Financials","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"the-higher-up-the-org-chart-the-more-you-will-be-working-with-vision-stragety-financials","chapter":"12 How to communicate your results","heading":"12.4.3 The higher up the org chart, the more you will be working with vision, stragety, financials","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"communicating-with-customers-and-vendors","chapter":"12 How to communicate your results","heading":"12.5 Communicating with customers and vendors","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"communicating-roi-and-other-results-of-data-science","chapter":"12 How to communicate your results","heading":"12.6 Communicating ROI and other results of data science","text":"","code":""},{"path":"how-to-communicate-your-results.html","id":"social-media-and-more","chapter":"12 How to communicate your results","heading":"12.7 Social media and more","text":"","code":""},{"path":"final-comprhensive-project.html","id":"final-comprhensive-project","chapter":"13 Final Comprhensive Project","heading":"13 Final Comprhensive Project","text":"text.","code":""},{"path":"building-your-own-no-code-solutions.html","id":"building-your-own-no-code-solutions","chapter":"14 Building your own no-code solutions","heading":"14 Building your own no-code solutions","text":"text","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":" Introduction\nPhoto City Chicago snow plow stuck snow: Victorgrigas English Wikipedia - (t3xt (talk)) created work entirely ., CC0, Link","code":""}]
